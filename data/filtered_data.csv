titles,abstracts,terms,urls
"A Survey on Deep Learning for Polyp Segmentation: Techniques, Challenges and Future Trends","Early detection and assessment of polyps play a crucial role in the
prevention and treatment of colorectal cancer (CRC). Polyp segmentation
provides an effective solution to assist clinicians in accurately locating and
segmenting polyp regions. In the past, people often relied on manually
extracted lower-level features such as color, texture, and shape, which often
had issues capturing global context and lacked robustness to complex scenarios.
With the advent of deep learning, more and more outstanding medical image
segmentation algorithms based on deep learning networks have emerged, making
significant progress in this field. This paper provides a comprehensive review
of polyp segmentation algorithms. We first review some traditional algorithms
based on manually extracted features and deep segmentation algorithms, then
detail benchmark datasets related to the topic. Specifically, we carry out a
comprehensive evaluation of recent deep learning models and results based on
polyp sizes, considering the pain points of research topics and differences in
network structures. Finally, we discuss the challenges of polyp segmentation
and future trends in this field. The models, benchmark datasets, and source
code links we collected are all published at
https://github.com/taozh2017/Awesome-Polyp-Segmentation.",['cs.CV'],http://arxiv.org/abs/2311.18373v3
A Multi-Scale Feature Extraction and Fusion Deep Learning Method for Classification of Wheat Diseases,"Wheat is an important source of dietary fiber and protein that is negatively
impacted by a number of risks to its growth. The difficulty of identifying and
classifying wheat diseases is discussed with an emphasis on wheat loose smut,
leaf rust, and crown and root rot. Addressing conditions like crown and root
rot, this study introduces an innovative approach that integrates multi-scale
feature extraction with advanced image segmentation techniques to enhance
classification accuracy. The proposed method uses neural network models
Xception, Inception V3, and ResNet 50 to train on a large wheat disease
classification dataset 2020 in conjunction with an ensemble of machine vision
classifiers, including voting and stacking. The study shows that the suggested
methodology has a superior accuracy of 99.75% in the classification of wheat
diseases when compared to current state-of-the-art approaches. A deep learning
ensemble model Xception showed the highest accuracy.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2501.09938v1
"Image Segmentation with transformers: An Overview, Challenges and Future","Image segmentation, a key task in computer vision, has traditionally relied
on convolutional neural networks (CNNs), yet these models struggle with
capturing complex spatial dependencies, objects with varying scales, need for
manually crafted architecture components and contextual information. This paper
explores the shortcomings of CNN-based models and the shift towards transformer
architectures -to overcome those limitations. This work reviews
state-of-the-art transformer-based segmentation models, addressing
segmentation-specific challenges and their solutions. The paper discusses
current challenges in transformer-based segmentation and outlines promising
future trends, such as lightweight architectures and enhanced data efficiency.
This survey serves as a guide for understanding the impact of transformers in
advancing segmentation capabilities and overcoming the limitations of
traditional models.",['cs.CV'],http://arxiv.org/abs/2501.09372v1
Shape-Based Single Object Classification Using Ensemble Method Classifiers,"Nowadays, more and more images are available. Annotation and retrieval of the
images pose classification problems, where each class is defined as the group
of database images labelled with a common semantic label. Various systems have
been proposed for content-based retrieval, as well as for image classification
and indexing. In this paper, a hierarchical classification framework has been
proposed for bridging the semantic gap effectively and achieving multi-category
image classification. A well known pre-processing and post-processing method
was used and applied to three problems; image segmentation, object
identification and image classification. The method was applied to classify
single object images from Amazon and Google datasets. The classification was
tested for four different classifiers; BayesNetwork (BN), Random Forest (RF),
Bagging and Vote. The estimated classification accuracies ranged from 20% to
99% (using 10-fold cross validation). The Bagging classifier presents the best
performance, followed by the Random Forest classifier.","['cs.CV', 'cs.AI', 'cs.CL']",http://arxiv.org/abs/2501.09311v1
Few-Shot Adaptation of Training-Free Foundation Model for 3D Medical Image Segmentation,"Vision foundation models have achieved remarkable progress across various
image analysis tasks. In the image segmentation task, foundation models like
the Segment Anything Model (SAM) enable generalizable zero-shot segmentation
through user-provided prompts. However, SAM primarily trained on natural
images, lacks the domain-specific expertise of medical imaging. This limitation
poses challenges when applying SAM to medical image segmentation, including the
need for extensive fine-tuning on specialized medical datasets and a dependency
on manual prompts, which are both labor-intensive and require intervention from
medical experts.
  This work introduces the Few-shot Adaptation of Training-frEe SAM (FATE-SAM),
a novel method designed to adapt the advanced Segment Anything Model 2 (SAM2)
for 3D medical image segmentation. FATE-SAM reassembles pre-trained modules of
SAM2 to enable few-shot adaptation, leveraging a small number of support
examples to capture anatomical knowledge and perform prompt-free segmentation,
without requiring model fine-tuning. To handle the volumetric nature of medical
images, we incorporate a Volumetric Consistency mechanism that enhances spatial
coherence across 3D slices. We evaluate FATE-SAM on multiple medical imaging
datasets and compare it with supervised learning methods, zero-shot SAM
approaches, and fine-tuned medical SAM methods. Results show that FATE-SAM
delivers robust and accurate segmentation while eliminating the need for large
annotated datasets and expert intervention. FATE-SAM provides a practical,
efficient solution for medical image segmentation, making it more accessible
for clinical applications.",['cs.CV'],http://arxiv.org/abs/2501.09138v1
Densely Connected Parameter-Efficient Tuning for Referring Image Segmentation,"In the domain of computer vision, Parameter-Efficient Tuning (PET) is
increasingly replacing the traditional paradigm of pre-training followed by
full fine-tuning. PET is particularly favored for its effectiveness in large
foundation models, as it streamlines transfer learning costs and optimizes
hardware utilization. However, the current PET methods are mainly designed for
single-modal optimization. While some pioneering studies have undertaken
preliminary explorations, they still remain at the level of aligned encoders
(e.g., CLIP) and lack exploration of misaligned encoders. These methods show
sub-optimal performance with misaligned encoders, as they fail to effectively
align the multimodal features during fine-tuning. In this paper, we introduce
DETRIS, a parameter-efficient tuning framework designed to enhance low-rank
visual feature propagation by establishing dense interconnections between each
layer and all preceding layers, which enables effective cross-modal feature
interaction and adaptation to misaligned encoders. We also suggest using text
adapters to improve textual features. Our simple yet efficient approach greatly
surpasses state-of-the-art methods with 0.9% to 1.8% backbone parameter
updates, evaluated on challenging benchmarks. Our project is available at
\url{https://github.com/jiaqihuang01/DETRIS}.",['cs.CV'],http://arxiv.org/abs/2501.08580v1
Adaptive Noise-Tolerant Network for Image Segmentation,"Unlike image classification and annotation, for which deep network models
have achieved dominating superior performances compared to traditional computer
vision algorithms, deep learning for automatic image segmentation still faces
critical challenges. One of such hurdles is to obtain ground-truth
segmentations as the training labels for deep network training. Especially when
we study biomedical images, such as histopathological images (histo-images), it
is unrealistic to ask for manual segmentation labels as the ground truth for
training due to the fine image resolution as well as the large image size and
complexity. In this paper, instead of relying on clean segmentation labels, we
study whether and how integrating imperfect or noisy segmentation results from
off-the-shelf segmentation algorithms may help achieve better segmentation
results through a new Adaptive Noise-Tolerant Network (ANTN) model. We extend
the noisy label deep learning to image segmentation with two novel aspects: (1)
multiple noisy labels can be integrated into one deep learning model; (2) noisy
segmentation modeling, including probabilistic parameters, is adaptive,
depending on the given testing image appearance. Implementation of the new ANTN
model on both the synthetic data and real-world histo-images demonstrates its
effectiveness and superiority over off-the-shelf and other existing
deep-learning-based image segmentation algorithms.",['cs.CV'],http://arxiv.org/abs/2501.07163v2
AI Driven Water Segmentation with deep learning models for Enhanced Flood Monitoring,"Flooding is a major natural hazard causing significant fatalities and
economic losses annually, with increasing frequency due to climate change.
Rapid and accurate flood detection and monitoring are crucial for mitigating
these impacts. This study compares the performance of three deep learning
models UNet, ResNet, and DeepLabv3 for pixelwise water segmentation to aid in
flood detection, utilizing images from drones, in field observations, and
social media. This study involves creating a new dataset that augments
wellknown benchmark datasets with flood-specific images, enhancing the
robustness of the models. The UNet, ResNet, and DeepLab v3 architectures are
tested to determine their effectiveness in various environmental conditions and
geographical locations, and the strengths and limitations of each model are
also discussed here, providing insights into their applicability in different
scenarios by predicting image segmentation masks. This fully automated approach
allows these models to isolate flooded areas in images, significantly reducing
processing time compared to traditional semi-automated methods. The outcome of
this study is to predict segmented masks for each image effected by a flood
disaster and the validation accuracy of these models. This methodology
facilitates timely and continuous flood monitoring, providing vital data for
emergency response teams to reduce loss of life and economic damages. It offers
a significant reduction in the time required to generate flood maps, cutting
down the manual processing time. Additionally, we present avenues for future
research, including the integration of multimodal data sources and the
development of robust deep learning architectures tailored specifically for
flood detection tasks. Overall, our work contributes to the advancement of
flood management strategies through innovative use of deep learning
technologies.","['cs.CV', 'cs.AI', 'cs.LG', 'eess.IV']",http://arxiv.org/abs/2501.08266v1
Knowledge Transfer and Domain Adaptation for Fine-Grained Remote Sensing Image Segmentation,"Fine-grained remote sensing image segmentation is essential for accurately
identifying detailed objects in remote sensing images. Recently, vision
transformer models (VTMs) pre-trained on large-scale datasets have demonstrated
strong zero-shot generalization. However, directly applying them to specific
tasks may lead to domain shift. We introduce a novel end-to-end learning
paradigm combining knowledge guidance with domain refinement to enhance
performance. We present two key components: the Feature Alignment Module (FAM)
and the Feature Modulation Module (FMM). FAM aligns features from a CNN-based
backbone with those from the pretrained VTM's encoder using channel
transformation and spatial interpolation, and transfers knowledge via KL
divergence and L2 normalization constraint. FMM further adapts the knowledge to
the specific domain to address domain shift. We also introduce a fine-grained
grass segmentation dataset and demonstrate, through experiments on two
datasets, that our method achieves a significant improvement of 2.57 mIoU on
the grass dataset and 3.73 mIoU on the cloud dataset. The results highlight the
potential of combining knowledge transfer and domain adaptation to overcome
domain-related challenges and data limitations. The project page is available
at https://xavierjiezou.github.io/KTDA/.",['cs.CV'],http://arxiv.org/abs/2412.06664v3
FedSemiDG: Domain Generalized Federated Semi-supervised Medical Image Segmentation,"Medical image segmentation is challenging due to the diversity of medical
images and the lack of labeled data, which motivates recent developments in
federated semi-supervised learning (FSSL) to leverage a large amount of
unlabeled data from multiple centers for model training without sharing raw
data. However, what remains under-explored in FSSL is the domain shift problem
which may cause suboptimal model aggregation and low effectivity of the
utilization of unlabeled data, eventually leading to unsatisfactory performance
in unseen domains. In this paper, we explore this previously ignored scenario,
namely domain generalized federated semi-supervised learning (FedSemiDG), which
aims to learn a model in a distributed manner from multiple domains with
limited labeled data and abundant unlabeled data such that the model can
generalize well to unseen domains. We present a novel framework, Federated
Generalization-Aware SemiSupervised Learning (FGASL), to address the challenges
in FedSemiDG by effectively tackling critical issues at both global and local
levels. Globally, we introduce Generalization-Aware Aggregation (GAA),
assigning adaptive weights to local models based on their generalization
performance. Locally, we use a Dual-Teacher Adaptive Pseudo Label Refinement
(DR) strategy to combine global and domain-specific knowledge, generating more
reliable pseudo labels. Additionally, Perturbation-Invariant Alignment (PIA)
enforces feature consistency under perturbations, promoting domain-invariant
learning. Extensive experiments on three medical segmentation tasks (cardiac
MRI, spine MRI and bladder cancer MRI) demonstrate that our method
significantly outperforms state-of-the-art FSSL and domain generalization
approaches, achieving robust generalization on unseen domains.",['cs.CV'],http://arxiv.org/abs/2501.07378v1
Hierarchical Superpixel Segmentation via Structural Information Theory,"Superpixel segmentation is a foundation for many higher-level computer vision
tasks, such as image segmentation, object recognition, and scene understanding.
Existing graph-based superpixel segmentation methods typically concentrate on
the relationships between a given pixel and its directly adjacent pixels while
overlooking the influence of non-adjacent pixels. These approaches do not fully
leverage the global information in the graph, leading to suboptimal
segmentation quality. To address this limitation, we present SIT-HSS, a
hierarchical superpixel segmentation method based on structural information
theory. Specifically, we first design a novel graph construction strategy that
incrementally explores the pixel neighborhood to add edges based on
1-dimensional structural entropy (1D SE). This strategy maximizes the retention
of graph information while avoiding an overly complex graph structure. Then, we
design a new 2D SE-guided hierarchical graph partitioning method, which
iteratively merges pixel clusters layer by layer to reduce the graph's 2D SE
until a predefined segmentation scale is achieved. Experimental results on
three benchmark datasets demonstrate that the SIT-HSS performs better than
state-of-the-art unsupervised superpixel segmentation algorithms. The source
code is available at \url{https://github.com/SELGroup/SIT-HSS}.",['cs.CV'],http://arxiv.org/abs/2501.07069v1
UNetVL: Enhancing 3D Medical Image Segmentation with Chebyshev KAN Powered Vision-LSTM,"3D medical image segmentation has progressed considerably due to
Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), yet these
methods struggle to balance long-range dependency acquisition with
computational efficiency. To address this challenge, we propose UNETVL (U-Net
Vision-LSTM), a novel architecture that leverages recent advancements in
temporal information processing. UNETVL incorporates Vision-LSTM (ViL) for
improved scalability and memory functions, alongside an efficient Chebyshev
Kolmogorov-Arnold Networks (KAN) to handle complex and long-range dependency
patterns more effectively. We validated our method on the ACDC and AMOS2022
(post challenge Task 2) benchmark datasets, showing a significant improvement
in mean Dice score compared to recent state-of-the-art approaches, especially
over its predecessor, UNETR, with increases of 7.3% on ACDC and 15.6% on AMOS,
respectively. Extensive ablation studies were conducted to demonstrate the
impact of each component in UNETVL, providing a comprehensive understanding of
its architecture. Our code is available at https://github.com/tgrex6/UNETVL,
facilitating further research and applications in this domain.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.07017v1
PointSAM: Pointly-Supervised Segment Anything Model for Remote Sensing Images,"Segment Anything Model (SAM) is an advanced foundational model for image
segmentation, which is gradually being applied to remote sensing images (RSIs).
Due to the domain gap between RSIs and natural images, traditional methods
typically use SAM as a source pre-trained model and fine-tune it with fully
supervised masks. Unlike these methods, our work focuses on fine-tuning SAM
using more convenient and challenging point annotations. Leveraging SAM's
zero-shot capabilities, we adopt a self-training framework that iteratively
generates pseudo-labels for training. However, if the pseudo-labels contain
noisy labels, there is a risk of error accumulation. To address this issue, we
extract target prototypes from the target dataset and use the Hungarian
algorithm to match them with prediction prototypes, preventing the model from
learning in the wrong direction. Additionally, due to the complex backgrounds
and dense distribution of objects in RSI, using point prompts may result in
multiple objects being recognized as one. To solve this problem, we propose a
negative prompt calibration method based on the non-overlapping nature of
instance masks. In brief, we use the prompts of overlapping masks as
corresponding negative signals, resulting in refined masks. Combining the above
methods, we propose a novel Pointly-supervised Segment Anything Model named
PointSAM. We conduct experiments on RSI datasets, including WHU, HRSID, and
NWPU VHR-10, and the results show that our method significantly outperforms
direct testing with SAM, SAM2, and other comparison methods. Furthermore, we
introduce PointSAM as a point-to-box converter and achieve encouraging results,
suggesting that this method can be extended to other point-supervised tasks.
The code is available at https://github.com/Lans1ng/PointSAM.",['cs.CV'],http://arxiv.org/abs/2409.13401v2
RSRefSeg: Referring Remote Sensing Image Segmentation with Foundation Models,"Referring remote sensing image segmentation is crucial for achieving
fine-grained visual understanding through free-format textual input, enabling
enhanced scene and object extraction in remote sensing applications. Current
research primarily utilizes pre-trained language models to encode textual
descriptions and align them with visual modalities, thereby facilitating the
expression of relevant visual features. However, these approaches often
struggle to establish robust alignments between fine-grained semantic concepts,
leading to inconsistent representations across textual and visual information.
To address these limitations, we introduce a referring remote sensing image
segmentation foundational model, RSRefSeg. RSRefSeg leverages CLIP for visual
and textual encoding, employing both global and local textual semantics as
filters to generate referring-related visual activation features in the latent
space. These activated features then serve as input prompts for SAM, which
refines the segmentation masks through its robust visual generalization
capabilities. Experimental results on the RRSIS-D dataset demonstrate that
RSRefSeg outperforms existing methods, underscoring the effectiveness of
foundational models in enhancing multimodal task comprehension. The code is
available at \url{https://github.com/KyanChen/RSRefSeg}.",['cs.CV'],http://arxiv.org/abs/2501.06809v1
Static Segmentation by Tracking: A Frustratingly Label-Efficient Approach to Fine-Grained Segmentation,"We study image segmentation in the biological domain, particularly trait and
part segmentation from specimen images (e.g., butterfly wing stripes or beetle
body parts). This is a crucial, fine-grained task that aids in understanding
the biology of organisms. The conventional approach involves hand-labeling
masks, often for hundreds of images per species, and training a segmentation
model to generalize these labels to other images, which can be exceedingly
laborious. We present a label-efficient method named Static Segmentation by
Tracking (SST). SST is built upon the insight: while specimens of the same
species have inherent variations, the traits and parts we aim to segment show
up consistently. This motivates us to concatenate specimen images into a
``pseudo-video'' and reframe trait and part segmentation as a tracking problem.
Concretely, SST generates masks for unlabeled images by propagating annotated
or predicted masks from the ``pseudo-preceding'' images. Powered by Segment
Anything Model 2 (SAM~2) initially developed for video segmentation, we show
that SST can achieve high-quality trait and part segmentation with merely one
labeled image per species -- a breakthrough for analyzing specimen images. We
further develop a cycle-consistent loss to fine-tune the model, again using one
labeled image. Additionally, we highlight the broader potential of SST,
including one-shot instance segmentation on images taken in the wild and
trait-based image retrieval.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.06749v1
Multi-task Visual Grounding with Coarse-to-Fine Consistency Constraints,"Multi-task visual grounding involves the simultaneous execution of
localization and segmentation in images based on textual expressions. The
majority of advanced methods predominantly focus on transformer-based
multimodal fusion, aiming to extract robust multimodal representations.
However, ambiguity between referring expression comprehension (REC) and
referring image segmentation (RIS) is error-prone, leading to inconsistencies
between multi-task predictions. Besides, insufficient multimodal understanding
directly contributes to biased target perception. To overcome these challenges,
we propose a Coarse-to-fine Consistency Constraints Visual Grounding
architecture ($\text{C}^3\text{VG}$), which integrates implicit and explicit
modeling approaches within a two-stage framework. Initially, query and pixel
decoders are employed to generate preliminary detection and segmentation
outputs, a process referred to as the Rough Semantic Perception (RSP) stage.
These coarse predictions are subsequently refined through the proposed
Mask-guided Interaction Module (MIM) and a novel explicit bidirectional
consistency constraint loss to ensure consistent representations across tasks,
which we term the Refined Consistency Interaction (RCI) stage. Furthermore, to
address the challenge of insufficient multimodal understanding, we leverage
pre-trained models based on visual-linguistic fusion representations. Empirical
evaluations on the RefCOCO, RefCOCO+, and RefCOCOg datasets demonstrate the
efficacy and soundness of $\text{C}^3\text{VG}$, which significantly
outperforms state-of-the-art REC and RIS methods by a substantial margin. Code
and model will be available at \url{https://github.com/Dmmm1997/C3VG}.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.06710v1
RobustEMD: Domain Robust Matching for Cross-domain Few-shot Medical Image Segmentation,"Few-shot medical image segmentation (FSMIS) aims to perform the limited
annotated data learning in the medical image analysis scope. Despite the
progress has been achieved, current FSMIS models are all trained and deployed
on the same data domain, as is not consistent with the clinical reality that
medical imaging data is always across different data domains (e.g. imaging
modalities, institutions and equipment sequences). How to enhance the FSMIS
models to generalize well across the different specific medical imaging
domains? In this paper, we focus on the matching mechanism of the few-shot
semantic segmentation models and introduce an Earth Mover's Distance (EMD)
calculation based domain robust matching mechanism for the cross-domain
scenario. Specifically, we formulate the EMD transportation process between the
foreground support-query features, the texture structure aware weights
generation method, which proposes to perform the sobel based image gradient
calculation over the nodes, is introduced in the EMD matching flow to restrain
the domain relevant nodes. Besides, the point set level distance measurement
metric is introduced to calculated the cost for the transportation from support
set nodes to query set nodes. To evaluate the performance of our model, we
conduct experiments on three scenarios (i.e., cross-modal, cross-sequence and
cross-institution), which includes eight medical datasets and involves three
body regions, and the results demonstrate that our model achieves the SoTA
performance against the compared models.",['cs.CV'],http://arxiv.org/abs/2410.01110v3
PGP-SAM: Prototype-Guided Prompt Learning for Efficient Few-Shot Medical Image Segmentation,"The Segment Anything Model (SAM) has demonstrated strong and versatile
segmentation capabilities, along with intuitive prompt-based interactions.
However, customizing SAM for medical image segmentation requires massive
amounts of pixel-level annotations and precise point- or box-based prompt
designs. To address these challenges, we introduce PGP-SAM, a novel
prototype-based few-shot tuning approach that uses limited samples to replace
tedious manual prompts. Our key idea is to leverage inter- and intra-class
prototypes to capture class-specific knowledge and relationships. We propose
two main components: (1) a plug-and-play contextual modulation module that
integrates multi-scale information, and (2) a class-guided cross-attention
mechanism that fuses prototypes and features for automatic prompt generation.
Experiments on a public multi-organ dataset and a private ventricle dataset
demonstrate that PGP-SAM achieves superior mean Dice scores compared with
existing prompt-free SAM variants, while using only 10\% of the 2D slices.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.06692v1
Imbalanced Medical Image Segmentation with Pixel-dependent Noisy Labels,"Accurate medical image segmentation is often hindered by noisy labels in
training data, due to the challenges of annotating medical images. Prior
research works addressing noisy labels tend to make class-dependent
assumptions, overlooking the pixel-dependent nature of most noisy labels.
Furthermore, existing methods typically apply fixed thresholds to filter out
noisy labels, risking the removal of minority classes and consequently
degrading segmentation performance. To bridge these gaps, our proposed
framework, Collaborative Learning with Curriculum Selection (CLCS), addresses
pixel-dependent noisy labels with class imbalance. CLCS advances the existing
works by i) treating noisy labels as pixel-dependent and addressing them
through a collaborative learning framework, and ii) employing a curriculum
dynamic thresholding approach adapting to model learning progress to select
clean data samples to mitigate the class imbalance issue, and iii) applying a
noise balance loss to noisy data samples to improve data utilization instead of
discarding them outright. Specifically, our CLCS contains two modules:
Curriculum Noisy Label Sample Selection (CNS) and Noise Balance Loss (NBL). In
the CNS module, we designed a two-branch network with discrepancy loss for
collaborative learning so that different feature representations of the same
instance could be extracted from distinct views and used to vote the class
probabilities of pixels. Besides, a curriculum dynamic threshold is adopted to
select clean-label samples through probability voting. In the NBL module,
instead of directly dropping the suspiciously noisy labels, we further adopt a
robust loss to leverage such instances to boost the performance.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.06678v1
UCloudNet: A Residual U-Net with Deep Supervision for Cloud Image Segmentation,"Recent advancements in meteorology involve the use of ground-based sky
cameras for cloud observation. Analyzing images from these cameras helps in
calculating cloud coverage and understanding atmospheric phenomena.
Traditionally, cloud image segmentation relied on conventional computer vision
techniques. However, with the advent of deep learning, convolutional neural
networks (CNNs) are increasingly applied for this purpose. Despite their
effectiveness, CNNs often require many epochs to converge, posing challenges
for real-time processing in sky camera systems. In this paper, we introduce a
residual U-Net with deep supervision for cloud segmentation which provides
better accuracy than previous approaches, and with less training consumption.
By utilizing residual connection in encoders of UCloudNet, the feature
extraction ability is further improved.","['cs.CV', 'eess.IV']",http://arxiv.org/abs/2501.06440v1
EditAR: Unified Conditional Generation with Autoregressive Models,"Recent progress in controllable image generation and editing is largely
driven by diffusion-based methods. Although diffusion models perform
exceptionally well in specific tasks with tailored designs, establishing a
unified model is still challenging. In contrast, autoregressive models
inherently feature a unified tokenized representation, which simplifies the
creation of a single foundational model for various tasks. In this work, we
propose EditAR, a single unified autoregressive framework for a variety of
conditional image generation tasks, e.g., image editing, depth-to-image,
edge-to-image, segmentation-to-image. The model takes both images and
instructions as inputs, and predicts the edited images tokens in a vanilla
next-token paradigm. To enhance the text-to-image alignment, we further propose
to distill the knowledge from foundation models into the autoregressive
modeling process. We evaluate its effectiveness across diverse tasks on
established benchmarks, showing competitive performance to various
state-of-the-art task-specific methods. Project page:
https://jitengmu.github.io/EditAR/",['cs.CV'],http://arxiv.org/abs/2501.04699v1
GLoG-CSUnet: Enhancing Vision Transformers with Adaptable Radiomic Features for Medical Image Segmentation,"Vision Transformers (ViTs) have shown promise in medical image semantic
segmentation (MISS) by capturing long-range correlations. However, ViTs often
struggle to model local spatial information effectively, which is essential for
accurately segmenting fine anatomical details, particularly when applied to
small datasets without extensive pre-training. We introduce Gabor and Laplacian
of Gaussian Convolutional Swin Network (GLoG-CSUnet), a novel architecture
enhancing Transformer-based models by incorporating learnable radiomic
features. This approach integrates dynamically adaptive Gabor and Laplacian of
Gaussian (LoG) filters to capture texture, edge, and boundary information,
enhancing the feature representation processed by the Transformer model. Our
method uniquely combines the long-range dependency modeling of Transformers
with the texture analysis capabilities of Gabor and LoG features. Evaluated on
the Synapse multi-organ and ACDC cardiac segmentation datasets, GLoG-CSUnet
demonstrates significant improvements over state-of-the-art models, achieving a
1.14% increase in Dice score for Synapse and 0.99% for ACDC, with minimal
computational overhead (only 15 and 30 additional parameters, respectively).
GLoG-CSUnet's flexible design allows integration with various base models,
offering a promising approach for incorporating radiomics-inspired feature
extraction in Transformer architectures for medical image analysis. The code
implementation is available on GitHub at: https://github.com/HAAIL/GLoG-CSUnet.","['cs.CV', 'cs.AI', 'cs.LG']",http://arxiv.org/abs/2501.02788v2
BEN: Using Confidence-Guided Matting for Dichotomous Image Segmentation,"Current approaches to dichotomous image segmentation (DIS) treat image
matting and object segmentation as fundamentally different tasks. As
improvements in image segmentation become increasingly challenging to achieve,
combining image matting and grayscale segmentation techniques offers promising
new directions for architectural innovation. Inspired by the possibility of
aligning these two model tasks, we propose a new architectural approach for DIS
called Confidence-Guided Matting (CGM). We created the first CGM model called
Background Erase Network (BEN). BEN is comprised of two components: BEN Base
for initial segmentation and BEN Refiner for confidence refinement. Our
approach achieves substantial improvements over current state-of-the-art
methods on the DIS5K validation dataset, demonstrating that matting-based
refinement can significantly enhance segmentation quality. This work opens new
possibilities for cross-pollination between matting and segmentation techniques
in computer vision.","['cs.CV', 'eess.IV']",http://arxiv.org/abs/2501.06230v1
Gaussian Building Mesh (GBM): Extract a Building's 3D Mesh with Google Earth and Gaussian Splatting,"Recently released open-source pre-trained foundational image segmentation and
object detection models (SAM2+GroundingDINO) allow for geometrically consistent
segmentation of objects of interest in multi-view 2D images. Users can use
text-based or click-based prompts to segment objects of interest without
requiring labeled training datasets. Gaussian Splatting allows for the learning
of the 3D representation of a scene's geometry and radiance based on 2D images.
Combining Google Earth Studio, SAM2+GroundingDINO, 2D Gaussian Splatting, and
our improvements in mask refinement based on morphological operations and
contour simplification, we created a pipeline to extract the 3D mesh of any
building based on its name, address, or geographic coordinates.","['cs.CV', 'cs.GR']",http://arxiv.org/abs/2501.00625v2
LM-Net: A Light-weight and Multi-scale Network for Medical Image Segmentation,"Current medical image segmentation approaches have limitations in deeply
exploring multi-scale information and effectively combining local detail
textures with global contextual semantic information. This results in
over-segmentation, under-segmentation, and blurred segmentation boundaries. To
tackle these challenges, we explore multi-scale feature representations from
different perspectives, proposing a novel, lightweight, and multi-scale
architecture (LM-Net) that integrates advantages of both Convolutional Neural
Networks (CNNs) and Vision Transformers (ViTs) to enhance segmentation
accuracy. LM-Net employs a lightweight multi-branch module to capture
multi-scale features at the same level. Furthermore, we introduce two modules
to concurrently capture local detail textures and global semantics with
multi-scale features at different levels: the Local Feature Transformer (LFT)
and Global Feature Transformer (GFT). The LFT integrates local window
self-attention to capture local detail textures, while the GFT leverages global
self-attention to capture global contextual semantics. By combining these
modules, our model achieves complementarity between local and global
representations, alleviating the problem of blurred segmentation boundaries in
medical image segmentation. To evaluate the feasibility of LM-Net, extensive
experiments have been conducted on three publicly available datasets with
different modalities. Our proposed model achieves state-of-the-art results,
surpassing previous methods, while only requiring 4.66G FLOPs and 5.4M
parameters. These state-of-the-art results on three datasets with different
modalities demonstrate the effectiveness and adaptability of our proposed
LM-Net for various medical image segmentation tasks.",['cs.CV'],http://arxiv.org/abs/2501.03838v1
Image Segmentation: Inducing graph-based learning,"This study explores the potential of graph neural networks (GNNs) to enhance
semantic segmentation across diverse image modalities. We evaluate the
effectiveness of a novel GNN-based U-Net architecture on three distinct
datasets: PascalVOC, a standard benchmark for natural image segmentation,
WoodScape, a challenging dataset of fisheye images commonly used in autonomous
driving, introducing significant geometric distortions; and ISIC2016, a dataset
of dermoscopic images for skin lesion segmentation. We compare our proposed
UNet-GNN model against established convolutional neural networks (CNNs) based
segmentation models, including U-Net and U-Net++, as well as the
transformer-based SwinUNet. Unlike these methods, which primarily rely on local
convolutional operations or global self-attention, GNNs explicitly model
relationships between image regions by constructing and operating on a graph
representation of the image features. This approach allows the model to capture
long-range dependencies and complex spatial relationships, which we hypothesize
will be particularly beneficial for handling geometric distortions present in
fisheye imagery and capturing intricate boundaries in medical images. Our
analysis demonstrates the versatility of GNNs in addressing diverse
segmentation challenges and highlights their potential to improve segmentation
accuracy in various applications, including autonomous driving and medical
image analysis.","['cs.CV', 'eess.IV']",http://arxiv.org/abs/2501.03765v1
A Review of Bayesian Uncertainty Quantification in Deep Probabilistic Image Segmentation,"Advancements in image segmentation play an integral role within the broad
scope of Deep Learning-based Computer Vision. Furthermore, their widespread
applicability in critical real-world tasks has resulted in challenges related
to the reliability of such algorithms. Hence, uncertainty quantification has
been extensively studied within this context, enabling the expression of model
ignorance (epistemic uncertainty) or data ambiguity (aleatoric uncertainty) to
prevent uninformed decision-making. Due to the rapid adoption of Convolutional
Neural Network (CNN)-based segmentation models in high-stake applications, a
substantial body of research has been published on this very topic, causing its
swift expansion into a distinct field. This work provides a comprehensive
overview of probabilistic segmentation, by discussing fundamental concepts of
uncertainty quantification, governing advancements in the field as well as the
application to various tasks. Moreover, literature on both types of
uncertainties trace back to four key applications: (1) to quantify statistical
inconsistencies in the annotation process due ambiguous images, (2) correlating
prediction error with uncertainty, (3) expanding the model hypothesis space for
better generalization, and (4) Active Learning. An extensive discussion follows
that includes an overview of utilized datasets for each of the applications and
evaluation of the available methods. We also highlight challenges related to
architectures, uncertainty quantification methods, standardization and
benchmarking, and finally end with recommendations for future work such as
methods based on single forward passes and models that appropriately leverage
volumetric data.","['cs.CV', 'cs.AI', 'cs.LG', 'eess.IV', 'stat.ML']",http://arxiv.org/abs/2411.16370v2
CFFormer: Cross CNN-Transformer Channel Attention and Spatial Feature Fusion for Improved Segmentation of Low Quality Medical Images,"Hybrid CNN-Transformer models are designed to combine the advantages of
Convolutional Neural Networks (CNNs) and Transformers to efficiently model both
local information and long-range dependencies. However, most research tends to
focus on integrating the spatial features of CNNs and Transformers, while
overlooking the critical importance of channel features. This is particularly
significant for model performance in low-quality medical image segmentation.
Effective channel feature extraction can significantly enhance the model's
ability to capture contextual information and improve its representation
capabilities. To address this issue, we propose a hybrid CNN-Transformer model,
CFFormer, and introduce two modules: the Cross Feature Channel Attention (CFCA)
module and the X-Spatial Feature Fusion (XFF) module. The model incorporates
dual encoders, with the CNN encoder focusing on capturing local features and
the Transformer encoder modeling global features. The CFCA module filters and
facilitates interactions between the channel features from the two encoders,
while the XFF module effectively reduces the significant semantic information
differences in spatial features, enabling a smooth and cohesive spatial feature
fusion. We evaluate our model across eight datasets covering five modalities to
test its generalization capability. Experimental results demonstrate that our
model outperforms current state-of-the-art (SOTA) methods, with particularly
superior performance on datasets characterized by blurry boundaries and low
contrast.","['cs.CV', 'I.2; I.4; I.5']",http://arxiv.org/abs/2501.03629v1
VOILA: Complexity-Aware Universal Segmentation of CT images by Voxel Interacting with Language,"Satisfactory progress has been achieved recently in universal segmentation of
CT images. Following the success of vision-language methods, there is a growing
trend towards utilizing text prompts and contrastive learning to develop
universal segmentation models. However, there exists a significant imbalance in
information density between 3D images and text prompts. Moreover, the standard
fully connected layer segmentation approach faces significant challenges in
handling multiple classes and exhibits poor generalizability. To address these
challenges, we propose the VOxel Interacting with LAnguage method (VOILA) for
universal CT image segmentation. Initially, we align voxels and language into a
shared representation space and classify voxels on the basis of cosine
similarity. Subsequently, we develop the Voxel-Language Interaction framework
to mitigate the impact of class imbalance caused by foreground-background
discrepancies and variations in target volumes. Furthermore, a Complexity-Aware
Sampling method is proposed to focus on region hard to segment, achieved by
generating pseudo-heatmaps from a trainable Gaussian mixture distribution. Our
results indicate the proposed VOILA is capable to achieve improved performance
with reduced parameters and computational cost during training. Furthermore, it
demonstrates significant generalizability across diverse datasets without
additional fine-tuning.",['cs.CV'],http://arxiv.org/abs/2501.03482v1
Rate-My-LoRA: Efficient and Adaptive Federated Model Tuning for Cardiac MRI Segmentation,"Cardiovascular disease (CVD) and cardiac dyssynchrony are major public health
problems in the United States. Precise cardiac image segmentation is crucial
for extracting quantitative measures that help categorize cardiac dyssynchrony.
However, achieving high accuracy often depends on centralizing large datasets
from different hospitals, which can be challenging due to privacy concerns. To
solve this problem, Federated Learning (FL) is proposed to enable decentralized
model training on such data without exchanging sensitive information. However,
bandwidth limitations and data heterogeneity remain as significant challenges
in conventional FL algorithms. In this paper, we propose a novel efficient and
adaptive federate learning method for cardiac segmentation that improves model
performance while reducing the bandwidth requirement. Our method leverages the
low-rank adaptation (LoRA) to regularize model weight update and reduce
communication overhead. We also propose a \mymethod{} aggregation technique to
address data heterogeneity among clients. This technique adaptively penalizes
the aggregated weights from different clients by comparing the validation
accuracy in each client, allowing better generalization performance and fast
local adaptation. In-client and cross-client evaluations on public cardiac MR
datasets demonstrate the superiority of our method over other LoRA-based
federate learning approaches.","['cs.CV', 'cs.DC', 'cs.LG']",http://arxiv.org/abs/2501.03223v1
AIF-SFDA: Autonomous Information Filter-driven Source-Free Domain Adaptation for Medical Image Segmentation,"Decoupling domain-variant information (DVI) from domain-invariant information
(DII) serves as a prominent strategy for mitigating domain shifts in the
practical implementation of deep learning algorithms. However, in medical
settings, concerns surrounding data collection and privacy often restrict
access to both training and test data, hindering the empirical decoupling of
information by existing methods. To tackle this issue, we propose an Autonomous
Information Filter-driven Source-free Domain Adaptation (AIF-SFDA) algorithm,
which leverages a frequency-based learnable information filter to autonomously
decouple DVI and DII. Information Bottleneck (IB) and Self-supervision (SS) are
incorporated to optimize the learnable frequency filter. The IB governs the
information flow within the filter to diminish redundant DVI, while SS
preserves DII in alignment with the specific task and image modality. Thus, the
autonomous information filter can overcome domain shifts relying solely on
target data. A series of experiments covering various medical image modalities
and segmentation tasks were conducted to demonstrate the benefits of AIF-SFDA
through comparisons with leading algorithms and ablation studies. The code is
available at https://github.com/JingHuaMan/AIF-SFDA.",['cs.CV'],http://arxiv.org/abs/2501.03074v1
Scale-wise Bidirectional Alignment Network for Referring Remote Sensing Image Segmentation,"The goal of referring remote sensing image segmentation (RRSIS) is to extract
specific pixel-level regions within an aerial image via a natural language
expression. Recent advancements, particularly Transformer-based fusion designs,
have demonstrated remarkable progress in this domain. However, existing methods
primarily focus on refining visual features using language-aware guidance
during the cross-modal fusion stage, neglecting the complementary
vision-to-language flow. This limitation often leads to irrelevant or
suboptimal representations. In addition, the diverse spatial scales of ground
objects in aerial images pose significant challenges to the visual perception
capabilities of existing models when conditioned on textual inputs. In this
paper, we propose an innovative framework called Scale-wise Bidirectional
Alignment Network (SBANet) to address these challenges for RRSIS. Specifically,
we design a Bidirectional Alignment Module (BAM) with learnable query tokens to
selectively and effectively represent visual and linguistic features,
emphasizing regions associated with key tokens. BAM is further enhanced with a
dynamic feature selection block, designed to provide both macro- and
micro-level visual features, preserving global context and local details to
facilitate more effective cross-modal interaction. Furthermore, SBANet
incorporates a text-conditioned channel and spatial aggregator to bridge the
gap between the encoder and decoder, enhancing cross-scale information exchange
in complex aerial scenarios. Extensive experiments demonstrate that our
proposed method achieves superior performance in comparison to previous
state-of-the-art methods on the RRSIS-D and RefSegRS datasets, both
quantitatively and qualitatively. The code will be released after publication.",['cs.CV'],http://arxiv.org/abs/2501.00851v2
Comprehensive Pathological Image Segmentation via Teacher Aggregation for Tumor Microenvironment Analysis,"The tumor microenvironment (TME) plays a crucial role in cancer progression
and treatment response, yet current methods for its comprehensive analysis in
H&E-stained tissue slides face significant limitations in the diversity of
tissue cell types and accuracy. Here, we present PAGET (Pathological image
segmentation via AGgrEgated Teachers), a new knowledge distillation approach
that integrates multiple segmentation models while considering the hierarchical
nature of cell types in the TME. By leveraging a unique dataset created through
immunohistochemical restaining techniques and existing segmentation models,
PAGET enables simultaneous identification and classification of 14 key TME
components. We demonstrate PAGET's ability to perform rapid, comprehensive TME
segmentation across various tissue types and medical institutions, advancing
the quantitative analysis of tumor microenvironments. This method represents a
significant step forward in enhancing our understanding of cancer biology and
supporting precise clinical decision-making from large-scale histopathology
images.",['cs.CV'],http://arxiv.org/abs/2501.02909v1
PARF-Net: integrating pixel-wise adaptive receptive fields into hybrid Transformer-CNN network for medical image segmentation,"Convolutional neural networks (CNNs) excel in local feature extraction while
Transformers are superior in processing global semantic information. By
leveraging the strengths of both, hybrid Transformer-CNN networks have become
the major architectures in medical image segmentation tasks. However, existing
hybrid methods still suffer deficient learning of local semantic features due
to the fixed receptive fields of convolutions, and also fall short in
effectively integrating local and long-range dependencies. To address these
issues, we develop a new method PARF-Net to integrate convolutions of
Pixel-wise Adaptive Receptive Fields (Conv-PARF) into hybrid Network for
medical image segmentation. The Conv-PARF is introduced to cope with
inter-pixel semantic differences and dynamically adjust convolutional receptive
fields for each pixel, thus providing distinguishable features to disentangle
the lesions with varying shapes and scales from the background. The features
derived from the Conv-PARF layers are further processed using hybrid
Transformer-CNN blocks under a lightweight manner, to effectively capture local
and long-range dependencies, thus boosting the segmentation performance. By
assessing PARF-Net on four widely used medical image datasets including
MoNuSeg, GlaS, DSB2018 and multi-organ Synapse, we showcase the advantages of
our method over the state-of-the-arts. For instance, PARF-Net achieves 84.27%
mean Dice on the Synapse dataset, surpassing existing methods by a large
margin.",['cs.CV'],http://arxiv.org/abs/2501.02882v1
Interactive 3D Medical Image Segmentation with SAM 2,"Interactive medical image segmentation (IMIS) has shown significant potential
in enhancing segmentation accuracy by integrating iterative feedback from
medical professionals. However, the limited availability of enough 3D medical
data restricts the generalization and robustness of most IMIS methods. The
Segment Anything Model (SAM), though effective for 2D images, requires
expensive semi-auto slice-by-slice annotations for 3D medical images. In this
paper, we explore the zero-shot capabilities of SAM 2, the next-generation Meta
SAM model trained on videos, for 3D medical image segmentation. By treating
sequential 2D slices of 3D images as video frames, SAM 2 can fully
automatically propagate annotations from a single frame to the entire 3D
volume. We propose a practical pipeline for using SAM 2 in 3D medical image
segmentation and present key findings highlighting its efficiency and potential
for further optimization. Concretely, numerical experiments on the BraTS2020
and the medical segmentation decathlon datasets demonstrate that SAM 2 still
has a gap with supervised methods but can narrow the gap in specific settings
and organ types, significantly reducing the annotation burden on medical
professionals. Our code will be open-sourced and available at
https://github.com/Chuyun-Shen/SAM_2_Medical_3D.",['cs.CV'],http://arxiv.org/abs/2408.02635v2
MedSegDiffNCA: Diffusion Models With Neural Cellular Automata for Skin Lesion Segmentation,"Denoising Diffusion Models (DDMs) are widely used for high-quality image
generation and medical image segmentation but often rely on Unet-based
architectures, leading to high computational overhead, especially with
high-resolution images. This work proposes three NCA-based improvements for
diffusion-based medical image segmentation. First, Multi-MedSegDiffNCA uses a
multilevel NCA framework to refine rough noise estimates generated by lower
level NCA models. Second, CBAM-MedSegDiffNCA incorporates channel and spatial
attention for improved segmentation. Third, MultiCBAM-MedSegDiffNCA combines
these methods with a new RGB channel loss for semantic guidance. Evaluations on
Lesion segmentation show that MultiCBAM-MedSegDiffNCA matches Unet-based model
performance with dice score of 87.84% while using 60-110 times fewer
parameters, offering a more efficient solution for low resource medical
settings.","['cs.CV', 'cs.LG', 'eess.IV']",http://arxiv.org/abs/2501.02447v1
Benchmarking Graph Representations and Graph Neural Networks for Multivariate Time Series Classification,"Multivariate Time Series Classification (MTSC) enables the analysis if
complex temporal data, and thus serves as a cornerstone in various real-world
applications, ranging from healthcare to finance. Since the relationship among
variables in MTS usually contain crucial cues, a large number of graph-based
MTSC approaches have been proposed, as the graph topology and edges can
explicitly represent relationships among variables (channels), where not only
various MTS graph representation learning strategies but also different Graph
Neural Networks (GNNs) have been explored. Despite such progresses, there is no
comprehensive study that fairly benchmarks and investigates the performances of
existing widely-used graph representation learning strategies/GNN classifiers
in the application of different MTSC tasks. In this paper, we present the first
benchmark which systematically investigates the effectiveness of the
widely-used three node feature definition strategies, four edge feature
learning strategies and five GNN architecture, resulting in 60 different
variants for graph-based MTSC. These variants are developed and evaluated with
a standardized data pipeline and training/validation/testing strategy on 26
widely-used suspensor MTSC datasets. Our experiments highlight that node
features significantly influence MTSC performance, while the visualization of
edge features illustrates why adaptive edge learning outperforms other edge
feature learning methods. The code of the proposed benchmark is publicly
available at
\url{https://github.com/CVI-yangwn/Benchmark-GNN-for-Multivariate-Time-Series-Classification}.","['cs.LG', '68T10']",http://arxiv.org/abs/2501.08305v2
RELIEF: Reinforcement Learning Empowered Graph Feature Prompt Tuning,"The advent of the ""pre-train, prompt"" paradigm has recently extended its
generalization ability and data efficiency to graph representation learning,
following its achievements in Natural Language Processing (NLP). Initial graph
prompt tuning approaches tailored specialized prompting functions for Graph
Neural Network (GNN) models pre-trained with specific strategies, such as edge
prediction, thus limiting their applicability. In contrast, another pioneering
line of research has explored universal prompting via adding prompts to the
input graph's feature space, thereby removing the reliance on specific
pre-training strategies. However, the necessity to add feature prompts to all
nodes remains an open question. Motivated by findings from prompt tuning
research in the NLP domain, which suggest that highly capable pre-trained
models need less conditioning signal to achieve desired behaviors, we advocate
for strategically incorporating necessary and lightweight feature prompts to
certain graph nodes to enhance downstream task performance. This introduces a
combinatorial optimization problem, requiring a policy to decide 1) which nodes
to prompt and 2) what specific feature prompts to attach. We then address the
problem by framing the prompt incorporation process as a sequential
decision-making problem and propose our method, RELIEF, which employs
Reinforcement Learning (RL) to optimize it. At each step, the RL agent selects
a node (discrete action) and determines the prompt content (continuous action),
aiming to maximize cumulative performance gain. Extensive experiments on graph
and node-level tasks with various pre-training strategies in few-shot scenarios
demonstrate that our RELIEF outperforms fine-tuning and other prompt-based
approaches in classification performance and data efficiency. The code is
available at https://github.com/JasonZhujp/RELIEF.",['cs.LG'],http://arxiv.org/abs/2408.03195v3
Harnessing small projectors and multiple views for efficient vision pretraining,"Recent progress in self-supervised (SSL) visual representation learning has
led to the development of several different proposed frameworks that rely on
augmentations of images but use different loss functions. However, there are
few theoretically grounded principles to guide practice, so practical
implementation of each SSL framework requires several heuristics to achieve
competitive performance. In this work, we build on recent analytical results to
design practical recommendations for competitive and efficient SSL that are
grounded in theory. Specifically, recent theory tells us that existing SSL
frameworks are minimizing the same idealized loss, which is to learn features
that best match the data similarity kernel defined by the augmentations used.
We show how this idealized loss can be reformulated to a functionally
equivalent loss that is more efficient to compute. We study the implicit bias
of using gradient descent to minimize our reformulated loss function and find
that using a stronger orthogonalization constraint with a reduced projector
dimensionality should yield good representations. Furthermore, the theory tells
us that approximating the reformulated loss should be improved by increasing
the number of augmentations, and as such using multiple augmentations should
lead to improved convergence. We empirically verify our findings on CIFAR, STL
and Imagenet datasets, wherein we demonstrate an improved linear readout
performance when training a ResNet-backbone using our theoretically grounded
recommendations. Remarkably, we also demonstrate that by leveraging these
insights, we can reduce the pretraining dataset size by up to 2$\times$ while
maintaining downstream accuracy simply by using more data augmentations. Taken
together, our work provides theoretically grounded recommendations that can be
used to improve SSL convergence and efficiency.","['cs.LG', 'cs.AI', 'cs.CV']",http://arxiv.org/abs/2312.10725v2
Sparse Binary Representation Learning for Knowledge Tracing,"Knowledge tracing (KT) models aim to predict students' future performance
based on their historical interactions. Most existing KT models rely
exclusively on human-defined knowledge concepts (KCs) associated with
exercises. As a result, the effectiveness of these models is highly dependent
on the quality and completeness of the predefined KCs. Human errors in labeling
and the cost of covering all potential underlying KCs can limit model
performance.
  In this paper, we propose a KT model, Sparse Binary Representation KT
(SBRKT), that generates new KC labels, referred to as auxiliary KCs, which can
augment the predefined KCs to address the limitations of relying solely on
human-defined KCs. These are learned through a binary vector representation,
where each bit indicates the presence (one) or absence (zero) of an auxiliary
KC. The resulting discrete representation allows these auxiliary KCs to be
utilized in training any KT model that incorporates KCs. Unlike pre-trained
dense embeddings, which are limited to models designed to accept such vectors,
our discrete representations are compatible with both classical models, such as
Bayesian Knowledge Tracing (BKT), and modern deep learning approaches.
  To generate this discrete representation, SBRKT employs a binarization method
that learns a sparse representation, fully trainable via stochastic gradient
descent. Additionally, SBRKT incorporates a recurrent neural network (RNN) to
capture temporal dynamics and predict future student responses by effectively
combining the auxiliary and predefined KCs. Experimental results demonstrate
that SBRKT outperforms the tested baselines on several datasets and achieves
competitive performance on others. Furthermore, incorporating the learned
auxiliary KCs consistently enhances the performance of BKT across all tested
datasets.",['cs.LG'],http://arxiv.org/abs/2501.09893v1
Latent Space Characterization of Autoencoder Variants,"Understanding the latent spaces learned by deep learning models is crucial in
exploring how they represent and generate complex data. Autoencoders (AEs) have
played a key role in the area of representation learning, with numerous
regularization techniques and training principles developed not only to enhance
their ability to learn compact and robust representations, but also to reveal
how different architectures influence the structure and smoothness of the
lower-dimensional non-linear manifold. We strive to characterize the structure
of the latent spaces learned by different autoencoders including convolutional
autoencoders (CAEs), denoising autoencoders (DAEs), and variational
autoencoders (VAEs) and how they change with the perturbations in the input. By
characterizing the matrix manifolds corresponding to the latent spaces, we
provide an explanation for the well-known observation that the latent spaces of
CAE and DAE form non-smooth manifolds, while that of VAE forms a smooth
manifold. We also map the points of the matrix manifold to a Hilbert space
using distance preserving transforms and provide an alternate view in terms of
the subspaces generated in the Hilbert space as a function of the distortion in
the input. The results show that the latent manifolds of CAE and DAE are
stratified with each stratum being a smooth product manifold, while the
manifold of VAE is a smooth product manifold of two symmetric positive definite
matrices and a symmetric positive semi-definite matrix.","['cs.LG', 'cs.CV', 'cs.IT', 'math.IT']",http://arxiv.org/abs/2412.04755v2
Class Incremental Fault Diagnosis under Limited Fault Data via Supervised Contrastive Knowledge Distillation,"Class-incremental fault diagnosis requires a model to adapt to new fault
classes while retaining previous knowledge. However, limited research exists
for imbalanced and long-tailed data. Extracting discriminative features from
few-shot fault data is challenging, and adding new fault classes often demands
costly model retraining. Moreover, incremental training of existing methods
risks catastrophic forgetting, and severe class imbalance can bias the model's
decisions toward normal classes. To tackle these issues, we introduce a
Supervised Contrastive knowledge distiLlation for class Incremental Fault
Diagnosis (SCLIFD) framework proposing supervised contrastive knowledge
distillation for improved representation learning capability and less
forgetting, a novel prioritized exemplar selection method for sample replay to
alleviate catastrophic forgetting, and the Random Forest Classifier to address
the class imbalance. Extensive experimentation on simulated and real-world
industrial datasets across various imbalance ratios demonstrates the
superiority of SCLIFD over existing approaches. Our code can be found at
https://github.com/Zhang-Henry/SCLIFD_TII.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2501.09525v1
The Devil is in the Details: Simple Remedies for Image-to-LiDAR Representation Learning,"LiDAR is a crucial sensor in autonomous driving, commonly used alongside
cameras. By exploiting this camera-LiDAR setup and recent advances in image
representation learning, prior studies have shown the promising potential of
image-to-LiDAR distillation. These prior arts focus on the designs of their own
losses to effectively distill the pre-trained 2D image representations into a
3D model. However, the other parts of the designs have been surprisingly
unexplored. We find that fundamental design elements, e.g., the LiDAR
coordinate system, quantization according to the existing input interface, and
data utilization, are more critical than developing loss functions, which have
been overlooked in prior works. In this work, we show that simple fixes to
these designs notably outperform existing methods by 16% in 3D semantic
segmentation on the nuScenes dataset and 13% in 3D object detection on the
KITTI dataset in downstream task performance. We focus on overlooked design
choices along the spatial and temporal axes. Spatially, prior work has used
cylindrical coordinate and voxel sizes without considering their side effects
yielded with a commonly deployed sparse convolution layer input interface,
leading to spatial quantization errors in 3D models. Temporally, existing work
has avoided cumbersome data curation by discarding unsynced data, limiting the
use to only the small portion of data that is temporally synced across sensors.
We analyze these effects and propose simple solutions for each overlooked
aspect.",['cs.CV'],http://arxiv.org/abs/2501.09485v1
Towards Robust and Realistic Human Pose Estimation via WiFi Signals,"Robust WiFi-based human pose estimation is a challenging task that bridges
discrete and subtle WiFi signals to human skeletons. This paper revisits this
problem and reveals two critical yet overlooked issues: 1) cross-domain gap,
i.e., due to significant variations between source-target domain pose
distributions; and 2) structural fidelity gap, i.e., predicted skeletal poses
manifest distorted topology, usually with misplaced joints and disproportionate
bone lengths. This paper fills these gaps by reformulating the task into a
novel two-phase framework dubbed DT-Pose: Domain-consistent representation
learning and Topology-constrained Pose decoding. Concretely, we first propose a
temporal-consistent contrastive learning strategy with uniformity
regularization, coupled with self-supervised masking-reconstruction operations,
to enable robust learning of domain-consistent and motion-discriminative
WiFi-specific representations. Beyond this, we introduce a simple yet effective
pose decoder with task prompts, which integrates Graph Convolution Network
(GCN) and Transformer layers to constrain the topology structure of the
generated skeleton by exploring the adjacent-overarching relationships among
human joints. Extensive experiments conducted on various benchmark datasets
highlight the superior performance of our method in tackling these fundamental
challenges in both 2D/3D human pose estimation tasks.",['cs.CV'],http://arxiv.org/abs/2501.09411v1
Strategic Base Representation Learning via Feature Augmentations for Few-Shot Class Incremental Learning,"Few-shot class incremental learning implies the model to learn new classes
while retaining knowledge of previously learned classes with a small number of
training instances. Existing frameworks typically freeze the parameters of the
previously learned classes during the incorporation of new classes. However,
this approach often results in suboptimal class separation of previously
learned classes, leading to overlap between old and new classes. Consequently,
the performance of old classes degrades on new classes. To address these
challenges, we propose a novel feature augmentation driven contrastive learning
framework designed to enhance the separation of previously learned classes to
accommodate new classes. Our approach involves augmenting feature vectors and
assigning proxy labels to these vectors. This strategy expands the feature
space, ensuring seamless integration of new classes within the expanded space.
Additionally, we employ a self-supervised contrastive loss to improve the
separation between previous classes. We validate our framework through
experiments on three FSCIL benchmark datasets: CIFAR100, miniImageNet, and
CUB200. The results demonstrate that our Feature Augmentation driven
Contrastive Learning framework significantly outperforms other approaches,
achieving state-of-the-art performance.",['cs.CV'],http://arxiv.org/abs/2501.09361v1
Discriminative Representation learning via Attention-Enhanced Contrastive Learning for Short Text Clustering,"Contrastive learning has gained significant attention in short text
clustering, yet it has an inherent drawback of mistakenly identifying samples
from the same category as negatives and then separating them in the feature
space (false negative separation), which hinders the generation of superior
representations. To generate more discriminative representations for efficient
clustering, we propose a novel short text clustering method, called
Discriminative Representation learning via \textbf{A}ttention-\textbf{E}nhanced
\textbf{C}ontrastive \textbf{L}earning for Short Text Clustering
(\textbf{AECL}). The \textbf{AECL} consists of two modules which are the
pseudo-label generation module and the contrastive learning module. Both
modules build a sample-level attention mechanism to capture similarity
relationships between samples and aggregate cross-sample features to generate
consistent representations. Then, the former module uses the more
discriminative consistent representation to produce reliable supervision
information for assist clustering, while the latter module explores similarity
relationships and consistent representations optimize the construction of
positive samples to perform similarity-guided contrastive learning, effectively
addressing the false negative separation issue. Experimental results
demonstrate that the proposed \textbf{AECL} outperforms state-of-the-art
methods. If the paper is accepted, we will open-source the code.","['cs.LG', 'cs.CL']",http://arxiv.org/abs/2501.03584v2
Finding the Trigger: Causal Abductive Reasoning on Video Events,"This paper introduces a new problem, Causal Abductive Reasoning on Video
Events (CARVE), which involves identifying causal relationships between events
in a video and generating hypotheses about causal chains that account for the
occurrence of a target event. To facilitate research in this direction, we
create two new benchmark datasets with both synthetic and realistic videos,
accompanied by trigger-target labels generated through a novel counterfactual
synthesis approach. To explore the challenge of solving CARVE, we present a
Causal Event Relation Network (CERN) that examines the relationships between
video events in temporal and semantic spaces to efficiently determine the
root-cause trigger events. Through extensive experiments, we demonstrate the
critical roles of event relational representation learning and interaction
modeling in solving video causal reasoning challenges. The introduction of the
CARVE task, along with the accompanying datasets and the CERN framework, will
advance future research on video causal reasoning and significantly facilitate
various applications, including video surveillance, root-cause analysis and
movie content management.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2501.09304v1
Enhancing Graph Representation Learning with Localized Topological Features,"Representation learning on graphs is a fundamental problem that can be
crucial in various tasks. Graph neural networks, the dominant approach for
graph representation learning, are limited in their representation power.
Therefore, it can be beneficial to explicitly extract and incorporate
high-order topological and geometric information into these models. In this
paper, we propose a principled approach to extract the rich connectivity
information of graphs based on the theory of persistent homology. Our method
utilizes the topological features to enhance the representation learning of
graph neural networks and achieve state-of-the-art performance on various node
classification and link prediction benchmarks. We also explore the option of
end-to-end learning of the topological features, i.e., treating topological
computation as a differentiable operator during learning. Our theoretical
analysis and empirical study provide insights and potential guidelines for
employing topological features in graph learning tasks.","['cs.LG', 'cs.SI']",http://arxiv.org/abs/2501.09178v1
3VL: Using Trees to Improve Vision-Language Models' Interpretability,"Vision-Language models (VLMs) have proven to be effective at aligning image
and text representations, producing superior zero-shot results when transferred
to many downstream tasks. However, these representations suffer from some key
shortcomings in understanding Compositional Language Concepts (CLC), such as
recognizing objects' attributes, states, and relations between different
objects. Moreover, VLMs typically have poor interpretability, making it
challenging to debug and mitigate compositional-understanding failures. In this
work, we introduce the architecture and training technique of Tree-augmented
Vision-Language (3VL) model accompanied by our proposed Anchor inference method
and Differential Relevance (DiRe) interpretability tool. By expanding the text
of an arbitrary image-text pair into a hierarchical tree structure using
language analysis tools, 3VL allows the induction of this structure into the
visual representation learned by the model, enhancing its interpretability and
compositional reasoning. Additionally, we show how Anchor, a simple technique
for text unification, can be used to filter nuisance factors while increasing
CLC understanding performance, e.g., on the fundamental VL-Checklist benchmark.
We also show how DiRe, which performs a differential comparison between VLM
relevancy maps, enables us to generate compelling visualizations of the reasons
for a model's success or failure. Our code is available at:
https://github.com/niryellinek/3VL.",['cs.CV'],http://arxiv.org/abs/2312.17345v2
Self-supervised Transformation Learning for Equivariant Representations,"Unsupervised representation learning has significantly advanced various
machine learning tasks. In the computer vision domain, state-of-the-art
approaches utilize transformations like random crop and color jitter to achieve
invariant representations, embedding semantically the same inputs despite
transformations. However, this can degrade performance in tasks requiring
precise features, such as localization or flower classification. To address
this, recent research incorporates equivariant representation learning, which
captures transformation-sensitive information. However, current methods depend
on transformation labels and thus struggle with interdependency and complex
transformations. We propose Self-supervised Transformation Learning (STL),
replacing transformation labels with transformation representations derived
from image pairs. The proposed method ensures transformation representation is
image-invariant and learns corresponding equivariant transformations, enhancing
performance without increased batch complexity. We demonstrate the approach's
effectiveness across diverse classification and detection tasks, outperforming
existing methods in 7 out of 11 benchmarks and excelling in detection. By
integrating complex transformations like AugMix, unusable by prior equivariant
methods, this approach enhances performance across tasks, underscoring its
adaptability and resilience. Additionally, its compatibility with various base
models highlights its flexibility and broad applicability. The code is
available at https://github.com/jaemyung-u/stl.","['cs.CV', 'cs.AI', 'cs.LG']",http://arxiv.org/abs/2501.08712v1
"Fully Distributed, Flexible Compositional Visual Representations via Soft Tensor Products","Since the inception of the classicalist vs. connectionist debate, it has been
argued that the ability to systematically combine symbol-like entities into
compositional representations is crucial for human intelligence. In
connectionist systems, the field of disentanglement has gained prominence for
its ability to produce explicitly compositional representations; however, it
relies on a fundamentally symbolic, concatenative representation of
compositional structure that clashes with the continuous, distributed
foundations of deep learning. To resolve this tension, we extend Smolensky's
Tensor Product Representation (TPR) and introduce Soft TPR, a representational
form that encodes compositional structure in an inherently distributed,
flexible manner, along with Soft TPR Autoencoder, a theoretically-principled
architecture designed specifically to learn Soft TPRs. Comprehensive
evaluations in the visual representation learning domain demonstrate that the
Soft TPR framework consistently outperforms conventional disentanglement
alternatives -- achieving state-of-the-art disentanglement, boosting
representation learner convergence, and delivering superior sample efficiency
and low-sample regime performance in downstream tasks. These findings highlight
the promise of a distributed and flexible approach to representing
compositional structure by potentially enhancing alignment with the core
principles of deep learning over the conventional symbolic approach.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2412.04671v2
Do Large Language Models Mirror Cognitive Language Processing?,"Large Language Models (LLMs) have demonstrated remarkable abilities in text
comprehension and logical reasoning, indicating that the text representations
learned by LLMs can facilitate their language processing capabilities. In
neuroscience, brain cognitive processing signals are typically utilized to
study human language processing. Therefore, it is natural to ask how well the
text embeddings from LLMs align with the brain cognitive processing signals,
and how training strategies affect the LLM-brain alignment? In this paper, we
employ Representational Similarity Analysis (RSA) to measure the alignment
between 23 mainstream LLMs and fMRI signals of the brain to evaluate how
effectively LLMs simulate cognitive language processing. We empirically
investigate the impact of various factors (e.g., pre-training data size, model
scaling, alignment training, and prompts) on such LLM-brain alignment.
Experimental results indicate that pre-training data size and model scaling are
positively correlated with LLM-brain similarity, and alignment training can
significantly improve LLM-brain similarity. Explicit prompts contribute to the
consistency of LLMs with brain cognitive language processing, while nonsensical
noisy prompts may attenuate such alignment. Additionally, the performance of a
wide range of LLM evaluations (e.g., MMLU, Chatbot Arena) is highly correlated
with the LLM-brain similarity.","['cs.AI', 'cs.CL']",http://arxiv.org/abs/2402.18023v3
DNMDR: Dynamic Networks and Multi-view Drug Representations for Safe Medication Recommendation,"Medication Recommendation (MR) is a promising research topic which booms
diverse applications in the healthcare and clinical domains. However, existing
methods mainly rely on sequential modeling and static graphs for representation
learning, which ignore the dynamic correlations in diverse medical events of a
patient's temporal visits, leading to insufficient global structural
exploration on nodes. Additionally, mitigating drug-drug interactions (DDIs) is
another issue determining the utility of the MR systems. To address the
challenges mentioned above, this paper proposes a novel MR method with the
integration of dynamic networks and multi-view drug representations (DNMDR).
Specifically, weighted snapshot sequences for dynamic heterogeneous networks
are constructed based on discrete visits in temporal EHRs, and all the dynamic
networks are jointly trained to gain both structural correlations in diverse
medical events and temporal dependency in historical health conditions, for
achieving comprehensive patient representations with both semantic features and
structural relationships. Moreover, combining the drug co-occurrences and
adverse drug-drug interactions (DDIs) in internal view of drug molecule
structure and interactive view of drug pairs, the safe drug representations are
available to obtain high-quality medication combination recommendation.
Finally, extensive experiments on real world datasets are conducted for
performance evaluation, and the experimental results demonstrate that the
proposed DNMDR method outperforms the state-of-the-art baseline models with a
large margin on various metrics such as PRAUC, Jaccard, DDI rates and so on.","['cs.LG', 'cs.IR']",http://arxiv.org/abs/2501.08572v1
Learning Cross-Domain Representations for Transferable Drug Perturbations on Single-Cell Transcriptional Responses,"Phenotypic drug discovery has attracted widespread attention because of its
potential to identify bioactive molecules. Transcriptomic profiling provides a
comprehensive reflection of phenotypic changes in cellular responses to
external perturbations. In this paper, we propose XTransferCDR, a novel
generative framework designed for feature decoupling and transferable
representation learning across domains. Given a pair of perturbed expression
profiles, our approach decouples the perturbation representations from basal
states through domain separation encoders and then cross-transfers them in the
latent space. The transferred representations are then used to reconstruct the
corresponding perturbed expression profiles via a shared decoder. This
cross-transfer constraint effectively promotes the learning of transferable
drug perturbation representations. We conducted extensive evaluations of our
model on multiple datasets, including single-cell transcriptional responses to
drugs and single- and combinatorial genetic perturbations. The experimental
results show that XTransferCDR achieved better performance than current
state-of-the-art methods, showcasing its potential to advance phenotypic drug
discovery.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2412.19228v2
Augmentation Invariant Manifold Learning,"Data augmentation is a widely used technique and an essential ingredient in
the recent advance in self-supervised representation learning. By preserving
the similarity between augmented data, the resulting data representation can
improve various downstream analyses and achieve state-of-the-art performance in
many applications. Despite the empirical effectiveness, most existing methods
lack theoretical understanding under a general nonlinear setting. To fill this
gap, we develop a statistical framework on a low-dimension product manifold to
model the data augmentation transformation. Under this framework, we introduce
a new representation learning method called augmentation invariant manifold
learning and design a computationally efficient algorithm by reformulating it
as a stochastic optimization problem. Compared with existing self-supervised
methods, the new method simultaneously exploits the manifold's geometric
structure and invariant property of augmented data and has an explicit
theoretical guarantee. Our theoretical investigation characterizes the role of
data augmentation in the proposed method and reveals why and how the data
representation learned from augmented data can improve the $k$-nearest neighbor
classifier in the downstream analysis, showing that a more complex data
augmentation leads to more improvement in downstream analysis. Finally,
numerical experiments on simulated and real data sets are presented to
demonstrate the merit of the proposed method.","['stat.ML', 'cs.LG', 'math.ST', 'stat.ME', 'stat.TH']",http://arxiv.org/abs/2211.00460v3
Efficient Distribution Matching of Representations via Noise-Injected Deep InfoMax,"Deep InfoMax (DIM) is a well-established method for self-supervised
representation learning (SSRL) based on maximization of the mutual information
between the input and the output of a deep neural network encoder. Despite the
DIM and contrastive SSRL in general being well-explored, the task of learning
representations conforming to a specific distribution (i.e., distribution
matching, DM) is still under-addressed. Motivated by the importance of DM to
several downstream tasks (including generative modeling, disentanglement,
outliers detection and other), we enhance DIM to enable automatic matching of
learned representations to a selected prior distribution. To achieve this, we
propose injecting an independent noise into the normalized outputs of the
encoder, while keeping the same InfoMax training objective. We show that such
modification allows for learning uniformly and normally distributed
representations, as well as representations of other absolutely continuous
distributions. Our approach is tested on various downstream tasks. The results
indicate a moderate trade-off between the performance on the downstream tasks
and quality of DM.","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML', '94A16 (Primary) 68T07, 94A17 (Secondary)', 'E.4; H.1.1']",http://arxiv.org/abs/2410.06993v2
KAN KAN Buff Signed Graph Neural Networks?,"Graph Representation Learning focuses on creating embeddings for nodes and
edges that capture their features and connections. Graph Neural Networks (GNNs)
use neural networks to model complex graph relationships. The Kolmogorov-Arnold
Neural Network (KAN) has recently emerged as an alternative to the Multi-Layer
Perceptron (MLP), offering better accuracy and interpretability with fewer
parameters. KANs have been applied to GNN tasks. This paper introduces the
integration of KANs into Signed Graph Convolutional Networks (SGCNs). We
evaluate KAN-enhanced SGCNs (KASGCN) on signed community detection and link
sign prediction tasks to improve embedding quality in signed networks. While
the results show some variability, KASGCN performs competitively with or
similarly to the standard SGCN in the functions tested. Its effectiveness
depends on the specific context, such as the signed graph and parameter
settings.",['cs.LG'],http://arxiv.org/abs/2501.00709v2
Quilt-1M: One Million Image-Text Pairs for Histopathology,"Recent accelerations in multi-modal applications have been made possible with
the plethora of image and text data available online. However, the scarcity of
analogous data in the medical field, specifically in histopathology, has slowed
comparable progress. To enable similar representation learning for
histopathology, we turn to YouTube, an untapped resource of videos, offering
$1,087$ hours of valuable educational histopathology videos from expert
clinicians. From YouTube, we curate QUILT: a large-scale vision-language
dataset consisting of $802, 144$ image and text pairs. QUILT was automatically
curated using a mixture of models, including large language models, handcrafted
algorithms, human knowledge databases, and automatic speech recognition. In
comparison, the most comprehensive datasets curated for histopathology amass
only around $200$K samples. We combine QUILT with datasets from other sources,
including Twitter, research papers, and the internet in general, to create an
even larger dataset: QUILT-1M, with $1$M paired image-text samples, marking it
as the largest vision-language histopathology dataset to date. We demonstrate
the value of QUILT-1M by fine-tuning a pre-trained CLIP model. Our model
outperforms state-of-the-art models on both zero-shot and linear probing tasks
for classifying new histopathology images across $13$ diverse patch-level
datasets of $8$ different sub-pathologies and cross-modal retrieval tasks.","['cs.CV', 'cs.CL', 'cs.LG']",http://arxiv.org/abs/2306.11207v4
Towards an Information Theoretic Framework of Context-Based Offline Meta-Reinforcement Learning,"As a marriage between offline RL and meta-RL, the advent of offline
meta-reinforcement learning (OMRL) has shown great promise in enabling RL
agents to multi-task and quickly adapt while acquiring knowledge safely. Among
which, context-based OMRL (COMRL) as a popular paradigm, aims to learn a
universal policy conditioned on effective task representations. In this work,
by examining several key milestones in the field of COMRL, we propose to
integrate these seemingly independent methodologies into a unified framework.
Most importantly, we show that the pre-existing COMRL algorithms are
essentially optimizing the same mutual information objective between the task
variable $M$ and its latent representation $Z$ by implementing various
approximate bounds. Such theoretical insight offers ample design freedom for
novel algorithms. As demonstrations, we propose a supervised and a
self-supervised implementation of $I(Z; M)$, and empirically show that the
corresponding optimization algorithms exhibit remarkable generalization across
a broad spectrum of RL benchmarks, context shift scenarios, data qualities and
deep learning architectures. This work lays the information theoretic
foundation for COMRL methods, leading to a better understanding of task
representation learning in the context of reinforcement learning. Given its
generality, we envision our framework as a promising offline pre-training
paradigm of foundation models for decision making.",['cs.LG'],http://arxiv.org/abs/2402.02429v3
Localization-Aware Multi-Scale Representation Learning for Repetitive Action Counting,"Repetitive action counting (RAC) aims to estimate the number of
class-agnostic action occurrences in a video without exemplars. Most current
RAC methods rely on a raw frame-to-frame similarity representation for period
prediction. However, this approach can be significantly disrupted by common
noise such as action interruptions and inconsistencies, leading to sub-optimal
counting performance in realistic scenarios. In this paper, we introduce a
foreground localization optimization objective into similarity representation
learning to obtain more robust and efficient video features. We propose a
Localization-Aware Multi-Scale Representation Learning (LMRL) framework.
Specifically, we apply a Multi-Scale Period-Aware Representation (MPR) with a
scale-specific design to accommodate various action frequencies and learn more
flexible temporal correlations. Furthermore, we introduce the Repetition
Foreground Localization (RFL) method, which enhances the representation by
coarsely identifying periodic actions and incorporating global semantic
information. These two modules can be jointly optimized, resulting in a more
discerning periodic action representation. Our approach significantly reduces
the impact of noise, thereby improving counting accuracy. Additionally, the
framework is designed to be scalable and adaptable to different types of video
content. Experimental results on the RepCountA and UCFRep datasets demonstrate
that our proposed method effectively handles repetitive action counting.",['cs.CV'],http://arxiv.org/abs/2501.07312v1
CureGraph: Contrastive Multi-Modal Graph Representation Learning for Urban Living Circle Health Profiling and Prediction,"The early detection and prediction of health status decline among the elderly
at the neighborhood level are of great significance for urban planning and
public health policymaking. While existing studies affirm the connection
between living environments and health outcomes, most rely on single data
modalities or simplistic feature concatenation of multi-modal information,
limiting their ability to comprehensively profile the health-oriented urban
environments. To fill this gap, we propose CureGraph, a contrastive multi-modal
representation learning framework for urban health prediction that employs
graph-based techniques to infer the prevalence of common chronic diseases among
the elderly within the urban living circles of each neighborhood. CureGraph
leverages rich multi-modal information, including photos and textual reviews of
residential areas and their surrounding points of interest, to generate urban
neighborhood embeddings. By integrating pre-trained visual and textual encoders
with graph modeling techniques, CureGraph captures cross-modal spatial
dependencies, offering a comprehensive understanding of urban environments
tailored to elderly health considerations. Extensive experiments on real-world
datasets demonstrate that CureGraph improves the best baseline by $28\%$ on
average in terms of $R^2$ across elderly disease risk prediction tasks.
Moreover, the model enables the identification of stage-wise chronic disease
progression and supports comparative public health analysis across
neighborhoods, offering actionable insights for sustainable urban development
and enhanced quality of life. The code is publicly available at
https://github.com/jinlin2021/CureGraph.",['cs.AI'],http://arxiv.org/abs/2501.07157v1
Duplex: Dual Prototype Learning for Compositional Zero-Shot Learning,"Compositional Zero-Shot Learning (CZSL) aims to enable models to recognize
novel compositions of visual states and objects that were absent during
training. Existing methods predominantly focus on learning semantic
representations of seen compositions but often fail to disentangle the
independent features of states and objects in images, thereby limiting their
ability to generalize to unseen compositions. To address this challenge, we
propose Duplex, a novel dual-prototype learning method that integrates semantic
and visual prototypes through a carefully designed dual-branch architecture,
enabling effective representation learning for compositional tasks. Duplex
utilizes a Graph Neural Network (GNN) to adaptively update visual prototypes,
capturing complex interactions between states and objects. Additionally, it
leverages the strong visual-semantic alignment of pre-trained Vision-Language
Models (VLMs) and employs a multi-path architecture combined with prompt
engineering to align image and text representations, ensuring robust
generalization. Extensive experiments on three benchmark datasets demonstrate
that Duplex outperforms state-of-the-art methods in both closed-world and
open-world settings.",['cs.CV'],http://arxiv.org/abs/2501.07114v1
Dynamic Multimodal Fusion via Meta-Learning Towards Micro-Video Recommendation,"Multimodal information (e.g., visual, acoustic, and textual) has been widely
used to enhance representation learning for micro-video recommendation. For
integrating multimodal information into a joint representation of micro-video,
multimodal fusion plays a vital role in the existing micro-video recommendation
approaches. However, the static multimodal fusion used in previous studies is
insufficient to model the various relationships among multimodal information of
different micro-videos. In this paper, we develop a novel meta-learning-based
multimodal fusion framework called Meta Multimodal Fusion (MetaMMF), which
dynamically assigns parameters to the multimodal fusion function for each
micro-video during its representation learning. Specifically, MetaMMF regards
the multimodal fusion of each micro-video as an independent task. Based on the
meta information extracted from the multimodal features of the input task,
MetaMMF parameterizes a neural network as the item-specific fusion function via
a meta learner. We perform extensive experiments on three benchmark datasets,
demonstrating the significant improvements over several state-of-the-art
multimodal recommendation models, like MMGCN, LATTICE, and InvRL. Furthermore,
we lighten our model by adopting canonical polyadic decomposition to improve
the training efficiency, and validate its effectiveness through experimental
results. Codes are available at https://github.com/hanliu95/MetaMMF.","['cs.CV', 'cs.IR', 'cs.MM']",http://arxiv.org/abs/2501.07110v1
How GPT learns layer by layer,"Large Language Models (LLMs) excel at tasks like language processing,
strategy games, and reasoning but struggle to build generalizable internal
representations essential for adaptive decision-making in agents. For agents to
effectively navigate complex environments, they must construct reliable world
models. While LLMs perform well on specific benchmarks, they often fail to
generalize, leading to brittle representations that limit their real-world
effectiveness. Understanding how LLMs build internal world models is key to
developing agents capable of consistent, adaptive behavior across tasks. We
analyze OthelloGPT, a GPT-based model trained on Othello gameplay, as a
controlled testbed for studying representation learning. Despite being trained
solely on next-token prediction with random valid moves, OthelloGPT shows
meaningful layer-wise progression in understanding board state and gameplay.
Early layers capture static attributes like board edges, while deeper layers
reflect dynamic tile changes. To interpret these representations, we compare
Sparse Autoencoders (SAEs) with linear probes, finding that SAEs offer more
robust, disentangled insights into compositional features, whereas linear
probes mainly detect features useful for classification. We use SAEs to decode
features related to tile color and tile stability, a previously unexamined
feature that reflects complex gameplay concepts like board control and
long-term planning. We study the progression of linear probe accuracy and tile
color using both SAE's and linear probes to compare their effectiveness at
capturing what the model is learning. Although we begin with a smaller language
model, OthelloGPT, this study establishes a framework for understanding the
internal representations learned by GPT models, transformers, and LLMs more
broadly. Our code is publicly available: https://github.com/ALT-JS/OthelloSAE.",['cs.AI'],http://arxiv.org/abs/2501.07108v1
ADKGD: Anomaly Detection in Knowledge Graphs with Dual-Channel Training,"In the current development of large language models (LLMs), it is important
to ensure the accuracy and reliability of the underlying data sources. LLMs are
critical for various applications, but they often suffer from hallucinations
and inaccuracies due to knowledge gaps in the training data. Knowledge graphs
(KGs), as a powerful structural tool, could serve as a vital external
information source to mitigate the aforementioned issues. By providing a
structured and comprehensive understanding of real-world data, KGs enhance the
performance and reliability of LLMs. However, it is common that errors exist in
KGs while extracting triplets from unstructured data to construct KGs. This
could lead to degraded performance in downstream tasks such as
question-answering and recommender systems. Therefore, anomaly detection in KGs
is essential to identify and correct these errors. This paper presents an
anomaly detection algorithm in knowledge graphs with dual-channel learning
(ADKGD). ADKGD leverages a dual-channel learning approach to enhance
representation learning from both the entity-view and triplet-view
perspectives. Furthermore, using a cross-layer approach, our framework
integrates internal information aggregation and context information
aggregation. We introduce a kullback-leibler (KL)-loss component to improve the
accuracy of the scoring function between the dual channels. To evaluate ADKGD's
performance, we conduct empirical studies on three real-world KGs: WN18RR,
FB15K, and NELL-995. Experimental results demonstrate that ADKGD outperforms
the state-of-the-art anomaly detection algorithms. The source code and datasets
are publicly available at https://github.com/csjywu1/ADKGD.","['cs.AI', 'cs.DB']",http://arxiv.org/abs/2501.07078v1
Representation Learning of Point Cloud Upsampling in Global and Local Inputs,"In recent years, point cloud upsampling has been widely applied in fields
such as 3D reconstruction. Our study investigates the factors influencing point
cloud upsampling on both global and local levels through representation
learning. Specifically, the paper inputs global and local information of the
same point cloud model object into two encoders to extract these features,
fuses them, and then feeds the combined features into an upsampling decoder.
The goal is to address issues of sparsity and noise in point clouds by
leveraging prior knowledge from both global and local inputs. And the proposed
framework can be applied to any state-of-the-art point cloud upsampling neural
network. Experiments were conducted on a series of autoencoder-based models
utilizing deep learning, yielding interpretability for both global and local
inputs, and it has been proven in the results that our proposed framework can
further improve the upsampling effect in previous SOTA works. At the same time,
the Saliency Map reflects the differences between global and local feature
inputs, as well as the effectiveness of training with both inputs in parallel.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.07076v1
ACCon: Angle-Compensated Contrastive Regularizer for Deep Regression,"In deep regression, capturing the relationship among continuous labels in
feature space is a fundamental challenge that has attracted increasing
interest. Addressing this issue can prevent models from converging to
suboptimal solutions across various regression tasks, leading to improved
performance, especially for imbalanced regression and under limited sample
sizes. However, existing approaches often rely on order-aware representation
learning or distance-based weighting. In this paper, we hypothesize a linear
negative correlation between label distances and representation similarities in
regression tasks. To implement this, we propose an angle-compensated
contrastive regularizer for deep regression, which adjusts the cosine distance
between anchor and negative samples within the contrastive learning framework.
Our method offers a plug-and-play compatible solution that extends most
existing contrastive learning methods for regression tasks. Extensive
experiments and theoretical analysis demonstrate that our proposed
angle-compensated contrastive regularizer not only achieves competitive
regression performance but also excels in data efficiency and effectiveness on
imbalanced datasets.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2501.07045v1
Scam Detection for Ethereum Smart Contracts: Leveraging Graph Representation Learning for Secure Blockchain,"Due to the increasing abuse of fraudulent activities that result in
significant financial and reputational harm, Ethereum smart contracts face a
significant problem in detecting fraud. Existing monitoring methods typically
rely on lease code analysis or physically extracted features, which suffer from
scalability and adaptability limitations. In this study, we use graph
representation learning to observe purchase trends and find fraudulent deals.
We can achieve powerful categorisation performance by using innovative machine
learning versions and transforming Ethereum invoice data into graph structures.
Our method addresses label imbalance through SMOTE-ENN techniques and evaluates
models like Multi-Layer Perceptron ( MLP ) and Graph Convolutional Networks (
GCN). Experimental results show that the MLP type surpasses the GCN in this
environment, with domain-specific assessments closely aligned with real-world
assessments. This study provides a scalable and efficient way to improve
Ethereum's ecosystem's confidence and security.","['cs.LG', 'cs.AI', 'cs.CR', 'cs.DC', 'cs.SI', 'I.2.1']",http://arxiv.org/abs/2412.12370v3
A Structure-Aware Framework for Learning Device Placements on Computation Graphs,"Computation graphs are Directed Acyclic Graphs (DAGs) where the nodes
correspond to mathematical operations and are used widely as abstractions in
optimizations of neural networks. The device placement problem aims to identify
optimal allocations of those nodes to a set of (potentially heterogeneous)
devices. Existing approaches rely on two types of architectures known as
grouper-placer and encoder-placer, respectively. In this work, we bridge the
gap between encoder-placer and grouper-placer techniques and propose a novel
framework for the task of device placement, relying on smaller computation
graphs extracted from the OpenVINO toolkit. The framework consists of five
steps, including graph coarsening, node representation learning and policy
optimization. It facilitates end-to-end training and takes into account the DAG
nature of the computation graphs. We also propose a model variant, inspired by
graph parsing networks and complex network analysis, enabling graph
representation learning and jointed, personalized graph partitioning, using an
unspecified number of groups. To train the entire framework, we use
reinforcement learning using the execution time of the placement as a reward.
We demonstrate the flexibility and effectiveness of our approach through
multiple experiments with three benchmark models, namely Inception-V3, ResNet,
and BERT. The robustness of the proposed framework is also highlighted through
an ablation study. The suggested placements improve the inference speed for the
benchmark models by up to 58.2% over CPU execution and by up to 60.24% compared
to other commonly used baselines.","['cs.LG', 'cs.PF']",http://arxiv.org/abs/2405.14185v2
Dual-Modality Representation Learning for Molecular Property Prediction,"Molecular property prediction has attracted substantial attention recently.
Accurate prediction of drug properties relies heavily on effective molecular
representations. The structures of chemical compounds are commonly represented
as graphs or SMILES sequences. Recent advances in learning drug properties
commonly employ Graph Neural Networks (GNNs) based on the graph representation.
For the SMILES representation, Transformer-based architectures have been
adopted by treating each SMILES string as a sequence of tokens. Because each
representation has its own advantages and disadvantages, combining both
representations in learning drug properties is a promising direction. We
propose a method named Dual-Modality Cross-Attention (DMCA) that can
effectively combine the strengths of two representations by employing the
cross-attention mechanism. DMCA was evaluated across eight datasets including
both classification and regression tasks. Results show that our method achieves
the best overall performance, highlighting its effectiveness in leveraging the
complementary information from both graph and SMILES modalities.","['cs.LG', 'q-bio.QM']",http://arxiv.org/abs/2501.06608v1
Natural Language Supervision for Low-light Image Enhancement,"With the development of deep learning, numerous methods for low-light image
enhancement (LLIE) have demonstrated remarkable performance. Mainstream LLIE
methods typically learn an end-to-end mapping based on pairs of low-light and
normal-light images. However, normal-light images under varying illumination
conditions serve as reference images, making it difficult to define a
``perfect'' reference image This leads to the challenge of reconciling
metric-oriented and visual-friendly results. Recently, many cross-modal studies
have found that side information from other related modalities can guide visual
representation learning. Based on this, we introduce a Natural Language
Supervision (NLS) strategy, which learns feature maps from text corresponding
to images, offering a general and flexible interface for describing an image
under different illumination.
  However, image distributions conditioned on textual descriptions are highly
multimodal, which makes training difficult. To address this issue, we design a
Textual Guidance Conditioning Mechanism (TCM) that incorporates the connections
between image regions and sentence words, enhancing the ability to capture
fine-grained cross-modal cues for images and text. This strategy not only
utilizes a wider range of supervised sources, but also provides a new paradigm
for LLIE based on visual and textual feature alignment. In order to effectively
identify and merge features from various levels of image and textual
information, we design an Information Fusion Attention (IFA) module to enhance
different regions at different levels. We integrate the proposed TCM and IFA
into a Natural Language Supervision network for LLIE, named NaLSuper. Finally,
extensive experiments demonstrate the robustness and superior effectiveness of
our proposed NaLSuper.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.06546v1
Multi-View Factorizing and Disentangling: A Novel Framework for Incomplete Multi-View Multi-Label Classification,"Multi-view multi-label classification (MvMLC) has recently garnered
significant research attention due to its wide range of real-world
applications. However, incompleteness in views and labels is a common
challenge, often resulting from data collection oversights and uncertainties in
manual annotation. Furthermore, the task of learning robust multi-view
representations that are both view-consistent and view-specific from diverse
views still a challenge problem in MvMLC. To address these issues, we propose a
novel framework for incomplete multi-view multi-label classification (iMvMLC).
Our method factorizes multi-view representations into two independent sets of
factors: view-consistent and view-specific, and we correspondingly design a
graph disentangling loss to fully reduce redundancy between these
representations. Additionally, our framework innovatively decomposes consistent
representation learning into three key sub-objectives: (i) how to extract
view-shared information across different views, (ii) how to eliminate
intra-view redundancy in consistent representations, and (iii) how to preserve
task-relevant information. To this end, we design a robust task-relevant
consistency learning module that collaboratively learns high-quality consistent
representations, leveraging a masked cross-view prediction (MCP) strategy and
information theory. Notably, all modules in our framework are developed to
function effectively under conditions of incomplete views and labels, making
our method adaptable to various multi-view and multi-label datasets. Extensive
experiments on five datasets demonstrate that our method outperforms other
leading approaches.",['cs.CV'],http://arxiv.org/abs/2501.06524v1
NVS-SQA: Exploring Self-Supervised Quality Representation Learning for Neurally Synthesized Scenes without References,"Neural View Synthesis (NVS), such as NeRF and 3D Gaussian Splatting,
effectively creates photorealistic scenes from sparse viewpoints, typically
evaluated by quality assessment methods like PSNR, SSIM, and LPIPS. However,
these full-reference methods, which compare synthesized views to reference
views, may not fully capture the perceptual quality of neurally synthesized
scenes (NSS), particularly due to the limited availability of dense reference
views. Furthermore, the challenges in acquiring human perceptual labels hinder
the creation of extensive labeled datasets, risking model overfitting and
reduced generalizability. To address these issues, we propose NVS-SQA, a NSS
quality assessment method to learn no-reference quality representations through
self-supervision without reliance on human labels. Traditional self-supervised
learning predominantly relies on the ""same instance, similar representation""
assumption and extensive datasets. However, given that these conditions do not
apply in NSS quality assessment, we employ heuristic cues and quality scores as
learning objectives, along with a specialized contrastive pair preparation
process to improve the effectiveness and efficiency of learning. The results
show that NVS-SQA outperforms 17 no-reference methods by a large margin (i.e.,
on average 109.5% in SRCC, 98.6% in PLCC, and 91.5% in KRCC over the second
best) and even exceeds 16 full-reference methods across all evaluation metrics
(i.e., 22.9% in SRCC, 19.1% in PLCC, and 18.6% in KRCC over the second best).","['cs.CV', 'cs.AI', 'cs.HC', 'cs.MM', 'eess.IV']",http://arxiv.org/abs/2501.06488v1
Comparing Self-Supervised Learning Models Pre-Trained on Human Speech and Animal Vocalizations for Bioacoustics Processing,"Self-supervised learning (SSL) foundation models have emerged as powerful,
domain-agnostic, general-purpose feature extractors applicable to a wide range
of tasks. Such models pre-trained on human speech have demonstrated high
transferability for bioacoustic processing. This paper investigates (i) whether
SSL models pre-trained directly on animal vocalizations offer a significant
advantage over those pre-trained on speech, and (ii) whether fine-tuning
speech-pretrained models on automatic speech recognition (ASR) tasks can
enhance bioacoustic classification. We conduct a comparative analysis using
three diverse bioacoustic datasets and two different bioacoustic tasks. Results
indicate that pre-training on bioacoustic data provides only marginal
improvements over speech-pretrained models, with comparable performance in most
scenarios. Fine-tuning on ASR tasks yields mixed outcomes, suggesting that the
general-purpose representations learned during SSL pre-training are already
well-suited for bioacoustic tasks. These findings highlight the robustness of
speech-pretrained SSL models for bioacoustics and imply that extensive
fine-tuning may not be necessary for optimal performance.","['cs.LG', 'eess.AS']",http://arxiv.org/abs/2501.05987v1
MiM: Mask in Mask Self-Supervised Pre-Training for 3D Medical Image Analysis,"The Vision Transformer (ViT) has demonstrated remarkable performance in
Self-Supervised Learning (SSL) for 3D medical image analysis. Masked
AutoEncoder (MAE) for feature pre-training can further unleash the potential of
ViT on various medical vision tasks. However, due to large spatial sizes with
much higher dimensions of 3D medical images, the lack of hierarchical design
for MAE may hinder the performance of downstream tasks. In this paper, we
propose a novel \textit{Mask in Mask (MiM)} pre-training framework for 3D
medical images, which aims to advance MAE by learning discriminative
representation from hierarchical visual tokens across varying scales. We
introduce multiple levels of granularity for masked inputs from the volume,
which are then reconstructed simultaneously ranging at both fine and coarse
levels. Additionally, a cross-level alignment mechanism is applied to adjacent
level volumes to enforce anatomical similarity hierarchically. Furthermore, we
adopt a hybrid backbone to enhance the hierarchical representation learning
efficiently during the pre-training. MiM was pre-trained on a large scale of
available 3D volumetric images, \textit{i.e.,} Computed Tomography (CT) images
containing various body parts. Extensive experiments on thirteen public
datasets demonstrate the superiority of MiM over other SSL methods in
organ/lesion/tumor segmentation and disease classification. We further scale up
the MiM to large pre-training datasets with more than 10k volumes, showing that
large-scale pre-training can further enhance the performance of downstream
tasks. The improvement also concluded that the research community should pay
more attention to the scale of the pre-training dataset towards the healthcare
foundation model for 3D medical images.",['cs.CV'],http://arxiv.org/abs/2404.15580v2
DPCL-Diff: The Temporal Knowledge Graph Reasoning Based on Graph Node Diffusion Model with Dual-Domain Periodic Contrastive Learning,"Temporal knowledge graph (TKG) reasoning that infers future missing facts is
an essential and challenging task. Predicting future events typically relies on
closely related historical facts, yielding more accurate results for repetitive
or periodic events. However, for future events with sparse historical
interactions, the effectiveness of this method, which focuses on leveraging
high-frequency historical information, diminishes. Recently, the capabilities
of diffusion models in image generation have opened new opportunities for TKG
reasoning. Therefore, we propose a graph node diffusion model with dual-domain
periodic contrastive learning (DPCL-Diff). Graph node diffusion model (GNDiff)
introduces noise into sparsely related events to simulate new events,
generating high-quality data that better conforms to the actual distribution.
This generative mechanism significantly enhances the model's ability to reason
about new events. Additionally, the dual-domain periodic contrastive learning
(DPCL) maps periodic and non-periodic event entities to Poincar\'e and
Euclidean spaces, leveraging their characteristics to distinguish similar
periodic events effectively. Experimental results on four public datasets
demonstrate that DPCL-Diff significantly outperforms state-of-the-art TKG
models in event prediction, demonstrating our approach's effectiveness. This
study also investigates the combined effectiveness of GNDiff and DPCL in TKG
tasks.","['cs.LG', 'cs.CL']",http://arxiv.org/abs/2411.01477v2
Isolated Diffusion: Optimizing Multi-Concept Text-to-Image Generation Training-Freely with Isolated Diffusion Guidance,"Large-scale text-to-image diffusion models have achieved great success in
synthesizing high-quality and diverse images given target text prompts. Despite
the revolutionary image generation ability, current state-of-the-art models
still struggle to deal with multi-concept generation accurately in many cases.
This phenomenon is known as ``concept bleeding"" and displays as the unexpected
overlapping or merging of various concepts. This paper presents a general
approach for text-to-image diffusion models to address the mutual interference
between different subjects and their attachments in complex scenes, pursuing
better text-image consistency. The core idea is to isolate the synthesizing
processes of different concepts. We propose to bind each attachment to
corresponding subjects separately with split text prompts. Besides, we
introduce a revision method to fix the concept bleeding problem in
multi-subject synthesis. We first depend on pre-trained object detection and
segmentation models to obtain the layouts of subjects. Then we isolate and
resynthesize each subject individually with corresponding text prompts to avoid
mutual interference. Overall, we achieve a training-free strategy, named
Isolated Diffusion, to optimize multi-concept text-to-image synthesis. It is
compatible with the latest Stable Diffusion XL (SDXL) and prior Stable
Diffusion (SD) models. We compare our approach with alternative methods using a
variety of multi-concept text prompts and demonstrate its effectiveness with
clear advantages in text-image consistency and user study.",['cs.CV'],http://arxiv.org/abs/2403.16954v2
DiffVSR: Enhancing Real-World Video Super-Resolution with Diffusion Models for Advanced Visual Quality and Temporal Consistency,"Diffusion models have demonstrated exceptional capabilities in image
generation and restoration, yet their application to video super-resolution
faces significant challenges in maintaining both high fidelity and temporal
consistency. We present DiffVSR, a diffusion-based framework for real-world
video super-resolution that effectively addresses these challenges through key
innovations. For intra-sequence coherence, we develop a multi-scale temporal
attention module and temporal-enhanced VAE decoder that capture fine-grained
motion details. To ensure inter-sequence stability, we introduce a noise
rescheduling mechanism with an interweaved latent transition approach, which
enhances temporal consistency without additional training overhead. We propose
a progressive learning strategy that transitions from simple to complex
degradations, enabling robust optimization despite limited high-quality video
data. Extensive experiments demonstrate that DiffVSR delivers superior results
in both visual quality and temporal consistency, setting a new performance
standard in real-world video super-resolution.",['cs.CV'],http://arxiv.org/abs/2501.10110v1
IE-Bench: Advancing the Measurement of Text-Driven Image Editing for Human Perception Alignment,"Recent advances in text-driven image editing have been significant, yet the
task of accurately evaluating these edited images continues to pose a
considerable challenge. Different from the assessment of text-driven image
generation, text-driven image editing is characterized by simultaneously
conditioning on both text and a source image. The edited images often retain an
intrinsic connection to the original image, which dynamically change with the
semantics of the text. However, previous methods tend to solely focus on
text-image alignment or have not aligned with human perception. In this work,
we introduce the Text-driven Image Editing Benchmark suite (IE-Bench) to
enhance the assessment of text-driven edited images. IE-Bench includes a
database contains diverse source images, various editing prompts and the
corresponding results different editing methods, and total 3,010 Mean Opinion
Scores (MOS) provided by 25 human subjects. Furthermore, we introduce IE-QA, a
multi-modality source-aware quality assessment method for text-driven image
editing. To the best of our knowledge, IE-Bench offers the first IQA dataset
and model tailored for text-driven image editing. Extensive experiments
demonstrate IE-QA's superior subjective-alignments on the text-driven image
editing task compared with previous metrics. We will make all related data and
code available to the public.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.09927v1
PIXELS: Progressive Image Xemplar-based Editing with Latent Surgery,"Recent advancements in language-guided diffusion models for image editing are
often bottle-necked by cumbersome prompt engineering to precisely articulate
desired changes. An intuitive alternative calls on guidance from in-the-wild
image exemplars to help users bring their imagined edits to life. Contemporary
exemplar-based editing methods shy away from leveraging the rich latent space
learnt by pre-existing large text-to-image (TTI) models and fall back on
training with curated objective functions to achieve the task. Though somewhat
effective, this demands significant computational resources and lacks
compatibility with diverse base models and arbitrary exemplar count. On further
investigation, we also find that these techniques restrict user control to only
applying uniform global changes over the entire edited region. In this paper,
we introduce a novel framework for progressive exemplar-driven editing with
off-the-shelf diffusion models, dubbed PIXELS, to enable customization by
providing granular control over edits, allowing adjustments at the pixel or
region level. Our method operates solely during inference to facilitate
imitative editing, enabling users to draw inspiration from a dynamic number of
reference images, or multimodal prompts, and progressively incorporate all the
desired changes without retraining or fine-tuning existing TTI models. This
capability of fine-grained control opens up a range of new possibilities,
including selective modification of individual objects and specifying gradual
spatial changes. We demonstrate that PIXELS delivers high-quality edits
efficiently, leading to a notable improvement in quantitative metrics as well
as human evaluation. By making high-quality image editing more accessible,
PIXELS has the potential to enable professional-grade edits to a wider audience
with the ease of using any open-source image generation model.",['cs.CV'],http://arxiv.org/abs/2501.09826v1
Learnings from Scaling Visual Tokenizers for Reconstruction and Generation,"Visual tokenization via auto-encoding empowers state-of-the-art image and
video generative models by compressing pixels into a latent space. Although
scaling Transformer-based generators has been central to recent advances, the
tokenizer component itself is rarely scaled, leaving open questions about how
auto-encoder design choices influence both its objective of reconstruction and
downstream generative performance. Our work aims to conduct an exploration of
scaling in auto-encoders to fill in this blank. To facilitate this exploration,
we replace the typical convolutional backbone with an enhanced Vision
Transformer architecture for Tokenization (ViTok). We train ViTok on
large-scale image and video datasets far exceeding ImageNet-1K, removing data
constraints on tokenizer scaling. We first study how scaling the auto-encoder
bottleneck affects both reconstruction and generation -- and find that while it
is highly correlated with reconstruction, its relationship with generation is
more complex. We next explored the effect of separately scaling the
auto-encoders' encoder and decoder on reconstruction and generation
performance. Crucially, we find that scaling the encoder yields minimal gains
for either reconstruction or generation, while scaling the decoder boosts
reconstruction but the benefits for generation are mixed. Building on our
exploration, we design ViTok as a lightweight auto-encoder that achieves
competitive performance with state-of-the-art auto-encoders on ImageNet-1K and
COCO reconstruction tasks (256p and 512p) while outperforming existing
auto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x
fewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates
competitive performance on image generation for ImageNet-1K and sets new
state-of-the-art benchmarks for class-conditional video generation on UCF-101.","['cs.CV', 'cs.AI', 'I.2.10; I.4.2; I.4.5']",http://arxiv.org/abs/2501.09755v1
Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps,"Generative models have made significant impacts across various domains,
largely due to their ability to scale during training by increasing data,
computational resources, and model size, a phenomenon characterized by the
scaling laws. Recent research has begun to explore inference-time scaling
behavior in Large Language Models (LLMs), revealing how performance can further
improve with additional computation during inference. Unlike LLMs, diffusion
models inherently possess the flexibility to adjust inference-time computation
via the number of denoising steps, although the performance gains typically
flatten after a few dozen. In this work, we explore the inference-time scaling
behavior of diffusion models beyond increasing denoising steps and investigate
how the generation performance can further improve with increased computation.
Specifically, we consider a search problem aimed at identifying better noises
for the diffusion sampling process. We structure the design space along two
axes: the verifiers used to provide feedback, and the algorithms used to find
better noise candidates. Through extensive experiments on class-conditioned and
text-conditioned image generation benchmarks, our findings reveal that
increasing inference-time compute leads to substantial improvements in the
quality of samples generated by diffusion models, and with the complicated
nature of images, combinations of the components in the framework can be
specifically chosen to conform with different application scenario.",['cs.CV'],http://arxiv.org/abs/2501.09732v1
AnyStory: Towards Unified Single and Multiple Subject Personalization in Text-to-Image Generation,"Recently, large-scale generative models have demonstrated outstanding
text-to-image generation capabilities. However, generating high-fidelity
personalized images with specific subjects still presents challenges,
especially in cases involving multiple subjects. In this paper, we propose
AnyStory, a unified approach for personalized subject generation. AnyStory not
only achieves high-fidelity personalization for single subjects, but also for
multiple subjects, without sacrificing subject fidelity. Specifically, AnyStory
models the subject personalization problem in an ""encode-then-route"" manner. In
the encoding step, AnyStory utilizes a universal and powerful image encoder,
i.e., ReferenceNet, in conjunction with CLIP vision encoder to achieve
high-fidelity encoding of subject features. In the routing step, AnyStory
utilizes a decoupled instance-aware subject router to accurately perceive and
predict the potential location of the corresponding subject in the latent
space, and guide the injection of subject conditions. Detailed experimental
results demonstrate the excellent performance of our method in retaining
subject details, aligning text descriptions, and personalizing for multiple
subjects. The project page is at https://aigcdesigngroup.github.io/AnyStory/ .",['cs.CV'],http://arxiv.org/abs/2501.09503v1
TextureCrop: Enhancing Synthetic Image Detection through Texture-based Cropping,"Generative AI technologies produce increasingly realistic imagery, which,
despite its potential for creative applications, can also be misused to produce
misleading and harmful content. This renders Synthetic Image Detection (SID)
methods essential for identifying AI-generated content online. State-of-the-art
SID methods typically resize or center-crop input images due to architectural
or computational constraints, which hampers the detection of artifacts that
appear in high-resolution images. To address this limitation, we propose
TextureCrop, an image pre-processing component that can be plugged in any
pre-trained SID model to improve its performance. By focusing on high-frequency
image parts where generative artifacts are prevalent, TextureCrop enhances SID
performance with manageable memory requirements. Experimental results
demonstrate a consistent improvement in AUC across various detectors by 6.1%
compared to center cropping and by 15% compared to resizing, across
high-resolution images from the Forensynths, Synthbuster and TWIGMA datasets.
Code available at https : //github.com/mever-team/texture-crop.",['cs.CV'],http://arxiv.org/abs/2407.15500v4
Dynamic Neural Style Transfer for Artistic Image Generation using VGG19,"Throughout history, humans have created remarkable works of art, but
artificial intelligence has only recently started to make strides in generating
visually compelling art. Breakthroughs in the past few years have focused on
using convolutional neural networks (CNNs) to separate and manipulate the
content and style of images, applying texture synthesis techniques.
Nevertheless, a number of current techniques continue to encounter obstacles,
including lengthy processing times, restricted choices of style images, and the
inability to modify the weight ratio of styles. We proposed a neural style
transfer system that can add various artistic styles to a desired image to
address these constraints allowing flexible adjustments to style weight ratios
and reducing processing time. The system uses the VGG19 model for feature
extraction, ensuring high-quality, flexible stylization without compromising
content integrity.","['cs.CV', 'cs.AI', 'cs.LG', 'eess.IV']",http://arxiv.org/abs/2501.09420v1
SVIA: A Street View Image Anonymization Framework for Self-Driving Applications,"In recent years, there has been an increasing interest in image
anonymization, particularly focusing on the de-identification of faces and
individuals. However, for self-driving applications, merely de-identifying
faces and individuals might not provide sufficient privacy protection since
street views like vehicles and buildings can still disclose locations,
trajectories, and other sensitive information. Therefore, it remains crucial to
extend anonymization techniques to street view images to fully preserve the
privacy of users, pedestrians, and vehicles. In this paper, we propose a Street
View Image Anonymization (SVIA) framework for self-driving applications. The
SVIA framework consists of three integral components: a semantic segmenter to
segment an input image into functional regions, an inpainter to generate
alternatives to privacy-sensitive regions, and a harmonizer to seamlessly
stitch modified regions to guarantee visual coherence. Compared to existing
methods, SVIA achieves a much better trade-off between image generation quality
and privacy protection, as evidenced by experimental results for five common
metrics on two widely used public datasets.",['cs.CV'],http://arxiv.org/abs/2501.09393v1
Grounding Text-To-Image Diffusion Models For Controlled High-Quality Image Generation,"Large-scale text-to-image (T2I) diffusion models have demonstrated an
outstanding performance in synthesizing diverse high-quality visuals from
natural language text captions. Multiple layout-to-image models have been
developed to control the generation process by utilizing a broad array of
layouts such as segmentation maps, edges, and human keypoints. In this work, we
present ObjectDiffusion, a model that takes inspirations from the top
cutting-edge image generative frameworks to seamlessly condition T2I models
with new bounding boxes capabilities. Specifically, we make substantial
modifications to the network architecture introduced in ContorlNet to integrate
it with the condition processing and injection techniques proposed in GLIGEN.
ObjectDiffusion is initialized with pretraining parameters to leverage the
generation knowledge obtained from training on large-scale datasets. We
fine-tune ObjectDiffusion on the COCO2017 training dataset and evaluate it on
the COCO2017 validation dataset. Our model achieves an AP$_{50}$ of 46.6, an AR
of 44.5, and a FID of 19.8 outperforming the current SOTA model trained on
open-source datasets in all of the three metrics. ObjectDiffusion demonstrates
a distinctive capability in synthesizing diverse, high-quality, high-fidelity
images that seamlessly conform to the semantic and spatial control layout.
Evaluated in qualitative and quantitative tests, ObjectDiffusion exhibits
remarkable grounding abilities on closed-set and open-set settings across a
wide variety of contexts. The qualitative assessment verifies the ability of
ObjectDiffusion to generate multiple objects of different sizes and locations.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.09194v1
Multimodal LLMs Can Reason about Aesthetics in Zero-Shot,"We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability
shall be elicited to evaluate the aesthetics of artworks. To facilitate this
investigation, we construct MM-StyleBench, a novel high-quality dataset for
benchmarking artistic stylization. We then develop a principled method for
human preference modeling and perform a systematic correlation analysis between
MLLMs' responses and human preference. Our experiments reveal an inherent
hallucination issue of MLLMs in art evaluation, associated with response
subjectivity. ArtCoT is proposed, demonstrating that art-specific task
decomposition and the use of concrete language boost MLLMs' reasoning ability
for aesthetics. Our findings offer valuable insights into MLLMs for art and can
benefit a wide range of downstream applications, such as style transfer and
artistic image generation. Code available at
https://github.com/songrise/MLLM4Art.","['cs.CV', 'cs.AI', 'cs.CL', 'cs.MM']",http://arxiv.org/abs/2501.09012v1
SHYI: Action Support for Contrastive Learning in High-Fidelity Text-to-Image Generation,"In this project, we address the issue of infidelity in text-to-image
generation, particularly for actions involving multiple objects. For this we
build on top of the CONFORM framework which uses Contrastive Learning to
improve the accuracy of the generated image for multiple objects. However the
depiction of actions which involves multiple different object has still large
room for improvement. To improve, we employ semantically hypergraphic
contrastive adjacency learning, a comprehension of enhanced contrastive
structure and ""contrast but link"" technique. We further amend Stable
Diffusion's understanding of actions by InteractDiffusion. As evaluation
metrics we use image-text similarity CLIP and TIFA. In addition, we conducted a
user study.
  Our method shows promising results even with verbs that Stable Diffusion
understands mediocrely. We then provide future directions by analyzing the
results.
  Our codebase can be found on polybox under the link:
https://polybox.ethz.ch/index.php/s/dJm3SWyRohUrFxn",['cs.CV'],http://arxiv.org/abs/2501.09055v1
Identifying Spurious Correlations using Counterfactual Alignment,"Models driven by spurious correlations often yield poor generalization
performance. We propose the counterfactual (CF) alignment method to detect and
quantify spurious correlations of black box classifiers. Our methodology is
based on counterfactual images generated with respect to one classifier being
input into other classifiers to see if they also induce changes in the outputs
of these classifiers. The relationship between these responses can be
quantified and used to identify specific instances where a spurious correlation
exists. This is validated by observing intuitive trends in face-attribute and
waterbird classifiers, as well as by fabricating spurious correlations and
detecting their presence, both visually and quantitatively. Furthermore,
utilizing the CF alignment method, we demonstrate that we can evaluate robust
optimization methods (GroupDRO, JTT, and FLAC) by detecting a reduction in
spurious correlations.","['cs.CV', 'cs.AI', 'cs.LG']",http://arxiv.org/abs/2312.02186v3
Enhanced Multi-Scale Cross-Attention for Person Image Generation,"In this paper, we propose a novel cross-attention-based generative
adversarial network (GAN) for the challenging person image generation task.
Cross-attention is a novel and intuitive multi-modal fusion method in which an
attention/correlation matrix is calculated between two feature maps of
different modalities. Specifically, we propose the novel XingGAN (or
CrossingGAN), which consists of two generation branches that capture the
person's appearance and shape, respectively. Moreover, we propose two novel
cross-attention blocks to effectively transfer and update the person's shape
and appearance embeddings for mutual improvement. This has not been considered
by any other existing GAN-based image generation work. To further learn the
long-range correlations between different person poses at different scales and
sub-regions, we propose two novel multi-scale cross-attention blocks. To tackle
the issue of independent correlation computations within the cross-attention
mechanism leading to noisy and ambiguous attention weights, which hinder
performance improvements, we propose a module called enhanced attention (EA).
Lastly, we introduce a novel densely connected co-attention module to fuse
appearance and shape features at different stages effectively. Extensive
experiments on two public datasets demonstrate that the proposed method
outperforms current GAN-based methods and performs on par with diffusion-based
methods. However, our method is significantly faster than diffusion-based
methods in both training and inference.",['cs.CV'],http://arxiv.org/abs/2501.08900v1
ACE++: Instruction-Based Image Creation and Editing via Context-Aware Content Filling,"We report ACE++, an instruction-based diffusion framework that tackles
various image generation and editing tasks. Inspired by the input format for
the inpainting task proposed by FLUX.1-Fill-dev, we improve the Long-context
Condition Unit (LCU) introduced in ACE and extend this input paradigm to any
editing and generation tasks. To take full advantage of image generative
priors, we develop a two-stage training scheme to minimize the efforts of
finetuning powerful text-to-image diffusion models like FLUX.1-dev. In the
first stage, we pre-train the model using task data with the 0-ref tasks from
the text-to-image model. There are many models in the community based on the
post-training of text-to-image foundational models that meet this training
paradigm of the first stage. For example, FLUX.1-Fill-dev deals primarily with
painting tasks and can be used as an initialization to accelerate the training
process. In the second stage, we finetune the above model to support the
general instructions using all tasks defined in ACE. To promote the widespread
application of ACE++ in different scenarios, we provide a comprehensive set of
models that cover both full finetuning and lightweight finetuning, while
considering general applicability and applicability in vertical scenarios. The
qualitative analysis showcases the superiority of ACE++ in terms of generating
image quality and prompt following ability. Code and models will be available
on the project page: https://ali-vilab. github.io/ACE_plus_page/.",['cs.CV'],http://arxiv.org/abs/2501.02487v3
Mask-guided cross-image attention for zero-shot in-silico histopathologic image generation with a diffusion model,"Creating in-silico data with generative AI promises a cost-effective
alternative to staining, imaging, and annotating whole slide images in
computational pathology. Diffusion models are the state-of-the-art solution for
generating in-silico images, offering unparalleled fidelity and realism. Using
appearance transfer diffusion models allows for zero-shot image generation,
facilitating fast application and making model training unnecessary. However
current appearance transfer diffusion models are designed for natural images,
where the main task is to transfer the foreground object from an origin to a
target domain, while the background is of insignificant importance. In
computational pathology, specifically in oncology, it is however not
straightforward to define which objects in an image should be classified as
foreground and background, as all objects in an image may be of critical
importance for the detailed understanding the tumor micro-environment. We
contribute to the applicability of appearance transfer diffusion models to
immunohistochemistry-stained images by modifying the appearance transfer
guidance to alternate between class-specific AdaIN feature statistics matchings
using existing segmentation masks. The performance of the proposed method is
demonstrated on the downstream task of supervised epithelium segmentation,
showing that the number of manual annotations required for model training can
be reduced by 75%, outperforming the baseline approach. Additionally, we
consulted with a certified pathologist to investigate future improvements. We
anticipate this work to inspire the application of zero-shot diffusion models
in computational pathology, providing an efficient method to generate in-silico
images with unmatched fidelity and realism, which prove meaningful for
downstream tasks, such as training existing deep learning models or finetuning
foundation models.",['cs.CV'],http://arxiv.org/abs/2407.11664v3
StereoGen: High-quality Stereo Image Generation from a Single Image,"State-of-the-art supervised stereo matching methods have achieved amazing
results on various benchmarks. However, these data-driven methods suffer from
generalization to real-world scenarios due to the lack of real-world annotated
data. In this paper, we propose StereoGen, a novel pipeline for high-quality
stereo image generation. This pipeline utilizes arbitrary single images as left
images and pseudo disparities generated by a monocular depth estimation model
to synthesize high-quality corresponding right images. Unlike previous methods
that fill the occluded area in warped right images using random backgrounds or
using convolutions to take nearby pixels selectively, we fine-tune a diffusion
inpainting model to recover the background. Images generated by our model
possess better details and undamaged semantic structures. Besides, we propose
Training-free Confidence Generation and Adaptive Disparity Selection. The
former suppresses the negative effect of harmful pseudo ground truth during
stereo training, while the latter helps generate a wider disparity distribution
and better synthetic images. Experiments show that models trained under our
pipeline achieve state-of-the-art zero-shot generalization results among all
published methods. The code will be available upon publication of the paper.",['cs.CV'],http://arxiv.org/abs/2501.08654v1
Joint Learning of Depth and Appearance for Portrait Image Animation,"2D portrait animation has experienced significant advancements in recent
years. Much research has utilized the prior knowledge embedded in large
generative diffusion models to enhance high-quality image manipulation.
However, most methods only focus on generating RGB images as output, and the
co-generation of consistent visual plus 3D output remains largely
under-explored. In our work, we propose to jointly learn the visual appearance
and depth simultaneously in a diffusion-based portrait image generator. Our
method embraces the end-to-end diffusion paradigm and introduces a new
architecture suitable for learning this conditional joint distribution,
consisting of a reference network and a channel-expanded diffusion backbone.
Once trained, our framework can be efficiently adapted to various downstream
applications, such as facial depth-to-image and image-to-depth generation,
portrait relighting, and audio-driven talking head animation with consistent 3D
output.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2501.08649v1
OminiControl: Minimal and Universal Control for Diffusion Transformer,"In this paper, we introduce OminiControl, a highly versatile and
parameter-efficient framework that integrates image conditions into pre-trained
Diffusion Transformer (DiT) models. At its core, OminiControl leverages a
parameter reuse mechanism, enabling the DiT to encode image conditions using
itself as a powerful backbone and process them with its flexible multi-modal
attention processors. Unlike existing methods, which rely heavily on additional
encoder modules with complex architectures, OminiControl (1) effectively and
efficiently incorporates injected image conditions with only ~0.1% additional
parameters, and (2) addresses a wide range of image conditioning tasks in a
unified manner, including subject-driven generation and spatially-aligned
conditions such as edges, depth, and more. Remarkably, these capabilities are
achieved by training on images generated by the DiT itself, which is
particularly beneficial for subject-driven generation. Extensive evaluations
demonstrate that OminiControl outperforms existing UNet-based and DiT-adapted
models in both subject-driven and spatially-aligned conditional generation.
Additionally, we release our training dataset, Subjects200K, a diverse
collection of over 200,000 identity-consistent images, along with an efficient
data synthesis pipeline to advance research in subject-consistent generation.","['cs.CV', 'cs.AI', 'cs.LG']",http://arxiv.org/abs/2411.15098v4
CookingDiffusion: Cooking Procedural Image Generation with Stable Diffusion,"Recent advancements in text-to-image generation models have excelled in
creating diverse and realistic images. This success extends to food imagery,
where various conditional inputs like cooking styles, ingredients, and recipes
are utilized. However, a yet-unexplored challenge is generating a sequence of
procedural images based on cooking steps from a recipe. This could enhance the
cooking experience with visual guidance and possibly lead to an intelligent
cooking simulation system. To fill this gap, we introduce a novel task called
\textbf{cooking procedural image generation}. This task is inherently
demanding, as it strives to create photo-realistic images that align with
cooking steps while preserving sequential consistency. To collectively tackle
these challenges, we present \textbf{CookingDiffusion}, a novel approach that
leverages Stable Diffusion and three innovative Memory Nets to model procedural
prompts. These prompts encompass text prompts (representing cooking steps),
image prompts (corresponding to cooking images), and multi-modal prompts
(mixing cooking steps and images), ensuring the consistent generation of
cooking procedural images. To validate the effectiveness of our approach, we
preprocess the YouCookII dataset, establishing a new benchmark. Our
experimental results demonstrate that our model excels at generating
high-quality cooking procedural images with remarkable consistency across
sequential cooking steps, as measured by both the FID and the proposed Average
Procedure Consistency metrics. Furthermore, CookingDiffusion demonstrates the
ability to manipulate ingredients and cooking methods in a recipe. We will make
our code, models, and dataset publicly accessible.","['cs.CV', 'cs.GR', 'cs.LG']",http://arxiv.org/abs/2501.09042v1
Watermarking in Diffusion Model: Gaussian Shading with Exact Diffusion Inversion via Coupled Transformations (EDICT),"This paper introduces a novel approach to enhance the performance of Gaussian
Shading, a prevalent watermarking technique, by integrating the Exact Diffusion
Inversion via Coupled Transformations (EDICT) framework. While Gaussian Shading
traditionally embeds watermarks in a noise latent space, followed by iterative
denoising for image generation and noise addition for watermark recovery, its
inversion process is not exact, leading to potential watermark distortion. We
propose to leverage EDICT's ability to derive exact inverse mappings to refine
this process. Our method involves duplicating the watermark-infused noisy
latent and employing a reciprocal, alternating denoising and noising scheme
between the two latents, facilitated by EDICT. This allows for a more precise
reconstruction of both the image and the embedded watermark. Empirical
evaluation on standard datasets demonstrates that our integrated approach
yields a slight, yet statistically significant improvement in watermark
recovery fidelity. These results highlight the potential of EDICT to enhance
existing diffusion-based watermarking techniques by providing a more accurate
and robust inversion mechanism. To the best of our knowledge, this is the first
work to explore the synergy between EDICT and Gaussian Shading for digital
watermarking, opening new avenues for research in robust and high-fidelity
watermark embedding and extraction.",['cs.CV'],http://arxiv.org/abs/2501.08604v1
Yuan: Yielding Unblemished Aesthetics Through A Unified Network for Visual Imperfections Removal in Generated Images,"Generative AI presents transformative potential across various domains, from
creative arts to scientific visualization. However, the utility of AI-generated
imagery is often compromised by visual flaws, including anatomical
inaccuracies, improper object placements, and misplaced textual elements. These
imperfections pose significant challenges for practical applications. To
overcome these limitations, we introduce \textit{Yuan}, a novel framework that
autonomously corrects visual imperfections in text-to-image synthesis.
\textit{Yuan} uniquely conditions on both the textual prompt and the segmented
image, generating precise masks that identify areas in need of refinement
without requiring manual intervention -- a common constraint in previous
methodologies. Following the automated masking process, an advanced inpainting
module seamlessly integrates contextually coherent content into the identified
regions, preserving the integrity and fidelity of the original image and
associated text prompts. Through extensive experimentation on publicly
available datasets such as ImageNet100 and Stanford Dogs, along with a
custom-generated dataset, \textit{Yuan} demonstrated superior performance in
eliminating visual imperfections. Our approach consistently achieved higher
scores in quantitative metrics, including NIQE, BRISQUE, and PI, alongside
favorable qualitative evaluations. These results underscore \textit{Yuan}'s
potential to significantly enhance the quality and applicability of
AI-generated images across diverse fields.","['cs.CV', 'eess.IV']",http://arxiv.org/abs/2501.08505v1
Expressive Text-to-Image Generation with Rich Text,"Plain text has become a prevalent interface for text-to-image synthesis.
However, its limited customization options hinder users from accurately
describing desired outputs. For example, plain text makes it hard to specify
continuous quantities, such as the precise RGB color value or importance of
each word. Furthermore, creating detailed text prompts for complex scenes is
tedious for humans to write and challenging for text encoders to interpret. To
address these challenges, we propose using a rich-text editor supporting
formats such as font style, size, color, and footnote. We extract each word's
attributes from rich text to enable local style control, explicit token
reweighting, precise color rendering, and detailed region synthesis. We achieve
these capabilities through a region-based diffusion process. We first obtain
each word's region based on attention maps of a diffusion process using plain
text. For each region, we enforce its text attributes by creating
region-specific detailed prompts and applying region-specific guidance, and
maintain its fidelity against plain-text generation through region-based
injections. We present various examples of image generation from rich text and
demonstrate that our method outperforms strong baselines with quantitative
evaluations.","['cs.CV', 'cs.GR', 'cs.LG']",http://arxiv.org/abs/2304.06720v4
D$^2$-DPM: Dual Denoising for Quantized Diffusion Probabilistic Models,"Diffusion models have achieved cutting-edge performance in image generation.
However, their lengthy denoising process and computationally intensive score
estimation network impede their scalability in low-latency and
resource-constrained scenarios. Post-training quantization (PTQ) compresses and
accelerates diffusion models without retraining, but it inevitably introduces
additional quantization noise, resulting in mean and variance deviations. In
this work, we propose D2-DPM, a dual denoising mechanism aimed at precisely
mitigating the adverse effects of quantization noise on the noise estimation
network. Specifically, we first unravel the impact of quantization noise on the
sampling equation into two components: the mean deviation and the variance
deviation. The mean deviation alters the drift coefficient of the sampling
equation, influencing the trajectory trend, while the variance deviation
magnifies the diffusion coefficient, impacting the convergence of the sampling
trajectory. The proposed D2-DPM is thus devised to denoise the quantization
noise at each time step, and then denoise the noisy sample through the inverse
diffusion iterations. Experimental results demonstrate that D2-DPM achieves
superior generation quality, yielding a 1.42 lower FID than the full-precision
model while achieving 3.99x compression and 11.67x bit-operation acceleration.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2501.08180v1
Benchmarking Multimodal Models for Fine-Grained Image Analysis: A Comparative Study Across Diverse Visual Features,"This article introduces a benchmark designed to evaluate the capabilities of
multimodal models in analyzing and interpreting images. The benchmark focuses
on seven key visual aspects: main object, additional objects, background,
detail, dominant colors, style, and viewpoint. A dataset of 14,580 images,
generated from diverse text prompts, was used to assess the performance of
seven leading multimodal models. These models were evaluated on their ability
to accurately identify and describe each visual aspect, providing insights into
their strengths and weaknesses for comprehensive image understanding. The
findings of this benchmark have significant implications for the development
and selection of multimodal models for various image analysis tasks.",['cs.CV'],http://arxiv.org/abs/2501.08170v1
Digi2Real: Bridging the Realism Gap in Synthetic Data Face Recognition via Foundation Models,"The accuracy of face recognition systems has improved significantly in the
past few years, thanks to the large amount of data collected and advancements
in neural network architectures. However, these large-scale datasets are often
collected without explicit consent, raising ethical and privacy concerns. To
address this, there have been proposals to use synthetic datasets for training
face recognition models. Yet, such models still rely on real data to train the
generative models and generally exhibit inferior performance compared to those
trained on real datasets. One of these datasets, DigiFace, uses a graphics
pipeline to generate different identities and intra-class variations without
using real data in model training. However, the performance of this approach is
poor on face recognition benchmarks, possibly due to the lack of realism in the
images generated by the graphics pipeline. In this work, we introduce a novel
framework for realism transfer aimed at enhancing the realism of synthetically
generated face images. Our method leverages the large-scale face foundation
model, and we adapt the pipeline for realism enhancement. By integrating the
controllable aspects of the graphics pipeline with our realism enhancement
technique, we generate a large amount of realistic variations, combining the
advantages of both approaches. Our empirical evaluations demonstrate that
models trained using our enhanced dataset significantly improve the performance
of face recognition systems over the baseline. The source code and dataset will
be publicly accessible at the following link:
https://www.idiap.ch/paper/digi2real",['cs.CV'],http://arxiv.org/abs/2411.02188v4
Democratizing Text-to-Image Masked Generative Models with Compact Text-Aware One-Dimensional Tokens,"Image tokenizers form the foundation of modern text-to-image generative
models but are notoriously difficult to train. Furthermore, most existing
text-to-image models rely on large-scale, high-quality private datasets, making
them challenging to replicate. In this work, we introduce Text-Aware
Transformer-based 1-Dimensional Tokenizer (TA-TiTok), an efficient and powerful
image tokenizer that can utilize either discrete or continuous 1-dimensional
tokens. TA-TiTok uniquely integrates textual information during the tokenizer
decoding stage (i.e., de-tokenization), accelerating convergence and enhancing
performance. TA-TiTok also benefits from a simplified, yet effective, one-stage
training process, eliminating the need for the complex two-stage distillation
used in previous 1-dimensional tokenizers. This design allows for seamless
scalability to large datasets. Building on this, we introduce a family of
text-to-image Masked Generative Models (MaskGen), trained exclusively on open
data while achieving comparable performance to models trained on private data.
We aim to release both the efficient, strong TA-TiTok tokenizers and the
open-data, open-weight MaskGen models to promote broader access and democratize
the field of text-to-image masked generative models.",['cs.CV'],http://arxiv.org/abs/2501.07730v1
Benchmarking Counterfactual Image Generation,"Generative AI has revolutionised visual content editing, empowering users to
effortlessly modify images and videos. However, not all edits are equal. To
perform realistic edits in domains such as natural image or medical imaging,
modifications must respect causal relationships inherent to the data generation
process. Such image editing falls into the counterfactual image generation
regime. Evaluating counterfactual image generation is substantially complex:
not only it lacks observable ground truths, but also requires adherence to
causal constraints. Although several counterfactual image generation methods
and evaluation metrics exist, a comprehensive comparison within a unified
setting is lacking. We present a comparison framework to thoroughly benchmark
counterfactual image generation methods. We integrate all models that have been
used for the task at hand and expand them to novel datasets and causal graphs,
demonstrating the superiority of Hierarchical VAEs across most datasets and
metrics. Our framework is implemented in a user-friendly Python package that
can be extended to incorporate additional SCMs, causal methods, generative
models, and datasets for the community to build on. Code:
https://github.com/gulnazaki/counterfactual-benchmark.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2403.20287v5
"Amortizing intractable inference in diffusion models for vision, language, and control","Diffusion models have emerged as effective distribution estimators in vision,
language, and reinforcement learning, but their use as priors in downstream
tasks poses an intractable posterior inference problem. This paper studies
amortized sampling of the posterior over data, $\mathbf{x}\sim p^{\rm
post}(\mathbf{x})\propto p(\mathbf{x})r(\mathbf{x})$, in a model that consists
of a diffusion generative model prior $p(\mathbf{x})$ and a black-box
constraint or likelihood function $r(\mathbf{x})$. We state and prove the
asymptotic correctness of a data-free learning objective, relative trajectory
balance, for training a diffusion model that samples from this posterior, a
problem that existing methods solve only approximately or in restricted cases.
Relative trajectory balance arises from the generative flow network perspective
on diffusion models, which allows the use of deep reinforcement learning
techniques to improve mode coverage. Experiments illustrate the broad potential
of unbiased inference of arbitrary posteriors under diffusion priors: in vision
(classifier guidance), language (infilling under a discrete diffusion LLM), and
multimodal data (text-to-image generation). Beyond generative modeling, we
apply relative trajectory balance to the problem of continuous control with a
score-based behavior prior, achieving state-of-the-art results on benchmarks in
offline reinforcement learning.","['cs.LG', 'cs.CV']",http://arxiv.org/abs/2405.20971v2
Buster: Implanting Semantic Backdoor into Text Encoder to Mitigate NSFW Content Generation,"The rise of deep learning models in the digital era has raised substantial
concerns regarding the generation of Not-Safe-for-Work (NSFW) content. Existing
defense methods primarily involve model fine-tuning and post-hoc content
moderation. Nevertheless, these approaches largely lack scalability in
eliminating harmful content, degrade the quality of benign image generation, or
incur high inference costs. To address these challenges, we propose an
innovative framework named \textit{Buster}, which injects backdoors into the
text encoder to prevent NSFW content generation. Buster leverages deep semantic
information rather than explicit prompts as triggers, redirecting NSFW prompts
towards targeted benign prompts. Additionally, Buster employs energy-based
training data generation through Langevin dynamics for adversarial knowledge
augmentation, thereby ensuring robustness in harmful concept definition. This
approach demonstrates exceptional resilience and scalability in mitigating NSFW
content. Particularly, Buster fine-tunes the text encoder of Text-to-Image
models within merely five minutes, showcasing its efficiency. Our extensive
experiments denote that Buster outperforms nine state-of-the-art baselines,
achieving a superior NSFW content removal rate of at least 91.2\% while
preserving the quality of harmless images.","['cs.CV', 'cs.AI', 'cs.LG']",http://arxiv.org/abs/2412.07249v2
Enhancing Image Generation Fidelity via Progressive Prompts,"The diffusion transformer (DiT) architecture has attracted significant
attention in image generation, achieving better fidelity, performance, and
diversity. However, most existing DiT - based image generation methods focus on
global - aware synthesis, and regional prompt control has been less explored.
In this paper, we propose a coarse - to - fine generation pipeline for regional
prompt - following generation. Specifically, we first utilize the powerful
large language model (LLM) to generate both high - level descriptions of the
image (such as content, topic, and objects) and low - level descriptions (such
as details and style). Then, we explore the influence of cross - attention
layers at different depths. We find that deeper layers are always responsible
for high - level content control, while shallow layers handle low - level
content control. Various prompts are injected into the proposed regional cross
- attention control for coarse - to - fine generation. By using the proposed
pipeline, we enhance the controllability of DiT - based image generation.
Extensive quantitative and qualitative results show that our pipeline can
improve the performance of the generated images.",['cs.CV'],http://arxiv.org/abs/2501.07070v1
Detection of AI Deepfake and Fraud in Online Payments Using GAN-Based Models,"This study explores the use of Generative Adversarial Networks (GANs) to
detect AI deepfakes and fraudulent activities in online payment systems. With
the growing prevalence of deepfake technology, which can manipulate facial
features in images and videos, the potential for fraud in online transactions
has escalated. Traditional security systems struggle to identify these
sophisticated forms of fraud. This research proposes a novel GAN-based model
that enhances online payment security by identifying subtle manipulations in
payment images. The model is trained on a dataset consisting of real-world
online payment images and deepfake images generated using advanced GAN
architectures, such as StyleGAN and DeepFake. The results demonstrate that the
proposed model can accurately distinguish between legitimate transactions and
deepfakes, achieving a high detection rate above 95%. This approach
significantly improves the robustness of payment systems against AI-driven
fraud. The paper contributes to the growing field of digital security, offering
insights into the application of GANs for fraud detection in financial
services. Keywords- Payment Security, Image Recognition, Generative Adversarial
Networks, AI Deepfake, Fraudulent Activities","['cs.LG', 'cs.CR', 'cs.CV']",http://arxiv.org/abs/2501.07033v1
DivTrackee versus DynTracker: Promoting Diversity in Anti-Facial Recognition against Dynamic FR Strategy,"The widespread adoption of facial recognition (FR) models raises serious
concerns about their potential misuse, motivating the development of
anti-facial recognition (AFR) to protect user facial privacy. In this paper, we
argue that the static FR strategy, predominantly adopted in prior literature
for evaluating AFR efficacy, cannot faithfully characterize the actual
capabilities of determined trackers who aim to track a specific target
identity. In particular, we introduce \emph{\ourAttack}, a dynamic FR strategy
where the model's gallery database is iteratively updated with newly recognized
target identity images. Surprisingly, such a simple approach renders all the
existing AFR protections ineffective. To mitigate the privacy threats posed by
DynTracker, we advocate for explicitly promoting diversity in the AFR-protected
images. We hypothesize that the lack of diversity is the primary cause of the
failure of existing AFR methods. Specifically, we develop \emph{DivTrackee}, a
novel method for crafting diverse AFR protections that builds upon a
text-guided image generation framework and diversity-promoting adversarial
losses. Through comprehensive experiments on various facial image benchmarks
and feature extractors, we demonstrate DynTracker's strength in breaking
existing AFR methods and the superiority of DivTrackee in preventing user
facial images from being identified by dynamic FR strategies. We believe our
work can act as an important initial step towards developing more effective AFR
methods for protecting user facial privacy against determined trackers.","['cs.CV', 'cs.CR']",http://arxiv.org/abs/2501.06533v1
Rethinking The Training And Evaluation of Rich-Context Layout-to-Image Generation,"Recent advancements in generative models have significantly enhanced their
capacity for image generation, enabling a wide range of applications such as
image editing, completion and video editing. A specialized area within
generative modeling is layout-to-image (L2I) generation, where predefined
layouts of objects guide the generative process. In this study, we introduce a
novel regional cross-attention module tailored to enrich layout-to-image
generation. This module notably improves the representation of layout regions,
particularly in scenarios where existing methods struggle with highly complex
and detailed textual descriptions. Moreover, while current open-vocabulary L2I
methods are trained in an open-set setting, their evaluations often occur in
closed-set environments. To bridge this gap, we propose two metrics to assess
L2I performance in open-vocabulary scenarios. Additionally, we conduct a
comprehensive user study to validate the consistency of these metrics with
human preferences.",['cs.CV'],http://arxiv.org/abs/2409.04847v2
Focus-N-Fix: Region-Aware Fine-Tuning for Text-to-Image Generation,"Text-to-image (T2I) generation has made significant advances in recent years,
but challenges still remain in the generation of perceptual artifacts,
misalignment with complex prompts, and safety. The prevailing approach to
address these issues involves collecting human feedback on generated images,
training reward models to estimate human feedback, and then fine-tuning T2I
models based on the reward models to align them with human preferences.
However, while existing reward fine-tuning methods can produce images with
higher rewards, they may change model behavior in unexpected ways. For example,
fine-tuning for one quality aspect (e.g., safety) may degrade other aspects
(e.g., prompt alignment), or may lead to reward hacking (e.g., finding a way to
increase rewards without having the intended effect). In this paper, we propose
Focus-N-Fix, a region-aware fine-tuning method that trains models to correct
only previously problematic image regions. The resulting fine-tuned model
generates images with the same high-level structure as the original model but
shows significant improvements in regions where the original model was
deficient in safety (over-sexualization and violence), plausibility, or other
criteria. Our experiments demonstrate that Focus-N-Fix improves these localized
quality aspects with little or no degradation to others and typically
imperceptible changes in the rest of the image. Disclaimer: This paper contains
images that may be overly sexual, violent, offensive, or harmful.",['cs.CV'],http://arxiv.org/abs/2501.06481v1
Elucidating Flow Matching ODE Dynamics with Respect to Data Geometries,"Diffusion-based generative models have become the standard for image
generation. ODE-based samplers and flow matching models improve efficiency, in
comparison to diffusion models, by reducing sampling steps through learned
vector fields. However, the theoretical foundations of flow matching models
remain limited, particularly regarding the convergence of individual sample
trajectories at terminal time - a critical property that impacts sample quality
and being critical assumption for models like the consistency model. In this
paper, we advance the theory of flow matching models through a comprehensive
analysis of sample trajectories, centered on the denoiser that drives ODE
dynamics. We establish the existence, uniqueness and convergence of ODE
trajectories at terminal time, ensuring stable sampling outcomes under minimal
assumptions. Our analysis reveals how trajectories evolve from capturing global
data features to local structures, providing the geometric characterization of
per-sample behavior in flow matching models. We also explain the memorization
phenomenon in diffusion-based training through our terminal time analysis.
These findings bridge critical gaps in understanding flow matching models, with
practical implications for sampling stability and model design.",['cs.LG'],http://arxiv.org/abs/2412.18730v2
MEt3R: Measuring Multi-View Consistency in Generated Images,"We introduce MEt3R, a metric for multi-view consistency in generated images.
Large-scale generative models for multi-view image generation are rapidly
advancing the field of 3D inference from sparse observations. However, due to
the nature of generative modeling, traditional reconstruction metrics are not
suitable to measure the quality of generated outputs and metrics that are
independent of the sampling procedure are desperately needed. In this work, we
specifically address the aspect of consistency between generated multi-view
images, which can be evaluated independently of the specific scene. Our
approach uses DUSt3R to obtain dense 3D reconstructions from image pairs in a
feed-forward manner, which are used to warp image contents from one view into
the other. Then, feature maps of these images are compared to obtain a
similarity score that is invariant to view-dependent effects. Using MEt3R, we
evaluate the consistency of a large set of previous methods for novel view and
video generation, including our open, multi-view latent diffusion model.","['cs.CV', 'cs.LG', 'eess.IV']",http://arxiv.org/abs/2501.06336v1
MutualForce: Mutual-Aware Enhancement for 4D Radar-LiDAR 3D Object Detection,"Radar and LiDAR have been widely used in autonomous driving as LiDAR provides
rich structure information, and radar demonstrates high robustness under
adverse weather. Recent studies highlight the effectiveness of fusing radar and
LiDAR point clouds. However, challenges remain due to the modality misalignment
and information loss during feature extractions. To address these issues, we
propose a 4D radar-LiDAR framework to mutually enhance their representations.
Initially, the indicative features from radar are utilized to guide both radar
and LiDAR geometric feature learning. Subsequently, to mitigate their sparsity
gap, the shape information from LiDAR is used to enrich radar BEV features.
Extensive experiments on the View-of-Delft (VoD) dataset demonstrate our
approach's superiority over existing methods, achieving the highest mAP of
71.76% across the entire area and 86.36\% within the driving corridor.
Especially for cars, we improve the AP by 4.17% and 4.20% due to the strong
indicative features and symmetric shapes.",['cs.CV'],http://arxiv.org/abs/2501.10266v1
Leveraging Confident Image Regions for Source-Free Domain-Adaptive Object Detection,"Source-free domain-adaptive object detection is an interesting but scarcely
addressed topic. It aims at adapting a source-pretrained detector to a distinct
target domain without resorting to source data during adaptation. So far, there
is no data augmentation scheme tailored to source-free domain-adaptive object
detection. To this end, this paper presents a novel data augmentation approach
that cuts out target image regions where the detector is confident, augments
them along with their respective pseudo-labels, and joins them into a
challenging target image to adapt the detector. As the source data is out of
reach during adaptation, we implement our approach within a teacher-student
learning paradigm to ensure that the model does not collapse during the
adaptation procedure. We evaluated our approach on three adaptation benchmarks
of traffic scenes, scoring new state-of-the-art on two of them.",['cs.CV'],http://arxiv.org/abs/2501.10081v1
One-D-Piece: Image Tokenizer Meets Quality-Controllable Compression,"Current image tokenization methods require a large number of tokens to
capture the information contained within images. Although the amount of
information varies across images, most image tokenizers only support
fixed-length tokenization, leading to inefficiency in token allocation. In this
study, we introduce One-D-Piece, a discrete image tokenizer designed for
variable-length tokenization, achieving quality-controllable mechanism. To
enable variable compression rate, we introduce a simple but effective
regularization mechanism named ""Tail Token Drop"" into discrete one-dimensional
image tokenizers. This method encourages critical information to concentrate at
the head of the token sequence, enabling support of variadic tokenization,
while preserving state-of-the-art reconstruction quality. We evaluate our
tokenizer across multiple reconstruction quality metrics and find that it
delivers significantly better perceptual quality than existing
quality-controllable compression methods, including JPEG and WebP, at smaller
byte sizes. Furthermore, we assess our tokenizer on various downstream computer
vision tasks, including image classification, object detection, semantic
segmentation, and depth estimation, confirming its adaptability to numerous
applications compared to other variable-rate methods. Our approach demonstrates
the versatility of variable-length discrete image tokenization, establishing a
new paradigm in both compression efficiency and reconstruction performance.
Finally, we validate the effectiveness of tail token drop via detailed analysis
of tokenizers.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2501.10064v1
LWGANet: A Lightweight Group Attention Backbone for Remote Sensing Visual Tasks,"Remote sensing (RS) visual tasks have gained significant academic and
practical importance. However, they encounter numerous challenges that hinder
effective feature extraction, including the detection and recognition of
multiple objects exhibiting substantial variations in scale within a single
image. While prior dual-branch or multi-branch architectural strategies have
been effective in managing these object variances, they have concurrently
resulted in considerable increases in computational demands and parameter
counts. Consequently, these architectures are rendered less viable for
deployment on resource-constrained devices. Contemporary lightweight backbone
networks, designed primarily for natural images, frequently encounter
difficulties in effectively extracting features from multi-scale objects, which
compromises their efficacy in RS visual tasks. This article introduces LWGANet,
a specialized lightweight backbone network tailored for RS visual tasks,
incorporating a novel lightweight group attention (LWGA) module designed to
address these specific challenges. LWGA module, tailored for RS imagery,
adeptly harnesses redundant features to extract a wide range of spatial
information, from local to global scales, without introducing additional
complexity or computational overhead. This facilitates precise feature
extraction across multiple scales within an efficient framework.LWGANet was
rigorously evaluated across twelve datasets, which span four crucial RS visual
tasks: scene classification, oriented object detection, semantic segmentation,
and change detection. The results confirm LWGANet's widespread applicability
and its ability to maintain an optimal balance between high performance and low
complexity, achieving SOTA results across diverse datasets. LWGANet emerged as
a novel solution for resource-limited scenarios requiring robust RS image
processing capabilities.",['cs.CV'],http://arxiv.org/abs/2501.10040v1
IOR: Inversed Objects Replay for Incremental Object Detection,"Existing Incremental Object Detection (IOD) methods partially alleviate
catastrophic forgetting when incrementally detecting new objects in real-world
scenarios. However, many of these methods rely on the assumption that unlabeled
old-class objects may co-occur with labeled new-class objects in the
incremental data. When unlabeled old-class objects are absent, the performance
of existing methods tends to degrade. The absence can be mitigated by
generating old-class samples, but it incurs high costs. This paper argues that
previous generation-based IOD suffers from redundancy, both in the use of
generative models, which require additional training and storage, and in the
overproduction of generated samples, many of which do not contribute
significantly to performance improvements. To eliminate the redundancy, we
propose Inversed Objects Replay (IOR). Specifically, we generate old-class
samples by inversing the original detectors, thus eliminating the necessity of
training and storing additional generative models. We propose augmented replay
to reuse the objects in generated samples, reducing redundant generations.
Moreover, we propose high-value knowledge distillation focusing on the
positions of old-class objects overwhelmed by the background, which transfers
the knowledge to the incremental detector. Extensive experiments conducted on
MS COCO 2017 demonstrate that our method can efficiently improve detection
performance in IOD scenarios with the absence of old-class objects.",['cs.CV'],http://arxiv.org/abs/2406.04829v4
FLORA: Formal Language Model Enables Robust Training-free Zero-shot Object Referring Analysis,"Object Referring Analysis (ORA), commonly known as referring expression
comprehension, requires the identification and localization of specific objects
in an image based on natural descriptions. Unlike generic object detection, ORA
requires both accurate language understanding and precise visual localization,
making it inherently more complex. Although recent pre-trained large visual
grounding detectors have achieved significant progress, they heavily rely on
extensively labeled data and time-consuming learning. To address these, we
introduce a novel, training-free framework for zero-shot ORA, termed FLORA
(Formal Language for Object Referring and Analysis). FLORA harnesses the
inherent reasoning capabilities of large language models (LLMs) and integrates
a formal language model - a logical framework that regulates language within
structured, rule-based descriptions - to provide effective zero-shot ORA. More
specifically, our formal language model (FLM) enables an effective,
logic-driven interpretation of object descriptions without necessitating any
training processes. Built upon FLM-regulated LLM outputs, we further devise a
Bayesian inference framework and employ appropriate off-the-shelf interpretive
models to finalize the reasoning, delivering favorable robustness against LLM
hallucinations and compelling ORA performance in a training-free manner. In
practice, our FLORA boosts the zero-shot performance of existing pretrained
grounding detectors by up to around 45%. Our comprehensive evaluation across
different challenging datasets also confirms that FLORA consistently surpasses
current state-of-the-art zero-shot methods in both detection and segmentation
tasks associated with zero-shot ORA. We believe our probabilistic parsing and
reasoning of the LLM outputs elevate the reliability and interpretability of
zero-shot ORA. We shall release codes upon publication.",['cs.CV'],http://arxiv.org/abs/2501.09887v1
"AgRegNet: A Deep Regression Network for Flower and Fruit Density Estimation, Localization, and Counting in Orchards","One of the major challenges for the agricultural industry today is the
uncertainty in manual labor availability and the associated cost. Automated
flower and fruit density estimation, localization, and counting could help
streamline harvesting, yield estimation, and crop-load management strategies
such as flower and fruitlet thinning. This article proposes a deep
regression-based network, AgRegNet, to estimate density, count, and location of
flower and fruit in tree fruit canopies without explicit object detection or
polygon annotation. Inspired by popular U-Net architecture, AgRegNet is a
U-shaped network with an encoder-to-decoder skip connection and modified
ConvNeXt-T as an encoder feature extractor. AgRegNet can be trained based on
information from point annotation and leverages segmentation information and
attention modules (spatial and channel) to highlight relevant flower and fruit
features while suppressing non-relevant background features. Experimental
evaluation in apple flower and fruit canopy images under an unstructured
orchard environment showed that AgRegNet achieved promising accuracy as
measured by Structural Similarity Index (SSIM), percentage Mean Absolute Error
(pMAE) and mean Average Precision (mAP) to estimate flower and fruit density,
count, and centroid location, respectively. Specifically, the SSIM, pMAE, and
mAP values for flower images were 0.938, 13.7%, and 0.81, respectively. For
fruit images, the corresponding values were 0.910, 5.6%, and 0.93. Since the
proposed approach relies on information from point annotation, it is suitable
for sparsely and densely located objects. This simplified technique will be
highly applicable for growers to accurately estimate yields and decide on
optimal chemical and mechanical flower thinning practices.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2409.17400v2
A Simple Aerial Detection Baseline of Multimodal Language Models,"The multimodal language models (MLMs) based on generative pre-trained
Transformer are considered powerful candidates for unifying various domains and
tasks. MLMs developed for remote sensing (RS) have demonstrated outstanding
performance in multiple tasks, such as visual question answering and visual
grounding. In addition to visual grounding that detects specific objects
corresponded to given instruction, aerial detection, which detects all objects
of multiple categories, is also a valuable and challenging task for RS
foundation models. However, aerial detection has not been explored by existing
RS MLMs because the autoregressive prediction mechanism of MLMs differs
significantly from the detection outputs. In this paper, we present a simple
baseline for applying MLMs to aerial detection for the first time, named
LMMRotate. Specifically, we first introduce a normalization method to transform
detection outputs into textual outputs to be compatible with the MLM framework.
Then, we propose a evaluation method, which ensures a fair comparison between
MLMs and conventional object detection models. We construct the baseline by
fine-tuning open-source general-purpose MLMs and achieve impressive detection
performance comparable to conventional detector. We hope that this baseline
will serve as a reference for future MLM development, enabling more
comprehensive capabilities for understanding RS images. Code is available at
https://github.com/Li-Qingyun/mllm-mmrotate.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.09720v1
Practical Continual Forgetting for Pre-trained Vision Models,"For privacy and security concerns, the need to erase unwanted information
from pre-trained vision models is becoming evident nowadays. In real-world
scenarios, erasure requests originate at any time from both users and model
owners, and these requests usually form a sequence. Therefore, under such a
setting, selective information is expected to be continuously removed from a
pre-trained model while maintaining the rest. We define this problem as
continual forgetting and identify three key challenges. (i) For unwanted
knowledge, efficient and effective deleting is crucial. (ii) For remaining
knowledge, the impact brought by the forgetting procedure should be minimal.
(iii) In real-world scenarios, the training samples may be scarce or partially
missing during the process of forgetting. To address them, we first propose
Group Sparse LoRA (GS-LoRA). Specifically, towards (i), we introduce LoRA
modules to fine-tune the FFN layers in Transformer blocks for each forgetting
task independently, and towards (ii), a simple group sparse regularization is
adopted, enabling automatic selection of specific LoRA groups and zeroing out
the others. To further extend GS-LoRA to more practical scenarios, we
incorporate prototype information as additional supervision and introduce a
more practical approach, GS-LoRA++. For each forgotten class, we move the
logits away from its original prototype. For the remaining classes, we pull the
logits closer to their respective prototypes. We conduct extensive experiments
on face recognition, object detection and image classification and demonstrate
that our method manages to forget specific classes with minimal impact on other
classes. Codes have been released on https://github.com/bjzhb666/GS-LoRA.","['cs.CV', 'cs.AI', 'cs.LG']",http://arxiv.org/abs/2501.09705v1
MonoSOWA: Scalable monocular 3D Object detector Without human Annotations,"Detecting the three-dimensional position and orientation of objects using a
single RGB camera is a foundational task in computer vision with many important
applications. Traditionally, 3D object detection methods are trained in a
fully-supervised setup, requiring vast amounts of human annotations, which are
laborious, costly, and do not scale well with the ever-increasing amounts of
data being captured.
  In this paper, we present the first method to train 3D object detectors for
monocular RGB cameras without domain-specific human annotations, thus making
orders of magnitude more data available for training. Thanks to newly proposed
Canonical Object Space, the method can not only exploit data across a variety
of datasets and camera setups to train a single 3D detector, but unlike
previous work it also works out of the box in previously unseen camera setups.
All this is crucial for practical applications, where the data and cameras are
extremely heterogeneous.
  The method is evaluated on two standard autonomous driving datasets, where it
outperforms previous works, which, unlike our method, still rely on 2D human
annotations.","['cs.CV', 'cs.AI', 'cs.LG']",http://arxiv.org/abs/2501.09481v1
RE-POSE: Synergizing Reinforcement Learning-Based Partitioning and Offloading for Edge Object Detection,"Object detection plays a crucial role in smart video analysis, with
applications ranging from autonomous driving and security to smart cities.
However, achieving real-time object detection on edge devices presents
significant challenges due to their limited computational resources and the
high demands of deep neural network (DNN)-based detection models, particularly
when processing high-resolution video. Conventional strategies, such as input
down-sampling and network up-scaling, often compromise detection accuracy for
faster performance or lead to higher inference latency. To address these
issues, this paper introduces RE-POSE, a Reinforcement Learning (RL)-Driven
Partitioning and Edge Offloading framework designed to optimize the
accuracy-latency trade-off in resource-constrained edge environments. Our
approach features an RL-Based Dynamic Clustering Algorithm (RL-DCA) that
partitions video frames into non-uniform blocks based on object distribution
and the computational characteristics of DNNs. Furthermore, a parallel edge
offloading scheme is implemented to distribute these blocks across multiple
edge servers for concurrent processing. Experimental evaluations show that
RE-POSE significantly enhances detection accuracy and reduces inference
latency, surpassing existing methods.","['cs.CV', 'cs.AI', 'cs.DC']",http://arxiv.org/abs/2501.09465v1
On the Relation between Optical Aperture and Automotive Object Detection,"We explore the impact of aperture size and shape on automotive camera systems
for deep-learning-based tasks like traffic sign recognition and light state
detection. A method is proposed to simulate optical effects using the point
spread function (PSF), enhancing realism and reducing the domain gap between
synthetic and real-world images. Computer-generated scenes are refined with
this technique to model optical distortions and improve simulation accuracy.",['cs.CV'],http://arxiv.org/abs/2501.09456v1
SoccerSynth-Detection: A Synthetic Dataset for Soccer Player Detection,"In soccer video analysis, player detection is essential for identifying key
events and reconstructing tactical positions. The presence of numerous players
and frequent occlusions, combined with copyright restrictions, severely
restricts the availability of datasets, leaving limited options such as
SoccerNet-Tracking and SportsMOT. These datasets suffer from a lack of
diversity, which hinders algorithms from adapting effectively to varied soccer
video contexts. To address these challenges, we developed
SoccerSynth-Detection, the first synthetic dataset designed for the detection
of synthetic soccer players. It includes a broad range of random lighting and
textures, as well as simulated camera motion blur. We validated its efficacy
using the object detection model (Yolov8n) against real-world datasets
(SoccerNet-Tracking and SportsMoT). In transfer tests, it matched the
performance of real datasets and significantly outperformed them in images with
motion blur; in pre-training tests, it demonstrated its efficacy as a
pre-training dataset, significantly enhancing the algorithm's overall
performance. Our work demonstrates the potential of synthetic datasets to
replace real datasets for algorithm training in the field of soccer video
analysis.",['cs.CV'],http://arxiv.org/abs/2501.09281v1
Enhancing Novel Object Detection via Cooperative Foundational Models,"In this work, we address the challenging and emergent problem of novel object
detection (NOD), focusing on the accurate detection of both known and novel
object categories during inference. Traditional object detection algorithms are
inherently closed-set, limiting their capability to handle NOD. We present a
novel approach to transform existing closed-set detectors into open-set
detectors. This transformation is achieved by leveraging the complementary
strengths of pre-trained foundational models, specifically CLIP and SAM,
through our cooperative mechanism. Furthermore, by integrating this mechanism
with state-of-the-art open-set detectors such as GDINO, we establish new
benchmarks in object detection performance. Our method achieves 17.42 mAP in
novel object detection and 42.08 mAP for known objects on the challenging LVIS
dataset. Adapting our approach to the COCO OVD split, we surpass the current
state-of-the-art by a margin of 7.2 $ \text{AP}_{50} $ for novel classes. Our
code is available at https://rohit901.github.io/coop-foundation-models/ .","['cs.CV', 'cs.AI', 'cs.LG']",http://arxiv.org/abs/2311.12068v4
CGCOD: Class-Guided Camouflaged Object Detection,"Camouflaged Object Detection (COD) aims to identify objects that blend
seamlessly into their surroundings. The inherent visual complexity of
camouflaged objects, including their low contrast with the background, diverse
textures, and subtle appearance variations, often obscures semantic cues,
making accurate segmentation highly challenging. Existing methods primarily
rely on visual features, which are insufficient to handle the variability and
intricacy of camouflaged objects, leading to unstable object perception and
ambiguous segmentation results. To tackle these limitations, we introduce a
novel task, class-guided camouflaged object detection (CGCOD), which extends
traditional COD task by incorporating object-specific class knowledge to
enhance detection robustness and accuracy. To facilitate this task, we present
a new dataset, CamoClass, comprising real-world camouflaged objects with class
annotations. Furthermore, we propose a multi-stage framework, CGNet, which
incorporates a plug-and-play class prompt generator and a simple yet effective
class-guided detector. This establishes a new paradigm for COD, bridging the
gap between contextual understanding and class-guided detection. Extensive
experimental results demonstrate the effectiveness of our flexible framework in
improving the performance of proposed and existing detectors by leveraging
class-level textual information.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2412.18977v2
Polyp detection in colonoscopy images using YOLOv11,"Colorectal cancer (CRC) is one of the most commonly diagnosed cancers all
over the world. It starts as a polyp in the inner lining of the colon. To
prevent CRC, early polyp detection is required. Colonosopy is used for the
inspection of the colon. Generally, the images taken by the camera placed at
the tip of the endoscope are analyzed by the experts manually. Various
traditional machine learning models have been used with the rise of machine
learning. Recently, deep learning models have shown more effectiveness in polyp
detection due to their superiority in generalizing and learning small features.
These deep learning models for object detection can be segregated into two
different types: single-stage and two-stage. Generally, two stage models have
higher accuracy than single stage ones but the single stage models have low
inference time. Hence, single stage models are easy to use for quick object
detection. YOLO is one of the singlestage models used successfully for polyp
detection. It has drawn the attention of researchers because of its lower
inference time. The researchers have used Different versions of YOLO so far,
and with each newer version, the accuracy of the model is increasing. This
paper aims to see the effectiveness of the recently released YOLOv11 to detect
polyp. We analyzed the performance for all five models of YOLOv11 (YOLO11n,
YOLO11s, YOLO11m, YOLO11l, YOLO11x) with Kvasir dataset for the training and
testing. Two different versions of the dataset were used. The first consisted
of the original dataset, and the other was created using augmentation
techniques. The performance of all the models with these two versions of the
dataset have been analysed.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.09051v1
Multispectral Pedestrian Detection with Sparsely Annotated Label,"Although existing Sparsely Annotated Object Detection (SAOD) approches have
made progress in handling sparsely annotated environments in multispectral
domain, where only some pedestrians are annotated, they still have the
following limitations: (i) they lack considerations for improving the quality
of pseudo-labels for missing annotations, and (ii) they rely on fixed ground
truth annotations, which leads to learning only a limited range of pedestrian
visual appearances in the multispectral domain. To address these issues, we
propose a novel framework called Sparsely Annotated Multispectral Pedestrian
Detection (SAMPD). For limitation (i), we introduce Multispectral
Pedestrian-aware Adaptive Weight (MPAW) and Positive Pseudo-label Enhancement
(PPE) module. Utilizing multispectral knowledge, these modules ensure the
generation of high-quality pseudo-labels and enable effective learning by
increasing weights for high-quality pseudo-labels based on modality
characteristics. To address limitation (ii), we propose an Adaptive Pedestrian
Retrieval Augmentation (APRA) module, which adaptively incorporates pedestrian
patches from ground-truth and dynamically integrates high-quality pseudo-labels
with the ground-truth, facilitating a more diverse learning pool of
pedestrians. Extensive experimental results demonstrate that our SAMPD
significantly enhances performance in sparsely annotated environments within
the multispectral domain.",['cs.CV'],http://arxiv.org/abs/2501.02640v3
PACF: Prototype Augmented Compact Features for Improving Domain Adaptive Object Detection,"In recent years, there has been significant advancement in object detection.
However, applying off-the-shelf detectors to a new domain leads to significant
performance drop, caused by the domain gap. These detectors exhibit
higher-variance class-conditional distributions in the target domain than that
in the source domain, along with mean shift. To address this problem, we
propose the Prototype Augmented Compact Features (PACF) framework to regularize
the distribution of intra-class features. Specifically, we provide an in-depth
theoretical analysis on the lower bound of the target features-related
likelihood and derive the prototype cross entropy loss to further calibrate the
distribution of target RoI features. Furthermore, a mutual regularization
strategy is designed to enable the linear and prototype-based classifiers to
learn from each other, promoting feature compactness while enhancing
discriminability. Thanks to this PACF framework, we have obtained a more
compact cross-domain feature space, within which the variance of the target
features' class-conditional distributions has significantly decreased, and the
class-mean shift between the two domains has also been further reduced. The
results on different adaptation settings are state-of-the-art, which
demonstrate the board applicability and effectiveness of the proposed approach.",['cs.CV'],http://arxiv.org/abs/2501.08605v1
Predicting Performance of Object Detection Models in Electron Microscopy Using Random Forests,"Quantifying prediction uncertainty when applying object detection models to
new, unlabeled datasets is critical in applied machine learning. This study
introduces an approach to estimate the performance of deep learning-based
object detection models for quantifying defects in transmission electron
microscopy (TEM) images, focusing on detecting irradiation-induced cavities in
TEM images of metal alloys. We developed a random forest regression model that
predicts the object detection F1 score, a statistical metric used to evaluate
the ability to accurately locate and classify objects of interest. The random
forest model uses features extracted from the predictions of the object
detection model whose uncertainty is being quantified, enabling fast prediction
on new, unlabeled images. The mean absolute error (MAE) for predicting F1 of
the trained model on test data is 0.09, and the $R^2$ score is 0.77, indicating
there is a significant correlation between the random forest regression model
predicted and true defect detection F1 scores. The approach is shown to be
robust across three distinct TEM image datasets with varying imaging and
material domains. Our approach enables users to estimate the reliability of a
defect detection and segmentation model predictions and assess the
applicability of the model to their specific datasets, providing valuable
information about possible domain shifts and whether the model needs to be
fine-tuned or trained on additional data to be maximally effective for the
desired use case.","['cs.CV', 'cond-mat.mtrl-sci']",http://arxiv.org/abs/2501.08465v1
Relaxed Rotational Equivariance via $G$-Biases in Vision,"Group Equivariant Convolution (GConv) can capture rotational equivariance
from original data. It assumes uniform and strict rotational equivariance
across all features as the transformations under the specific group. However,
the presentation or distribution of real-world data rarely conforms to strict
rotational equivariance, commonly referred to as Rotational Symmetry-Breaking
(RSB) in the system or dataset, making GConv unable to adapt effectively to
this phenomenon. Motivated by this, we propose a simple but highly effective
method to address this problem, which utilizes a set of learnable biases called
$G$-Biases under the group order to break strict group constraints and then
achieve a Relaxed Rotational Equivariant Convolution (RREConv). To validate the
efficiency of RREConv, we conduct extensive ablation experiments on the
discrete rotational group $\mathcal{C}_n$. Experiments demonstrate that the
proposed RREConv-based methods achieve excellent performance compared to
existing GConv-based methods in both classification and 2D object detection
tasks on the natural image datasets.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2408.12454v3
Feedback-driven object detection and iterative model improvement,"Automated object detection has become increasingly valuable across diverse
applications, yet efficient, high-quality annotation remains a persistent
challenge. In this paper, we present the development and evaluation of a
platform designed to interactively improve object detection models. The
platform allows uploading and annotating images as well as fine-tuning object
detection models. Users can then manually review and refine annotations,
further creating improved snapshots that are used for automatic object
detection on subsequent image uploads - a process we refer to as semi-automatic
annotation resulting in a significant gain in annotation efficiency.
  Whereas iterative refinement of model results to speed up annotation has
become common practice, we are the first to quantitatively evaluate its
benefits with respect to time, effort, and interaction savings. Our
experimental results show clear evidence for a significant time reduction of up
to 53% for semi-automatic compared to manual annotation. Importantly, these
efficiency gains did not compromise annotation quality, while matching or
occasionally even exceeding the accuracy of manual annotations. These findings
demonstrate the potential of our lightweight annotation platform for creating
high-quality object detection datasets and provide best practices to guide
future development of annotation platforms.
  The platform is open-source, with the frontend and backend repositories
available on GitHub (https://github.com/ml-lab-htw/iterative-annotate). To
support the understanding of our labeling process, we have created an
explanatory video demonstrating the methodology using microscopy images of E.
coli bacteria as an example. The video is available on YouTube
(https://www.youtube.com/watch?v=CM9uhE8NN5E).","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2411.19835v2
Bootstrapping Corner Cases: High-Resolution Inpainting for Safety Critical Detect and Avoid for Automated Flying,"Modern machine learning techniques have shown tremendous potential,
especially for object detection on camera images. For this reason, they are
also used to enable safety-critical automated processes such as autonomous
drone flights. We present a study on object detection for Detect and Avoid, a
safety critical function for drones that detects air traffic during automated
flights for safety reasons. An ill-posed problem is the generation of good and
especially large data sets, since detection itself is the corner case. Most
models suffer from limited ground truth in raw data, \eg recorded air traffic
or frontal flight with a small aircraft. It often leads to poor and critical
detection rates. We overcome this problem by using inpainting methods to
bootstrap the dataset such that it explicitly contains the corner cases of the
raw data. We provide an overview of inpainting methods and generative models
and present an example pipeline given a small annotated dataset. We validate
our method by generating a high-resolution dataset, which we make publicly
available and present it to an independent object detector that was fully
trained on real data.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2501.08142v1
Learning Motion and Temporal Cues for Unsupervised Video Object Segmentation,"In this paper, we address the challenges in unsupervised video object
segmentation (UVOS) by proposing an efficient algorithm, termed MTNet, which
concurrently exploits motion and temporal cues. Unlike previous methods that
focus solely on integrating appearance with motion or on modeling temporal
relations, our method combines both aspects by integrating them within a
unified framework. MTNet is devised by effectively merging appearance and
motion features during the feature extraction process within encoders,
promoting a more complementary representation. To capture the intricate
long-range contextual dynamics and information embedded within videos, a
temporal transformer module is introduced, facilitating efficacious inter-frame
interactions throughout a video clip. Furthermore, we employ a cascade of
decoders all feature levels across all feature levels to optimally exploit the
derived features, aiming to generate increasingly precise segmentation masks.
As a result, MTNet provides a strong and compact framework that explores both
temporal and cross-modality knowledge to robustly localize and track the
primary object accurately in various challenging scenarios efficiently.
Extensive experiments across diverse benchmarks conclusively show that our
method not only attains state-of-the-art performance in unsupervised video
object segmentation but also delivers competitive results in video salient
object detection. These findings highlight the method's robust versatility and
its adeptness in adapting to a range of segmentation tasks. Source code is
available on https://github.com/hy0523/MTNet.",['cs.CV'],http://arxiv.org/abs/2501.07806v1
Parameter-Inverted Image Pyramid Networks for Visual Perception and Multimodal Understanding,"Image pyramids are widely adopted in top-performing methods to obtain
multi-scale features for precise visual perception and understanding. However,
current image pyramids use the same large-scale model to process multiple
resolutions of images, leading to significant computational cost. To address
this challenge, we propose a novel network architecture, called
Parameter-Inverted Image Pyramid Networks (PIIP). Specifically, PIIP uses
pretrained models (ViTs or CNNs) as branches to process multi-scale images,
where images of higher resolutions are processed by smaller network branches to
balance computational cost and performance. To integrate information from
different spatial scales, we further propose a novel cross-branch feature
interaction mechanism. To validate PIIP, we apply it to various perception
models and a representative multimodal large language model called LLaVA, and
conduct extensive experiments on various tasks such as object detection,
segmentation, image classification and multimodal understanding. PIIP achieves
superior performance compared to single-branch and existing multi-resolution
approaches with lower computational cost. When applied to InternViT-6B, a
large-scale vision foundation model, PIIP can improve its performance by 1%-2%
on detection and segmentation with only 40%-60% of the original computation,
finally achieving 60.0 box AP on MS COCO and 59.7 mIoU on ADE20K. For
multimodal understanding, our PIIP-LLaVA achieves 73.0% accuracy on TextVQA and
74.5% on MMBench with only 2.8M training data. Our code is released at
https://github.com/OpenGVLab/PIIP.","['cs.CV', 'cs.CL']",http://arxiv.org/abs/2501.07783v1
"SST-EM: Advanced Metrics for Evaluating Semantic, Spatial and Temporal Aspects in Video Editing","Video editing models have advanced significantly, but evaluating their
performance remains challenging. Traditional metrics, such as CLIP text and
image scores, often fall short: text scores are limited by inadequate training
data and hierarchical dependencies, while image scores fail to assess temporal
consistency. We present SST-EM (Semantic, Spatial, and Temporal Evaluation
Metric), a novel evaluation framework that leverages modern Vision-Language
Models (VLMs), Object Detection, and Temporal Consistency checks. SST-EM
comprises four components: (1) semantic extraction from frames using a VLM, (2)
primary object tracking with Object Detection, (3) focused object refinement
via an LLM agent, and (4) temporal consistency assessment using a Vision
Transformer (ViT). These components are integrated into a unified metric with
weights derived from human evaluations and regression analysis. The name SST-EM
reflects its focus on Semantic, Spatial, and Temporal aspects of video
evaluation. SST-EM provides a comprehensive evaluation of semantic fidelity and
temporal smoothness in video editing. The source code is available in the
\textbf{\href{https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git}{GitHub
Repository}}.","['cs.CV', 'cs.CL']",http://arxiv.org/abs/2501.07554v1
ML Mule: Mobile-Driven Context-Aware Collaborative Learning,"Artificial intelligence has been integrated into nearly every aspect of daily
life, powering applications from object detection with computer vision to large
language models for writing emails and compact models in smart homes. These
machine learning models cater to individual users but are often detached from
them, as they are typically stored and processed in centralized data centers.
This centralized approach raises privacy concerns, incurs high infrastructure
costs, and struggles with personalization. Federated and fully decentralized
learning methods have been proposed to address these issues, but they still
depend on centralized servers or face slow convergence due to communication
constraints. To overcome these challenges, we propose ML Mule, a approach that
utilizes individual mobile devices as 'Mules' to train and transport model
snapshots as they move through physical spaces, sharing these models with the
physical 'Spaces' they inhabit. This method implicitly forms affinity groups
among devices associated with users who share particular spaces, enabling
collaborative model evolution, and protecting users' privacy. Our approach
addresses several major shortcomings of traditional, federated, and fully
decentralized learning systems. The proposed framework represents a new class
of machine learning methods that are more robust, distributed, and
personalized, bringing the field closer to realizing the original vision of
intelligent, adaptive, and genuinely context-aware smart environments. The
results show that ML Mule converges faster and achieves higher model accuracy
compared to other existing methods.","['cs.LG', 'cs.HC']",http://arxiv.org/abs/2501.07536v1
TimberVision: A Multi-Task Dataset and Framework for Log-Component Segmentation and Tracking in Autonomous Forestry Operations,"Timber represents an increasingly valuable and versatile resource. However,
forestry operations such as harvesting, handling and measuring logs still
require substantial human labor in remote environments posing significant
safety risks. Progressively automating these tasks has the potential of
increasing their efficiency as well as safety, but requires an accurate
detection of individual logs as well as live trees and their context. Although
initial approaches have been proposed for this challenging application domain,
specialized data and algorithms are still too scarce to develop robust
solutions. To mitigate this gap, we introduce the TimberVision dataset,
consisting of more than 2k annotated RGB images containing a total of 51k trunk
components including cut and lateral surfaces, thereby surpassing any existing
dataset in this domain in terms of both quantity and detail by a large margin.
Based on this data, we conduct a series of ablation experiments for oriented
object detection and instance segmentation and evaluate the influence of
multiple scene parameters on model performance. We introduce a generic
framework to fuse the components detected by our models for both tasks into
unified trunk representations. Furthermore, we automatically derive geometric
properties and apply multi-object tracking to further enhance robustness. Our
detection and tracking approach provides highly descriptive and accurate trunk
representations solely from RGB image data, even under challenging
environmental conditions. Our solution is suitable for a wide range of
application scenarios and can be readily combined with other sensor modalities.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2501.07360v1
Toward Realistic Camouflaged Object Detection: Benchmarks and Method,"Camouflaged object detection (COD) primarily relies on semantic or instance
segmentation methods. While these methods have made significant advancements in
identifying the contours of camouflaged objects, they may be inefficient or
cost-effective for tasks that only require the specific location of the object.
Object detection algorithms offer an optimized solution for Realistic
Camouflaged Object Detection (RCOD) in such cases. However, detecting
camouflaged objects remains a formidable challenge due to the high degree of
similarity between the features of the objects and their backgrounds. Unlike
segmentation methods that perform pixel-wise comparisons to differentiate
between foreground and background, object detectors omit this analysis, further
aggravating the challenge. To solve this problem, we propose a camouflage-aware
feature refinement (CAFR) strategy. Since camouflaged objects are not rare
categories, CAFR fully utilizes a clear perception of the current object within
the prior knowledge of large models to assist detectors in deeply understanding
the distinctions between background and foreground. Specifically, in CAFR, we
introduce the Adaptive Gradient Propagation (AGP) module that fine-tunes all
feature extractor layers in large detection models to fully refine
class-specific features from camouflaged contexts. We then design the Sparse
Feature Refinement (SFR) module that optimizes the transformer-based feature
extractor to focus primarily on capturing class-specific features in
camouflaged scenarios. To facilitate the assessment of RCOD tasks, we manually
annotate the labels required for detection on three existing segmentation COD
datasets, creating a new benchmark for RCOD tasks. Code and datasets are
available at: https://github.com/zhimengXin/RCOD.",['cs.CV'],http://arxiv.org/abs/2501.07297v1
Dual Scale-aware Adaptive Masked Knowledge Distillation for Object Detection,"Recent feature masking knowledge distillation methods make use of attention
mechanisms to identify either important spatial regions or channel clues for
discriminative feature reconstruction. However, most of existing strategies
perform global attention-guided feature masking distillation without delving
into fine-grained visual clues in feature maps. In particular, uncovering
locality-aware clues across different scales are conducive to reconstructing
region-aware features, thereby significantly benefiting distillation
performance. In this study, we propose a fine-grained adaptive feature masking
distillation framework for accurate object detection. Different from previous
methods in which global masking is performed on single-scale feature maps, we
explore the scale-aware feature masking by performing feature distillation
across various scales, such that the object-aware locality is encoded for
improved feature reconstruction. In addition, our fine-grained feature
distillation strategy is combined with a masking logits distillation scheme in
which logits difference between teacher and student networks is utilized to
guide the distillation process. Thus, it can help the student model to better
learn from the teacher counterpart with improved knowledge transfer. Extensive
experiments for detection task demonstrate the superiority of our method. For
example, when RetinaNet, RepPoints and Cascade Mask RCNN are used as teacher
detectors, the student network achieves mAP scores of 41.5\%, 42.9\%, and
42.6\%, respectively, outperforming state-of-the-art methods such as DMKD and
FreeKD.",['cs.CV'],http://arxiv.org/abs/2501.07101v1
On the Robustness of Object Detection Models on Aerial Images,"The robustness of object detection models is a major concern when applied to
real-world scenarios. The performance of most models tends to degrade when
confronted with images affected by corruptions, since they are usually trained
and evaluated on clean datasets. While numerous studies have explored the
robustness of object detection models on natural images, there is a paucity of
research focused on models applied to aerial images, which feature complex
backgrounds, substantial variations in scales, and orientations of objects.
This paper addresses the challenge of assessing the robustness of object
detection models on aerial images, with a specific emphasis on scenarios where
images are affected by clouds. In this study, we introduce two novel benchmarks
based on DOTA-v1.0. The first benchmark encompasses 19 prevalent corruptions,
while the second focuses on the cloud-corrupted condition-a phenomenon uncommon
in natural images yet frequent in aerial photography. We systematically
evaluate the robustness of mainstream object detection models and perform
necessary ablation experiments. Through our investigations, we find that
rotation-invariant modeling and enhanced backbone architectures can improve the
robustness of models. Furthermore, increasing the capacity of Transformer-based
backbones can strengthen their robustness. The benchmarks we propose and our
comprehensive experimental analyses can facilitate research on robust object
detection on aerial images. The codes and datasets are available at:
https://github.com/hehaodong530/DOTA-C.",['cs.CV'],http://arxiv.org/abs/2308.15378v2
SL-YOLO: A Stronger and Lighter Drone Target Detection Model,"Detecting small objects in complex scenes, such as those captured by drones,
is a daunting challenge due to the difficulty in capturing the complex features
of small targets. While the YOLO family has achieved great success in large
target detection, its performance is less than satisfactory when faced with
small targets. Because of this, this paper proposes a revolutionary model
SL-YOLO (Stronger and Lighter YOLO) that aims to break the bottleneck of small
target detection. We propose the Hierarchical Extended Path Aggregation Network
(HEPAN), a pioneering cross-scale feature fusion method that can ensure
unparalleled detection accuracy even in the most challenging environments. At
the same time, without sacrificing detection capabilities, we design the C2fDCB
lightweight module and add the SCDown downsampling module to greatly reduce the
model's parameters and computational complexity. Our experimental results on
the VisDrone2019 dataset reveal a significant improvement in performance, with
mAP@0.5 jumping from 43.0% to 46.9% and mAP@0.5:0.95 increasing from 26.0% to
28.9%. At the same time, the model parameters are reduced from 11.1M to 9.6M,
and the FPS can reach 132, making it an ideal solution for real-time small
object detection in resource-constrained environments.",['cs.CV'],http://arxiv.org/abs/2411.11477v3
Visual question answering: from early developments to recent advances -- a survey,"Visual Question Answering (VQA) is an evolving research field aimed at
enabling machines to answer questions about visual content by integrating image
and language processing techniques such as feature extraction, object
detection, text embedding, natural language understanding, and language
generation. With the growth of multimodal data research, VQA has gained
significant attention due to its broad applications, including interactive
educational tools, medical image diagnosis, customer service, entertainment,
and social media captioning. Additionally, VQA plays a vital role in assisting
visually impaired individuals by generating descriptive content from images.
This survey introduces a taxonomy of VQA architectures, categorizing them based
on design choices and key components to facilitate comparative analysis and
evaluation. We review major VQA approaches, focusing on deep learning-based
methods, and explore the emerging field of Large Visual Language Models (LVLMs)
that have demonstrated success in multimodal tasks like VQA. The paper further
examines available datasets and evaluation metrics essential for measuring VQA
system performance, followed by an exploration of real-world VQA applications.
Finally, we highlight ongoing challenges and future directions in VQA research,
presenting open questions and potential areas for further development. This
survey serves as a comprehensive resource for researchers and practitioners
interested in the latest advancements and future","['cs.CV', 'cs.MM']",http://arxiv.org/abs/2501.03939v2
CoreNet: Conflict Resolution Network for Point-Pixel Misalignment and Sub-Task Suppression of 3D LiDAR-Camera Object Detection,"Fusing multi-modality inputs from different sensors is an effective way to
improve the performance of 3D object detection. However, current methods
overlook two important conflicts: point-pixel misalignment and sub-task
suppression. The former means a pixel feature from the opaque object is
projected to multiple point features of the same ray in the world space, and
the latter means the classification prediction and bounding box regression may
cause mutual suppression. In this paper, we propose a novel method named
Conflict Resolution Network (CoreNet) to address the aforementioned issues.
Specifically, we first propose a dual-stream transformation module to tackle
point-pixel misalignment. It consists of ray-based and point-based 2D-to-BEV
transformations. Both of them achieve approximately unique mapping from the
image space to the world space. Moreover, we introduce a task-specific
predictor to tackle sub-task suppression. It uses the dual-branch structure
which adopts class-specific query and Bbox-specific query to corresponding
sub-tasks. Each task-specific query is constructed of task-specific feature and
general feature, which allows the heads to adaptively select information of
interest based on different sub-tasks. Experiments on the large-scale nuScenes
dataset demonstrate the superiority of our proposed CoreNet, by achieving
75.6\% NDS and 73.3\% mAP on the nuScenes test set without test-time
augmentation and model ensemble techniques. The ample ablation study also
demonstrates the effectiveness of each component. The code is released on
https://github.com/liyih/CoreNet.",['cs.CV'],http://arxiv.org/abs/2501.06550v1
PromptDet: A Lightweight 3D Object Detection Framework with LiDAR Prompts,"Multi-camera 3D object detection aims to detect and localize objects in 3D
space using multiple cameras, which has attracted more attention due to its
cost-effectiveness trade-off. However, these methods often struggle with the
lack of accurate depth estimation caused by the natural weakness of the camera
in ranging. Recently, multi-modal fusion and knowledge distillation methods for
3D object detection have been proposed to solve this problem, which are
time-consuming during the training phase and not friendly to memory cost. In
light of this, we propose PromptDet, a lightweight yet effective 3D object
detection framework motivated by the success of prompt learning in 2D
foundation model. Our proposed framework, PromptDet, comprises two integral
components: a general camera-based detection module, exemplified by models like
BEVDet and BEVDepth, and a LiDAR-assisted prompter. The LiDAR-assisted prompter
leverages the LiDAR points as a complementary signal, enriched with a minimal
set of additional trainable parameters. Notably, our framework is flexible due
to our prompt-like design, which can not only be used as a lightweight
multi-modal fusion method but also as a camera-only method for 3D object
detection during the inference phase. Extensive experiments on nuScenes
validate the effectiveness of the proposed PromptDet. As a multi-modal
detector, PromptDet improves the mAP and NDS by at most 22.8\% and 21.1\% with
fewer than 2\% extra parameters compared with the camera-only baseline. Without
LiDAR points, PromptDet still achieves an improvement of at most 2.4\% mAP and
4.0\% NDS with almost no impact on camera detection inference time.",['cs.CV'],http://arxiv.org/abs/2412.12460v2
CPDR: Towards Highly-Efficient Salient Object Detection via Crossed Post-decoder Refinement,"Most of the current salient object detection approaches use deeper networks
with large backbones to produce more accurate predictions, which results in a
significant increase in computational complexity. A great number of network
designs follow the pure UNet and Feature Pyramid Network (FPN) architecture
which has limited feature extraction and aggregation ability which motivated us
to design a lightweight post-decoder refinement module, the crossed
post-decoder refinement (CPDR) to enhance the feature representation of a
standard FPN or U-Net framework. Specifically, we introduce the Attention Down
Sample Fusion (ADF), which employs channel attention mechanisms with attention
maps generated by high-level representation to refine the low-level features,
and Attention Up Sample Fusion (AUF), leveraging the low-level information to
guide the high-level features through spatial attention. Additionally, we
proposed the Dual Attention Cross Fusion (DACF) upon ADFs and AUFs, which
reduces the number of parameters while maintaining the performance. Experiments
on five benchmark datasets demonstrate that our method outperforms previous
state-of-the-art approaches.",['cs.CV'],http://arxiv.org/abs/2501.06441v1
FocusDD: Real-World Scene Infusion for Robust Dataset Distillation,"Dataset distillation has emerged as a strategy to compress real-world
datasets for efficient training. However, it struggles with large-scale and
high-resolution datasets, limiting its practicality. This paper introduces a
novel resolution-independent dataset distillation method Focus ed Dataset
Distillation (FocusDD), which achieves diversity and realism in distilled data
by identifying key information patches, thereby ensuring the generalization
capability of the distilled dataset across different network architectures.
Specifically, FocusDD leverages a pre-trained Vision Transformer (ViT) to
extract key image patches, which are then synthesized into a single distilled
image. These distilled images, which capture multiple targets, are suitable not
only for classification tasks but also for dense tasks such as object
detection. To further improve the generalization of the distilled dataset, each
synthesized image is augmented with a downsampled view of the original image.
Experimental results on the ImageNet-1K dataset demonstrate that, with 100
images per class (IPC), ResNet50 and MobileNet-v2 achieve validation accuracies
of 71.0% and 62.6%, respectively, outperforming state-of-the-art methods by
2.8% and 4.7%. Notably, FocusDD is the first method to use distilled datasets
for object detection tasks. On the COCO2017 dataset, with an IPC of 50,
YOLOv11n and YOLOv11s achieve 24.4% and 32.1% mAP, respectively, further
validating the effectiveness of our approach.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.06405v1
A Holistically Point-guided Text Framework for Weakly-Supervised Camouflaged Object Detection,"Weakly-Supervised Camouflaged Object Detection (WSCOD) has gained popularity
for its promise to train models with weak labels to segment objects that
visually blend into their surroundings. Recently, some methods using
sparsely-annotated supervision shown promising results through scribbling in
WSCOD, while point-text supervision remains underexplored. Hence, this paper
introduces a novel holistically point-guided text framework for WSCOD by
decomposing into three phases: segment, choose, train. Specifically, we propose
Point-guided Candidate Generation (PCG), where the point's foreground serves as
a correction for the text path to explicitly correct and rejuvenate the loss
detection object during the mask generation process (SEGMENT). We also
introduce a Qualified Candidate Discriminator (QCD) to choose the optimal mask
from a given text prompt using CLIP (CHOOSE), and employ the chosen pseudo mask
for training with a self-supervised Vision Transformer (TRAIN). Additionally,
we developed a new point-supervised dataset (P2C-COD) and a text-supervised
dataset (T-COD). Comprehensive experiments on four benchmark datasets
demonstrate our method outperforms state-of-the-art methods by a large margin,
and also outperforms some existing fully-supervised camouflaged object
detection methods.",['cs.CV'],http://arxiv.org/abs/2501.06038v1
Minimizing Occlusion Effect on Multi-View Camera Perception in BEV with Multi-Sensor Fusion,"Autonomous driving technology is rapidly evolving, offering the potential for
safer and more efficient transportation. However, the performance of these
systems can be significantly compromised by the occlusion on sensors due to
environmental factors like dirt, dust, rain, and fog. These occlusions severely
affect vision-based tasks such as object detection, vehicle segmentation, and
lane recognition. In this paper, we investigate the impact of various kinds of
occlusions on camera sensor by projecting their effects from multi-view camera
images of the nuScenes dataset into the Bird's-Eye View (BEV) domain. This
approach allows us to analyze how occlusions spatially distribute and influence
vehicle segmentation accuracy within the BEV domain. Despite significant
advances in sensor technology and multi-sensor fusion, a gap remains in the
existing literature regarding the specific effects of camera occlusions on
BEV-based perception systems. To address this gap, we use a multi-sensor fusion
technique that integrates LiDAR and radar sensor data to mitigate the
performance degradation caused by occluded cameras. Our findings demonstrate
that this approach significantly enhances the accuracy and robustness of
vehicle segmentation tasks, leading to more reliable autonomous driving
systems.",['cs.CV'],http://arxiv.org/abs/2501.05997v1
Strip R-CNN: Large Strip Convolution for Remote Sensing Object Detection,"While witnessed with rapid development, remote sensing object detection
remains challenging for detecting high aspect ratio objects. This paper shows
that large strip convolutions are good feature representation learners for
remote sensing object detection and can detect objects of various aspect ratios
well. Based on large strip convolutions, we build a new network architecture
called Strip R-CNN, which is simple, efficient, and powerful. Unlike recent
remote sensing object detectors that leverage large-kernel convolutions with
square shapes, our Strip R-CNN takes advantage of sequential orthogonal large
strip convolutions to capture spatial information. In addition, we enhance the
localization capability of remote-sensing object detectors by decoupling the
detection heads and equipping the localization head with strip convolutions to
better localize the target objects. Extensive experiments on several
benchmarks, e.g., DOTA, FAIR1M, HRSC2016, and DIOR, show that our Strip R-CNN
can largely improve previous works. Notably, our 30M model achieves 82.75% mAP
on DOTA-v1.0, setting a new state-of-the-art record.Code is available at
https://github.com/YXB-NKU/Strip-R-CNN.",['cs.CV'],http://arxiv.org/abs/2501.03775v3
"EDNet: Edge-Optimized Small Target Detection in UAV Imagery -- Faster Context Attention, Better Feature Fusion, and Hardware Acceleration","Detecting small targets in drone imagery is challenging due to low
resolution, complex backgrounds, and dynamic scenes. We propose EDNet, a novel
edge-target detection framework built on an enhanced YOLOv10 architecture,
optimized for real-time applications without post-processing. EDNet
incorporates an XSmall detection head and a Cross Concat strategy to improve
feature fusion and multi-scale context awareness for detecting tiny targets in
diverse environments. Our unique C2f-FCA block employs Faster Context Attention
to enhance feature extraction while reducing computational complexity. The WIoU
loss function is employed for improved bounding box regression. With seven
model sizes ranging from Tiny to XL, EDNet accommodates various deployment
environments, enabling local real-time inference and ensuring data privacy.
Notably, EDNet achieves up to a 5.6% gain in mAP@50 with significantly fewer
parameters. On an iPhone 12, EDNet variants operate at speeds ranging from 16
to 55 FPS, providing a scalable and efficient solution for edge-based object
detection in challenging drone imagery. The source code and pre-trained models
are available at: https://github.com/zsniko/EDNet.","['cs.CV', 'cs.AI', 'cs.LG']",http://arxiv.org/abs/2501.05885v1
Zero-shot Shark Tracking and Biometrics from Aerial Imagery,"The recent widespread adoption of drones for studying marine animals provides
opportunities for deriving biological information from aerial imagery. The
large scale of imagery data acquired from drones is well suited for machine
learning (ML) analysis. Development of ML models for analyzing marine animal
aerial imagery has followed the classical paradigm of training, testing, and
deploying a new model for each dataset, requiring significant time, human
effort, and ML expertise. We introduce Frame Level ALIgment and tRacking
(FLAIR), which leverages the video understanding of Segment Anything Model 2
(SAM2) and the vision-language capabilities of Contrastive Language-Image
Pre-training (CLIP). FLAIR takes a drone video as input and outputs
segmentation masks of the species of interest across the video. Notably, FLAIR
leverages a zero-shot approach, eliminating the need for labeled data, training
a new model, or fine-tuning an existing model to generalize to other species.
With a dataset of 18,000 drone images of Pacific nurse sharks, we trained
state-of-the-art object detection models to compare against FLAIR. We show that
FLAIR massively outperforms these object detectors and performs competitively
against two human-in-the-loop methods for prompting SAM2, achieving a Dice
score of 0.81. FLAIR readily generalizes to other shark species without
additional human effort and can be combined with novel heuristics to
automatically extract relevant information including length and tailbeat
frequency. FLAIR has significant potential to accelerate aerial imagery
analysis workflows, requiring markedly less human effort and expertise than
traditional machine learning workflows, while achieving superior accuracy. By
reducing the effort required for aerial imagery analysis, FLAIR allows
scientists to spend more time interpreting results and deriving insights about
marine ecosystems.","['cs.CV', 'cs.AI', 'q-bio.QM']",http://arxiv.org/abs/2501.05717v1
Approximate Supervised Object Distance Estimation on Unmanned Surface Vehicles,"Unmanned surface vehicles (USVs) and boats are increasingly important in
maritime operations, yet their deployment is limited due to costly sensors and
complexity. LiDAR, radar, and depth cameras are either costly, yield sparse
point clouds or are noisy, and require extensive calibration. Here, we
introduce a novel approach for approximate distance estimation in USVs using
supervised object detection. We collected a dataset comprising images with
manually annotated bounding boxes and corresponding distance measurements.
Leveraging this data, we propose a specialized branch of an object detection
model, not only to detect objects but also to predict their distances from the
USV. This method offers a cost-efficient and intuitive alternative to
conventional distance measurement techniques, aligning more closely with human
estimation capabilities. We demonstrate its application in a marine assistance
system that alerts operators to nearby objects such as boats, buoys, or other
waterborne hazards.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.05567v1
Performance of YOLOv7 in Kitchen Safety While Handling Knife,"Safe knife practices in the kitchen significantly reduce the risk of cuts,
injuries, and serious accidents during food preparation. Using YOLOv7, an
advanced object detection model, this study focuses on identifying safety risks
during knife handling, particularly improper finger placement and blade contact
with hand. The model's performance was evaluated using metrics such as
precision, recall, mAP50, and mAP50-95. The results demonstrate that YOLOv7
achieved its best performance at epoch 31, with a mAP50-95 score of 0.7879,
precision of 0.9063, and recall of 0.7503. These findings highlight YOLOv7's
potential to accurately detect knife-related hazards, promoting the development
of improved kitchen safety.",['cs.CV'],http://arxiv.org/abs/2501.05399v1
Sequential PatchCore: Anomaly Detection for Surface Inspection using Synthetic Impurities,"The appearance of surface impurities (e.g., water stains, fingerprints,
stickers) is an often-mentioned issue that causes degradation of automated
visual inspection systems. At the same time, synthetic data generation
techniques for visual surface inspection have focused primarily on generating
perfect examples and defects, disregarding impurities. This study highlights
the importance of considering impurities when generating synthetic data. We
introduce a procedural method to include photorealistic water stains in
synthetic data. The synthetic datasets are generated to correspond to real
datasets and are further used to train an anomaly detection model and
investigate the influence of water stains. The high-resolution images used for
surface inspection lead to memory bottlenecks during anomaly detection
training. To address this, we introduce Sequential PatchCore - a method to
build coresets sequentially and make training on large images using
consumer-grade hardware tractable. This allows us to perform transfer learning
using coresets pre-trained on different dataset versions. Our results show the
benefits of using synthetic data for pre-training an explicit coreset anomaly
model and the extended performance benefits of finetuning the coreset using
real data. We observed how the impurities and labelling ambiguity lower the
model performance and have additionally reported the defect-wise recall to
provide an industrially relevant perspective on model performance.","['cs.CV', 'cs.GR', 'cs.LG', '68U05, 68U10', 'I.2.1; I.2.10; I.4.6; I.4.9; I.4.7; I.3.8; I.3.6; I.3.5; I.3.7;\n  I.5.4; J.6; J.7']",http://arxiv.org/abs/2501.09579v1
Model-Based Transfer Learning for Contextual Reinforcement Learning,"Deep reinforcement learning (RL) is a powerful approach to complex decision
making. However, one issue that limits its practical application is its
brittleness, sometimes failing to train in the presence of small changes in the
environment. Motivated by the success of zero-shot transfer-where pre-trained
models perform well on related tasks-we consider the problem of selecting a
good set of training tasks to maximize generalization performance across a
range of tasks. Given the high cost of training, it is critical to select
training tasks strategically, but not well understood how to do so. We hence
introduce Model-Based Transfer Learning (MBTL), which layers on top of existing
RL methods to effectively solve contextual RL problems. MBTL models the
generalization performance in two parts: 1) the performance set point, modeled
using Gaussian processes, and 2) performance loss (generalization gap), modeled
as a linear function of contextual similarity. MBTL combines these two pieces
of information within a Bayesian optimization (BO) framework to strategically
select training tasks. We show theoretically that the method exhibits sublinear
regret in the number of training tasks and discuss conditions to further
tighten regret bounds. We experimentally validate our methods using urban
traffic and standard continuous control benchmarks. The experimental results
suggest that MBTL can achieve up to 43x improved sample efficiency compared
with canonical independent training and multi-task training. Further
experiments demonstrate the efficacy of BO and the insensitivity to the
underlying RL algorithm and hyperparameters. This work lays the foundations for
investigating explicit modeling of generalization, thereby enabling principled
yet effective methods for contextual RL.",['cs.LG'],http://arxiv.org/abs/2408.04498v3
An analysis of data variation and bias in image-based dermatological datasets for machine learning classification,"AI algorithms have become valuable in aiding professionals in healthcare. The
increasing confidence obtained by these models is helpful in critical decision
demands. In clinical dermatology, classification models can detect malignant
lesions on patients' skin using only RGB images as input. However, most
learning-based methods employ data acquired from dermoscopic datasets on
training, which are large and validated by a gold standard. Clinical models aim
to deal with classification on users' smartphone cameras that do not contain
the corresponding resolution provided by dermoscopy. Also, clinical
applications bring new challenges. It can contain captures from uncontrolled
environments, skin tone variations, viewpoint changes, noises in data and
labels, and unbalanced classes. A possible alternative would be to use transfer
learning to deal with the clinical images. However, as the number of samples is
low, it can cause degradations on the model's performance; the source
distribution used in training differs from the test set. This work aims to
evaluate the gap between dermoscopic and clinical samples and understand how
the dataset variations impact training. It assesses the main differences
between distributions that disturb the model's prediction. Finally, from
experiments on different architectures, we argue how to combine the data from
divergent distributions, decreasing the impact on the model's final accuracy.","['cs.CV', 'cs.AI', 'I.5.4; J.3']",http://arxiv.org/abs/2501.08962v1
Empowering Agricultural Insights: RiceLeafBD -- A Novel Dataset and Optimal Model Selection for Rice Leaf Disease Diagnosis through Transfer Learning Technique,"The number of people living in this agricultural nation of ours, which is
surrounded by lush greenery, is growing on a daily basis. As a result of this,
the level of arable land is decreasing, as well as residential houses and
industrial factories. The food crisis is becoming the main threat for us in the
upcoming days. Because on the one hand, the population is increasing, and on
the other hand, the amount of food crop production is decreasing due to the
attack of diseases. Rice is one of the most significant cultivated crops since
it provides food for more than half of the world's population. Bangladesh is
dependent on rice (Oryza sativa) as a vital crop for its agriculture, but it
faces a significant problem as a result of the ongoing decline in rice yield
brought on by common diseases. Early disease detection is the main difficulty
in rice crop cultivation. In this paper, we proposed our own dataset, which was
collected from the Bangladesh field, and also applied deep learning and
transfer learning models for the evaluation of the datasets. We elaborately
explain our dataset and also give direction for further research work to serve
society using this dataset. We applied a light CNN model and pre-trained
InceptionNet-V2, EfficientNet-V2, and MobileNet-V2 models, which achieved 91.5%
performance for the EfficientNet-V2 model of this work. The results obtained
assaulted other models and even exceeded approaches that are considered to be
part of the state of the art. It has been demonstrated by this study that it is
possible to precisely and effectively identify diseases that affect rice leaves
using this unbiased datasets. After analysis of the performance of different
models, the proposed datasets are significant for the society for research work
to provide solutions for decreasing rice leaf disease.",['cs.CV'],http://arxiv.org/abs/2501.08912v1
Detecting Wildfire Flame and Smoke through Edge Computing using Transfer Learning Enhanced Deep Learning Models,"Autonomous unmanned aerial vehicles (UAVs) integrated with edge computing
capabilities empower real-time data processing directly on the device,
dramatically reducing latency in critical scenarios such as wildfire detection.
This study underscores Transfer Learning's (TL) significance in boosting the
performance of object detectors for identifying wildfire smoke and flames,
especially when trained on limited datasets, and investigates the impact TL has
on edge computing metrics. With the latter focusing how TL-enhanced You Only
Look Once (YOLO) models perform in terms of inference time, power usage, and
energy consumption when using edge computing devices. This study utilizes the
Aerial Fire and Smoke Essential (AFSE) dataset as the target, with the Flame
and Smoke Detection Dataset (FASDD) and the Microsoft Common Objects in Context
(COCO) dataset serving as source datasets. We explore a two-stage cascaded TL
method, utilizing D-Fire or FASDD as initial stage target datasets and AFSE as
the subsequent stage. Through fine-tuning, TL significantly enhances detection
precision, achieving up to 79.2% mean Average Precision (mAP@0.5), reduces
training time, and increases model generalizability across the AFSE dataset.
However, cascaded TL yielded no notable improvements and TL alone did not
benefit the edge computing metrics evaluated. Lastly, this work found that
YOLOv5n remains a powerful model when lacking hardware acceleration, finding
that YOLOv5n can process images nearly twice as fast as its newer counterpart,
YOLO11n. Overall, the results affirm TL's role in augmenting the accuracy of
object detectors while also illustrating that additional enhancements are
needed to improve edge computing performance.","['cs.CV', 'eess.IV']",http://arxiv.org/abs/2501.08639v1
Continual Deep Active Learning for Medical Imaging: Replay-Base Architecture for Context Adaptation,"Deep Learning for medical imaging faces challenges in adapting and
generalizing to new contexts. Additionally, it often lacks sufficient labeled
data for specific tasks requiring significant annotation effort. Continual
Learning (CL) tackles adaptability and generalizability by enabling lifelong
learning from a data stream while mitigating forgetting of previously learned
knowledge. Active Learning (AL) reduces the number of required annotations for
effective training. This work explores both approaches (CAL) to develop a novel
framework for robust medical image analysis. Based on the automatic recognition
of shifts in image characteristics, Replay-Base Architecture for Context
Adaptation (RBACA) employs a CL rehearsal method to continually learn from
diverse contexts, and an AL component to select the most informative instances
for annotation. A novel approach to evaluate CAL methods is established using a
defined metric denominated IL-Score, which allows for the simultaneous
assessment of transfer learning, forgetting, and final model performance. We
show that RBACA works in domain and class-incremental learning scenarios, by
assessing its IL-Score on the segmentation and diagnosis of cardiac images. The
results show that RBACA outperforms a baseline framework without CAL, and a
state-of-the-art CAL method across various memory sizes and annotation budgets.
Our code is available in https://github.com/RuiDaniel/RBACA .","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2501.08245v1
Optimal Policy Adaptation under Covariate Shift,"Transfer learning of prediction models has been extensively studied, while
the corresponding policy learning approaches are rarely discussed. In this
paper, we propose principled approaches for learning the optimal policy in the
target domain by leveraging two datasets: one with full information from the
source domain and the other from the target domain with only covariates. First,
under the setting of covariate shift, we formulate the problem from a
perspective of causality and present the identifiability assumptions for the
reward induced by a given policy. Then, we derive the efficient influence
function and the semiparametric efficiency bound for the reward. Based on this,
we construct a doubly robust and semiparametric efficient estimator for the
reward and then learn the optimal policy by optimizing the estimated reward.
Moreover, we theoretically analyze the bias and the generalization error bound
for the learned policy. Furthermore, in the presence of both covariate and
concept shifts, we propose a novel sensitivity analysis method to evaluate the
robustness of the proposed policy learning approach. Extensive experiments
demonstrate that the approach not only estimates the reward more accurately but
also yields a policy that closely approximates the theoretically optimal
policy.",['cs.LG'],http://arxiv.org/abs/2501.08067v1
FoMo: A Foundation Model for Mobile Traffic Forecasting with Diffusion Model,"Mobile traffic forecasting allows operators to anticipate network dynamics
and performance in advance, offering substantial potential for enhancing
service quality and improving user experience. However, existing models are
often task-oriented and are trained with tailored data, which limits their
effectiveness in diverse mobile network tasks of Base Station (BS) deployment,
resource allocation, energy optimization, etc. and hinders generalization
across different urban environments. Foundation models have made remarkable
strides across various domains of NLP and CV due to their multi-tasking
adaption and zero/few-shot learning capabilities. In this paper, we propose an
innovative Foundation model for Mo}bile traffic forecasting (FoMo), aiming to
handle diverse forecasting tasks of short/long-term predictions and
distribution generation across multiple cities to support network planning and
optimization. FoMo combines diffusion models and transformers, where various
spatio-temporal masks are proposed to enable FoMo to learn intrinsic features
of different tasks, and a contrastive learning strategy is developed to capture
the correlations between mobile traffic and urban contexts, thereby improving
its transfer learning capability. Extensive experiments on 9 real-world
datasets demonstrate that FoMo outperforms current models concerning diverse
forecasting tasks and zero/few-shot learning, showcasing a strong universality.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2410.15322v2
BayesAdapter: enhanced uncertainty estimation in CLIP few-shot adaptation,"The emergence of large pre-trained vision-language models (VLMs) represents a
paradigm shift in machine learning, with unprecedented results in a broad span
of visual recognition tasks. CLIP, one of the most popular VLMs, has exhibited
remarkable zero-shot and transfer learning capabilities in classification. To
transfer CLIP to downstream tasks, adapters constitute a parameter-efficient
approach that avoids backpropagation through the large model (unlike related
prompt learning methods). However, CLIP adapters have been developed to target
discriminative performance, and the quality of their uncertainty estimates has
been overlooked. In this work we show that the discriminative performance of
state-of-the-art CLIP adapters does not always correlate with their uncertainty
estimation capabilities, which are essential for a safe deployment in
real-world scenarios. We also demonstrate that one of such adapters is obtained
through MAP inference from a more general probabilistic framework. Based on
this observation we introduce BayesAdapter, which leverages Bayesian inference
to estimate a full probability distribution instead of a single point, better
capturing the variability inherent in the parameter space. In a comprehensive
empirical evaluation we show that our approach obtains high quality uncertainty
estimates in the predictions, standing out in calibration and selective
classification. Our code will be publicly available upon acceptance of the
paper.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2412.09718v2
Exploring the Use of Contrastive Language-Image Pre-Training for Human Posture Classification: Insights from Yoga Pose Analysis,"Accurate human posture classification in images and videos is crucial for
automated applications across various fields, including work safety, physical
rehabilitation, sports training, or daily assisted living. Recently, multimodal
learning methods, such as Contrastive Language-Image Pretraining (CLIP), have
advanced significantly in jointly understanding images and text. This study
aims to assess the effectiveness of CLIP in classifying human postures,
focusing on its application in yoga. Despite the initial limitations of the
zero-shot approach, applying transfer learning on 15,301 images (real and
synthetic) with 82 classes has shown promising results. The article describes
the full procedure for fine-tuning, including the choice for image description
syntax, models and hyperparameters adjustment. The fine-tuned CLIP model,
tested on 3826 images, achieves an accuracy of over 85%, surpassing the current
state-of-the-art of previous works on the same dataset by approximately 6%, its
training time being 3.5 times lower than what is needed to fine-tune a
YOLOv8-based model. For more application-oriented scenarios, with smaller
datasets of six postures each, containing 1301 and 401 training images, the
fine-tuned models attain an accuracy of 98.8% and 99.1%, respectively.
Furthermore, our experiments indicate that training with as few as 20 images
per pose can yield around 90% accuracy in a six-class dataset. This study
demonstrates that this multimodal technique can be effectively used for yoga
pose classification, and possibly for human posture classification, in general.
Additionally, CLIP inference time (around 7 ms) supports that the model can be
integrated into automated systems for posture evaluation, e.g., for developing
a real-time personal yoga assistant for performance assessment.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.07221v1
AlgoRxplorers | Precision in Mutation -- Enhancing Drug Design with Advanced Protein Stability Prediction Tools,"Predicting the impact of single-point amino acid mutations on protein
stability is essential for understanding disease mechanisms and advancing drug
development. Protein stability, quantified by changes in Gibbs free energy
($\Delta\Delta G$), is influenced by these mutations. However, the scarcity of
data and the complexity of model interpretation pose challenges in accurately
predicting stability changes. This study proposes the application of deep
neural networks, leveraging transfer learning and fusing complementary
information from different models, to create a feature-rich representation of
the protein stability landscape. We developed four models, with our third
model, ThermoMPNN+, demonstrating the best performance in predicting
$\Delta\Delta G$ values. This approach, which integrates diverse feature sets
and embeddings through latent transfusion techniques, aims to refine
$\Delta\Delta G$ predictions and contribute to a deeper understanding of
protein dynamics, potentially leading to advancements in disease research and
drug discovery.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2501.07014v1
Transfer Learning of Tabular Data by Finetuning Large Language Models,"Despite the artificial intelligence (AI) revolution, deep learning has yet to
achieve much success with tabular data due to heterogeneous feature space and
limited sample sizes without viable transfer learning. The new era of
generative AI, powered by large language models (LLM), brings unprecedented
learning opportunities to diverse data and domains. This paper investigates the
effectiveness of an LLM application programming interface (API) and transfer
learning of LLM in tabular data classification. LLM APIs respond to input text
prompts with tokenized data and instructions, whereas transfer learning
finetunes an LLM for a target classification task. This paper proposes an
end-to-end finetuning of LLM to demonstrate cross-data transfer learning on ten
benchmark data sets when large pre-trained tabular data models do not exist to
facilitate transfer learning. The proposed LLM finetuning method outperforms
state-of-the-art machine and deep learning methods on tabular data with less
than ten features - a standard feature size for tabular data sets. The transfer
learning approach uses a fraction of the computational cost of other deep
learning or API-based solutions while ensuring competitive or superior
classification performance.","['cs.LG', 'cs.AI', 'cs.CL']",http://arxiv.org/abs/2501.06863v1
"Rice Leaf Disease Detection: A Comparative Study Between CNN, Transformer and Non-neural Network Architectures","In nations such as Bangladesh, agriculture plays a vital role in providing
livelihoods for a significant portion of the population. Identifying and
classifying plant diseases early is critical to prevent their spread and
minimize their impact on crop yield and quality. Various computer vision
techniques can be used for such detection and classification. While CNNs have
been dominant on such image classification tasks, vision transformers has
become equally good in recent time also. In this paper we study the various
computer vision techniques for Bangladeshi rice leaf disease detection. We use
the Dhan-Shomadhan -- a Bangladeshi rice leaf disease dataset, to experiment
with various CNN and ViT models. We also compared the performance of such deep
neural network architecture with traditional machine learning architecture like
Support Vector Machine(SVM). We leveraged transfer learning for better
generalization with lower amount of training data. Among the models tested,
ResNet50 exhibited the best performance over other CNN and transformer-based
models making it the optimal choice for this task.",['cs.CV'],http://arxiv.org/abs/2501.06740v1
Transfer Learning on Multi-Dimensional Data: A Novel Approach to Neural Network-Based Surrogate Modeling,"The development of efficient surrogates for partial differential equations
(PDEs) is a critical step towards scalable modeling of complex, multiscale
systems-of-systems. Convolutional neural networks (CNNs) have gained popularity
as the basis for such surrogate models due to their success in capturing
high-dimensional input-output mappings and the negligible cost of a forward
pass. However, the high cost of generating training data -- typically via
classical numerical solvers -- raises the question of whether these models are
worth pursuing over more straightforward alternatives with well-established
theoretical foundations, such as Monte Carlo methods. To reduce the cost of
data generation, we propose training a CNN surrogate model on a mixture of
numerical solutions to both the $d$-dimensional problem and its
($d-1$)-dimensional approximation, taking advantage of the efficiency savings
guaranteed by the curse of dimensionality. We demonstrate our approach on a
multiphase flow test problem, using transfer learning to train a dense
fully-convolutional encoder-decoder CNN on the two classes of data. Numerical
results from a sample uncertainty quantification task demonstrate that our
surrogate model outperforms Monte Carlo with several times the data generation
budget.","['cs.LG', 'cs.NA', 'math.NA']",http://arxiv.org/abs/2410.12241v2
Transforming Social Science Research with Transfer Learning: Social Science Survey Data Integration with AI,"Large-N nationally representative surveys, which have profoundly shaped
American politics scholarship, represent related but distinct domains -a key
condition for transfer learning applications. These surveys are related through
their shared demographic, party identification, and ideological variables, yet
differ in that individual surveys often lack specific policy preference
questions that researchers require. Our study introduces a novel application of
transfer learning (TL) to address these gaps, marking the first systematic use
of TL paradigms in the context of survey data. Specifically, models pre-trained
on the Cooperative Election Study (CES) dataset are fine-tuned for use in the
American National Election Studies (ANES) dataset to predict policy questions
based on demographic variables. Even with a naive architecture, our transfer
learning approach achieves approximately 92 percentage accuracy in predicting
missing variables across surveys, demonstrating the robust potential of this
method. Beyond this specific application, our paper argues that transfer
learning is a promising framework for maximizing the utility of existing survey
data. We contend that artificial intelligence, particularly transfer learning,
opens new frontiers in social science methodology by enabling systematic
knowledge transfer between well-administered surveys that share common
variables but differ in their outcomes of interest.","['cs.AI', 'I.2.7, I.2.6, H.1.2, I.2.10']",http://arxiv.org/abs/2501.06577v1
Mathematics of Digital Twins and Transfer Learning for PDE Models,"We define a digital twin (DT) of a physical system governed by partial
differential equations (PDEs) as a model for real-time simulations and control
of the system behavior under changing conditions. We construct DTs using the
Karhunen-Lo\`{e}ve Neural Network (KL-NN) surrogate model and transfer learning
(TL). The surrogate model allows fast inference and differentiability with
respect to control parameters for control and optimization. TL is used to
retrain the model for new conditions with minimal additional data. We employ
the moment equations to analyze TL and identify parameters that can be
transferred to new conditions. The proposed analysis also guides the control
variable selection in DT to facilitate efficient TL.
  For linear PDE problems, the non-transferable parameters in the KL-NN
surrogate model can be exactly estimated from a single solution of the PDE
corresponding to the mean values of the control variables under new target
conditions. Retraining an ML model with a single solution sample is known as
one-shot learning, and our analysis shows that the one-shot TL is exact for
linear PDEs. For nonlinear PDE problems, transferring of any parameters
introduces errors. For a nonlinear diffusion PDE model, we find that for a
relatively small range of control variables, some surrogate model parameters
can be transferred without introducing a significant error, some can be
approximately estimated from the mean-field equation, and the rest can be found
using a linear residual least square problem or an ordinary linear least square
problem if a small labeled dataset for new conditions is available. The former
approach results in a one-shot TL while the latter approach is an example of a
few-shot TL. Both methods are approximate for the nonlinear PDEs.","['cs.LG', 'cs.NA', 'math.NA', 'stat.ML']",http://arxiv.org/abs/2501.06400v1
Patch-GAN Transfer Learning with Reconstructive Models for Cloud Removal,"Cloud removal plays a crucial role in enhancing remote sensing image
analysis, yet accurately reconstructing cloud-obscured regions remains a
significant challenge. Recent advancements in generative models have made the
generation of realistic images increasingly accessible, offering new
opportunities for this task. Given the conceptual alignment between image
generation and cloud removal tasks, generative models present a promising
approach for addressing cloud removal in remote sensing. In this work, we
propose a deep transfer learning approach built on a generative adversarial
network (GAN) framework to explore the potential of the novel masked
autoencoder (MAE) image reconstruction model in cloud removal. Due to the
complexity of remote sensing imagery, we further propose using a patch-wise
discriminator to determine whether each patch of the image is real or not. The
proposed reconstructive transfer learning approach demonstrates significant
improvements in cloud removal performance compared to other GAN-based methods.
Additionally, whilst direct comparisons with some of the state-of-the-art cloud
removal techniques are limited due to unclear details regarding their
train/test data splits, the proposed model achieves competitive results based
on available benchmarks.","['cs.CV', 'eess.IV']",http://arxiv.org/abs/2501.05265v1
Load Forecasting for Households and Energy Communities: Are Deep Learning Models Worth the Effort?,"Accurate load forecasting is crucial for predictive control in many energy
domain applications, with significant economic and ecological implications. To
address these implications, this study provides an extensive benchmark of
state-of-the-art deep learning models for short-term load forecasting in energy
communities. Namely, LSTM, xLSTM, and Transformers are compared with benchmarks
such as KNNs, synthetic load models, and persistence forecasting models. This
comparison considers different scales of aggregation (e.g., number of household
loads) and varying training data availability (e.g., training data time spans).
Further, the impact of transfer learning from synthetic (standard) load
profiles and the deep learning model size (i.e., parameter count) is
investigated in terms of forecasting error. Implementations are publicly
available and other researchers are encouraged to benchmark models using this
framework. Additionally, a comprehensive case study, comprising an energy
community of 50 households and a battery storage demonstrates the beneficial
financial implications of accurate predictions. Key findings of this research
include: (1) Simple persistence benchmarks outperform deep learning models for
short-term load forecasting when the available training data is limited to six
months or less; (2) Pretraining with publicly available synthetic load profiles
improves the normalized Mean Absolute Error (nMAE) by an average of 1.28%pt
during the first nine months of training data; (3) Increased aggregation
significantly enhances the performance of deep learning models relative to
persistence benchmarks; (4) Improved load forecasting, with an nMAE reduction
of 1.1%pt, translates to an economic benefit of approximately 600EUR per year
in an energy community comprising 50 households.",['cs.LG'],http://arxiv.org/abs/2501.05000v1
Deep Transfer $Q$-Learning for Offline Non-Stationary Reinforcement Learning,"In dynamic decision-making scenarios across business and healthcare,
leveraging sample trajectories from diverse populations can significantly
enhance reinforcement learning (RL) performance for specific target
populations, especially when sample sizes are limited. While existing transfer
learning methods primarily focus on linear regression settings, they lack
direct applicability to reinforcement learning algorithms. This paper pioneers
the study of transfer learning for dynamic decision scenarios modeled by
non-stationary finite-horizon Markov decision processes, utilizing neural
networks as powerful function approximators and backward inductive learning. We
demonstrate that naive sample pooling strategies, effective in regression
settings, fail in Markov decision processes.To address this challenge, we
introduce a novel ``re-weighted targeting procedure'' to construct
``transferable RL samples'' and propose ``transfer deep $Q^*$-learning'',
enabling neural network approximation with theoretical guarantees. We assume
that the reward functions are transferable and deal with both situations in
which the transition densities are transferable or nontransferable. Our
analytical techniques for transfer learning in neural network approximation and
transition density transfers have broader implications, extending to supervised
transfer learning with neural networks and domain shift scenarios. Empirical
experiments on both synthetic and real datasets corroborate the advantages of
our method, showcasing its potential for improving decision-making through
strategically constructing transferable RL samples in non-stationary
reinforcement learning contexts.","['stat.ML', 'cs.LG']",http://arxiv.org/abs/2501.04870v1
A novel Facial Recognition technique with Focusing on Masked Faces,"Recognizing the same faces with and without masks is important for ensuring
consistent identification in security, access control, and public safety. This
capability is crucial in scenarios like law enforcement, healthcare, and
surveillance, where accurate recognition must be maintained despite facial
occlusion. This research focuses on the challenge of recognizing the same faces
with and without masks by employing cosine similarity as the primary technique.
With the increased use of masks, traditional facial recognition systems face
significant accuracy issues, making it crucial to develop methods that can
reliably identify individuals in masked conditions. For that reason, this study
proposed Masked-Unmasked Face Matching Model (MUFM). This model employs
transfer learning using the Visual Geometry Group (VGG16) model to extract
significant facial features, which are subsequently classified utilizing the
K-Nearest Neighbors (K-NN) algorithm. The cosine similarity metric is employed
to compare masked and unmasked faces of the same individuals. This approach
represents a novel contribution, as the task of recognizing the same individual
with and without a mask using cosine similarity has not been previously
addressed. By integrating these advanced methodologies, the research
demonstrates effective identification of individuals despite the presence of
masks, addressing a significant limitation in traditional systems. Using data
is another essential part of this work, by collecting and preparing an image
dataset from three different sources especially some of those data are real
provided a comprehensive power of this research. The image dataset used were
already collected in three different datasets of masked and unmasked for the
same faces.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.04444v1
TADFormer : Task-Adaptive Dynamic Transformer for Efficient Multi-Task Learning,"Transfer learning paradigm has driven substantial advancements in various
vision tasks. However, as state-of-the-art models continue to grow, classical
full fine-tuning often becomes computationally impractical, particularly in
multi-task learning (MTL) setup where training complexity increases
proportional to the number of tasks. Consequently, recent studies have explored
Parameter-Efficient Fine-Tuning (PEFT) for MTL architectures. Despite some
progress, these approaches still exhibit limitations in capturing fine-grained,
task-specific features that are crucial to MTL. In this paper, we introduce
Task-Adaptive Dynamic transFormer, termed TADFormer, a novel PEFT framework
that performs task-aware feature adaptation in the fine-grained manner by
dynamically considering task-specific input contexts. TADFormer proposes the
parameter-efficient prompting for task adaptation and the Dynamic Task Filter
(DTF) to capture task information conditioned on input contexts. Experiments on
the PASCAL-Context benchmark demonstrate that the proposed method achieves
higher accuracy in dense scene understanding tasks, while reducing the number
of trainable parameters by up to 8.4 times when compared to full fine-tuning of
MTL models. TADFormer also demonstrates superior parameter efficiency and
accuracy compared to recent PEFT methods.",['cs.CV'],http://arxiv.org/abs/2501.04293v1
Transfer learning via Regularized Linear Discriminant Analysis,"Linear discriminant analysis is a widely used method for classification.
However, the high dimensionality of predictors combined with small sample sizes
often results in large classification errors. To address this challenge, it is
crucial to leverage data from related source models to enhance the
classification performance of a target model. We propose to address this
problem in the framework of transfer learning.
  In this paper, we present novel transfer learning methods via regularized
random-effects linear discriminant analysis, where the discriminant direction
is estimated as a weighted combination of ridge estimates obtained from both
the target and source models. Multiple strategies for determining these weights
are introduced and evaluated, including one that minimizes the estimation risk
of the discriminant vector and another that minimizes the classification error.
Utilizing results from random matrix theory, we explicitly derive the
asymptotic values of these weights and the associated classification error
rates in the high-dimensional setting, where $p/n \rightarrow \gamma$, with $p$
representing the predictor dimension and $n$ the sample size. We also provide
geometric interpretations of various weights and a guidance on which weights to
choose. Extensive numerical studies, including simulations and analysis of
proteomics-based 10-year cardiovascular disease risk classification,
demonstrate the effectiveness of the proposed approach.","['stat.ML', 'cs.LG']",http://arxiv.org/abs/2501.02411v3
DeepVIVONet: Using deep neural operators to optimize sensor locations with application to vortex-induced vibrations,"We introduce DeepVIVONet, a new framework for optimal dynamic reconstruction
and forecasting of the vortex-induced vibrations (VIV) of a marine riser, using
field data. We demonstrate the effectiveness of DeepVIVONet in accurately
reconstructing the motion of an off--shore marine riser by using sparse
spatio-temporal measurements. We also show the generalization of our model in
extrapolating to other flow conditions via transfer learning, underscoring its
potential to streamline operational efficiency and enhance predictive accuracy.
The trained DeepVIVONet serves as a fast and accurate surrogate model for the
marine riser, which we use in an outer--loop optimization algorithm to obtain
the optimal locations for placing the sensors. Furthermore, we employ an
existing sensor placement method based on proper orthogonal decomposition (POD)
to compare with our data-driven approach. We find that that while POD offers a
good approach for initial sensor placement, DeepVIVONet's adaptive capabilities
yield more precise and cost-effective configurations.","['cs.LG', 'math.OC', 'physics.flu-dyn']",http://arxiv.org/abs/2501.04105v1
GraphLoRA: Structure-Aware Contrastive Low-Rank Adaptation for Cross-Graph Transfer Learning,"Graph Neural Networks (GNNs) have demonstrated remarkable proficiency in
handling a range of graph analytical tasks across various domains, such as
e-commerce and social networks. Despite their versatility, GNNs face
significant challenges in transferability, limiting their utility in real-world
applications. Existing research in GNN transfer learning overlooks
discrepancies in distribution among various graph datasets, facing challenges
when transferring across different distributions. How to effectively adopt a
well-trained GNN to new graphs with varying feature and structural
distributions remains an under-explored problem. Taking inspiration from the
success of Low-Rank Adaptation (LoRA) in adapting large language models to
various domains, we propose GraphLoRA, an effective and parameter-efficient
method for transferring well-trained GNNs to diverse graph domains.
Specifically, we first propose a Structure-aware Maximum Mean Discrepancy
(SMMD) to align divergent node feature distributions across source and target
graphs. Moreover, we introduce low-rank adaptation by injecting a small
trainable GNN alongside the pre-trained one, effectively bridging structural
distribution gaps while mitigating the catastrophic forgetting. Additionally, a
structure-aware regularization objective is proposed to enhance the
adaptability of the pre-trained GNN to target graph with scarce supervision
labels. Extensive experiments on eight real-world datasets demonstrate the
effectiveness of GraphLoRA against fourteen baselines by tuning only 20% of
parameters, even across disparate graph domains. The code is available at
https://github.com/AllminerLab/GraphLoRA.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2409.16670v2
"Data-driven tool wear prediction in milling, based on a process-integrated single-sensor approach","Accurate tool wear prediction is essential for maintaining productivity and
minimizing costs in machining. However, the complex nature of the tool wear
process poses significant challenges to achieving reliable predictions. This
study explores data-driven methods, in particular deep learning, for tool wear
prediction. Traditional data-driven approaches often focus on a single process,
relying on multi-sensor setups and extensive data generation, which limits
generalization to new settings. Moreover, multi-sensor integration is often
impractical in industrial environments. To address these limitations, this
research investigates the transferability of predictive models using minimal
training data, validated across two processes. Furthermore, it uses a simple
setup with a single acceleration sensor to establish a low-cost data generation
approach that facilitates the generalization of models to other processes via
transfer learning. The study evaluates several machine learning models,
including convolutional neural networks (CNN), long short-term memory networks
(LSTM), support vector machines (SVM) and decision trees, trained on different
input formats such as feature vectors and short-time Fourier transform (STFT).
The performance of the models is evaluated on different amounts of training
data, including scenarios with significantly reduced datasets, providing
insight into their effectiveness under constrained data conditions. The results
demonstrate the potential of specific models and configurations for effective
tool wear prediction, contributing to the development of more adaptable and
efficient predictive maintenance strategies in machining. Notably, the ConvNeXt
model has an exceptional performance, achieving an 99.1% accuracy in
identifying tool wear using data from only four milling tools operated until
they are worn.","['cs.LG', 'cs.RO', 'eess.SP']",http://arxiv.org/abs/2412.19950v2
A Multimodal Lightweight Approach to Fault Diagnosis of Induction Motors in High-Dimensional Dataset,"An accurate AI-based diagnostic system for induction motors (IMs) holds the
potential to enhance proactive maintenance, mitigating unplanned downtime and
curbing overall maintenance costs within an industrial environment. Notably,
among the prevalent faults in IMs, a Broken Rotor Bar (BRB) fault is frequently
encountered. Researchers have proposed various fault diagnosis approaches using
signal processing (SP), machine learning (ML), deep learning (DL), and hybrid
architectures for BRB faults. One limitation in the existing literature is the
training of these architectures on relatively small datasets, risking
overfitting when implementing such systems in industrial environments. This
paper addresses this limitation by implementing large-scale data of BRB faults
by using a transfer-learning-based lightweight DL model named ShuffleNetV2 for
diagnosing one, two, three, and four BRB faults using current and vibration
signal data. Spectral images for training and testing are generated using a
Short-Time Fourier Transform (STFT). The dataset comprises 57,500 images, with
47,500 used for training and 10,000 for testing. Remarkably, the ShuffleNetV2
model exhibited superior performance, in less computational cost as well as
accurately classifying 98.856% of spectral images. To further enhance the
visualization of harmonic sidebands resulting from broken bars, Fast Fourier
Transform (FFT) is applied to current and vibration data. The paper also
provides insights into the training and testing times for each model,
contributing to a comprehensive understanding of the proposed fault diagnosis
methodology. The findings of our research provide valuable insights into the
performance and efficiency of different ML and DL models, offering a foundation
for the development of robust fault diagnosis systems for induction motors in
industrial settings.","['cs.LG', 'cs.SY', 'eess.SP', 'eess.SY']",http://arxiv.org/abs/2501.03746v1
Large language models for automated scholarly paper review: A survey,"Large language models (LLMs) have significantly impacted human society,
influencing various domains. Among them, academia is not simply a domain
affected by LLMs, but it is also the pivotal force in the development of LLMs.
In academic publications, this phenomenon is represented during the
incorporation of LLMs into the peer review mechanism for reviewing manuscripts.
We proposed the concept of automated scholarly paper review (ASPR) in our
previous paper. As the incorporation grows, it now enters the coexistence phase
of ASPR and peer review, which is described in that paper. LLMs hold
transformative potential for the full-scale implementation of ASPR, but they
also pose new issues and challenges that need to be addressed. In this survey
paper, we aim to provide a holistic view of ASPR in the era of LLMs. We begin
with a survey to find out which LLMs are used to conduct ASPR. Then, we review
what ASPR-related technological bottlenecks have been solved with the
incorporation of LLM technology. After that, we move on to explore new methods,
new datasets, new source code, and new online systems that come with LLMs for
ASPR. Furthermore, we summarize the performance and issues of LLMs in ASPR, and
investigate the attitudes and reactions of publishers and academia to ASPR.
Lastly, we discuss the challenges associated with the development of LLMs for
ASPR. We hope this survey can serve as an inspirational reference for the
researchers and promote the progress of ASPR for its actual implementation.","['cs.AI', 'cs.CL', 'cs.DL']",http://arxiv.org/abs/2501.10326v1
DiffStereo: High-Frequency Aware Diffusion Model for Stereo Image Restoration,"Diffusion models (DMs) have achieved promising performance in image
restoration but haven't been explored for stereo images. The application of DM
in stereo image restoration is confronted with a series of challenges. The need
to reconstruct two images exacerbates DM's computational cost. Additionally,
existing latent DMs usually focus on semantic information and remove
high-frequency details as redundancy during latent compression, which is
precisely what matters for image restoration. To address the above problems, we
propose a high-frequency aware diffusion model, DiffStereo for stereo image
restoration as the first attempt at DM in this domain. Specifically, DiffStereo
first learns latent high-frequency representations (LHFR) of HQ images. DM is
then trained in the learned space to estimate LHFR for stereo images, which are
fused into a transformer-based stereo image restoration network providing
beneficial high-frequency information of corresponding HQ images. The
resolution of LHFR is kept the same as input images, which preserves the
inherent texture from distortion. And the compression in channels alleviates
the computational burden of DM. Furthermore, we devise a position encoding
scheme when integrating the LHFR into the restoration network, enabling
distinctive guidance in different depths of the restoration network.
Comprehensive experiments verify that by combining generative DM and
transformer, DiffStereo achieves both higher reconstruction accuracy and better
perceptual quality on stereo super-resolution, deblurring, and low-light
enhancement compared with state-of-the-art methods.",['cs.CV'],http://arxiv.org/abs/2501.10325v1
"Towards Human-Guided, Data-Centric LLM Co-Pilots","Machine learning (ML) has the potential to revolutionize healthcare, but its
adoption is often hindered by the disconnect between the needs of domain
experts and translating these needs into robust and valid ML tools. Despite
recent advances in LLM-based co-pilots to democratize ML for non-technical
domain experts, these systems remain predominantly focused on model-centric
aspects while overlooking critical data-centric challenges. This limitation is
problematic in complex real-world settings where raw data often contains
complex issues, such as missing values, label noise, and domain-specific
nuances requiring tailored handling. To address this we introduce CliMB-DC, a
human-guided, data-centric framework for LLM co-pilots that combines advanced
data-centric tools with LLM-driven reasoning to enable robust, context-aware
data processing. At its core, CliMB-DC introduces a novel, multi-agent
reasoning system that combines a strategic coordinator for dynamic planning and
adaptation with a specialized worker agent for precise execution. Domain
expertise is then systematically incorporated to guide the reasoning process
using a human-in-the-loop approach. To guide development, we formalize a
taxonomy of key data-centric challenges that co-pilots must address.
Thereafter, to address the dimensions of the taxonomy, we integrate
state-of-the-art data-centric tools into an extensible, open-source
architecture, facilitating the addition of new tools from the research
community. Empirically, using real-world healthcare datasets we demonstrate
CliMB-DC's ability to transform uncurated datasets into ML-ready formats,
significantly outperforming existing co-pilot baselines for handling
data-centric challenges. CliMB-DC promises to empower domain experts from
diverse domains -- healthcare, finance, social sciences and more -- to actively
participate in driving real-world impact using ML.","['cs.LG', 'stat.ML']",http://arxiv.org/abs/2501.10321v1
Over-the-Air Multi-Sensor Inference with Neural Networks Using Memristor-Based Analog Computing,"Deep neural networks provide reliable solutions for many classification and
regression tasks; however, their application in real-time wireless systems with
simple sensor networks is limited due to high energy consumption and
significant bandwidth needs. This study proposes a multi-sensor wireless
inference system with memristor-based analog computing. Given the sensors'
limited computational capabilities, the features from the network's front end
are transmitted to a central device where an $L_p$-norm inspired approximation
of the maximum operation is employed to achieve transformation-invariant
features, enabling efficient over-the-air transmission. We also introduce a
trainable over-the-air sensor fusion method based on $L_p$-norm inspired
combining function that customizes sensor fusion to match the network and
sensor distribution characteristics, enhancing adaptability. To address the
energy constraints of sensors, we utilize memristors, known for their
energy-efficient in-memory computing, enabling analog-domain computations that
reduce energy use and computational overhead in edge computing. This dual
approach of memristors and $L_p$-norm inspired sensor fusion fosters
energy-efficient computational and transmission paradigms and serves as a
practical energy-efficient solution with minimal performance loss.","['cs.LG', 'cs.DC', 'cs.IT', 'math.IT']",http://arxiv.org/abs/2501.10245v1
The Relevance of AWS Chronos: An Evaluation of Standard Methods for Time Series Forecasting with Limited Tuning,"A systematic comparison of Chronos, a transformer-based time series
forecasting framework, against traditional approaches including ARIMA and
Prophet. We evaluate these models across multiple time horizons and user
categories, with a focus on the impact of historical context length. Our
analysis reveals that while Chronos demonstrates superior performance for
longer-term predictions and maintains accuracy with increased context,
traditional models show significant degradation as context length increases. We
find that prediction quality varies systematically between user classes,
suggesting that underlying behavior patterns always influence model
performance. This study provides a case for deploying Chronos in real-world
applications where limited model tuning is feasible, especially in scenarios
requiring longer prediction.",['cs.LG'],http://arxiv.org/abs/2501.10216v1
Deep Compression Autoencoder for Efficient High-Resolution Diffusion Models,"We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder
models for accelerating high-resolution diffusion models. Existing autoencoder
models have demonstrated impressive results at a moderate spatial compression
ratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for
high spatial compression ratios (e.g., 64x). We address this challenge by
introducing two key techniques: (1) Residual Autoencoding, where we design our
models to learn residuals based on the space-to-channel transformed features to
alleviate the optimization difficulty of high spatial-compression autoencoders;
(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases
training strategy for mitigating the generalization penalty of high
spatial-compression autoencoders. With these designs, we improve the
autoencoder's spatial compression ratio up to 128 while maintaining the
reconstruction quality. Applying our DC-AE to latent diffusion models, we
achieve significant speedup without accuracy drop. For example, on ImageNet
512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup
on H100 GPU for UViT-H while achieving a better FID, compared with the widely
used SD-VAE-f8 autoencoder. Our code is available at
https://github.com/mit-han-lab/efficientvit.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2410.10733v5
Provably Safeguarding a Classifier from OOD and Adversarial Samples: an Extreme Value Theory Approach,"This paper introduces a novel method, Sample-efficient Probabilistic
Detection using Extreme Value Theory (SPADE), which transforms a classifier
into an abstaining classifier, offering provable protection against
out-of-distribution and adversarial samples. The approach is based on a
Generalized Extreme Value (GEV) model of the training distribution in the
classifier's latent space, enabling the formal characterization of OOD samples.
Interestingly, under mild assumptions, the GEV model also allows for formally
characterizing adversarial samples. The abstaining classifier, which rejects
samples based on their assessment by the GEV model, provably avoids OOD and
adversarial samples. The empirical validation of the approach, conducted on
various neural architectures (ResNet, VGG, and Vision Transformer) and medium
and large-sized datasets (CIFAR-10, CIFAR-100, and ImageNet), demonstrates its
frugality, stability, and efficiency compared to the state of the art.","['stat.ML', 'cs.LG']",http://arxiv.org/abs/2501.10202v1
CSHNet: A Novel Information Asymmetric Image Translation Method,"Despite advancements in cross-domain image translation, challenges persist in
asymmetric tasks such as SAR-to-Optical and Sketch-to-Instance conversions,
which involve transforming data from a less detailed domain into one with
richer content. Traditional CNN-based methods are effective at capturing fine
details but struggle with global structure, leading to unwanted merging of
image regions. To address this, we propose the CNN-Swin Hybrid Network
(CSHNet), which combines two key modules: Swin Embedded CNN (SEC) and CNN
Embedded Swin (CES), forming the SEC-CES-Bottleneck (SCB). SEC leverages CNN's
detailed feature extraction while integrating the Swin Transformer's structural
bias. CES, in turn, preserves the Swin Transformer's global integrity,
compensating for CNN's lack of focus on structure. Additionally, CSHNet
includes two components designed to enhance cross-domain information retention:
the Interactive Guided Connection (IGC), which enables dynamic information
exchange between SEC and CES, and Adaptive Edge Perception Loss (AEPL), which
maintains structural boundaries during translation. Experimental results show
that CSHNet outperforms existing methods in both visual quality and performance
metrics across scene-level and instance-level datasets. Our code is available
at: https://github.com/XduShi/CSHNet.",['cs.CV'],http://arxiv.org/abs/2501.10197v1
Temporal Causal Reasoning with (Non-Recursive) Structural Equation Models,"Structural Equation Models (SEM) are the standard approach to representing
causal dependencies between variables in causal models. In this paper we
propose a new interpretation of SEMs when reasoning about Actual Causality, in
which SEMs are viewed as mechanisms transforming the dynamics of exogenous
variables into the dynamics of endogenous variables. This allows us to combine
counterfactual causal reasoning with existing temporal logic formalisms, and to
introduce a temporal logic, CPLTL, for causal reasoning about such structures.
We show that the standard restriction to so-called \textit{recursive} models
(with no cycles in the dependency graph) is not necessary in our approach,
allowing us to reason about mutually dependent processes and feedback loops.
Finally, we introduce new notions of model equivalence for temporal causal
models, and show that CPLTL has an efficient model-checking procedure.","['cs.AI', 'cs.LO']",http://arxiv.org/abs/2501.10190v1
CSSDM Ontology to Enable Continuity of Care Data Interoperability,"The rapid advancement of digital technologies and recent global pandemic
scenarios have led to a growing focus on how these technologies can enhance
healthcare service delivery and workflow to address crises. Action plans that
consolidate existing digital transformation programs are being reviewed to
establish core infrastructure and foundations for sustainable healthcare
solutions. Reforming health and social care to personalize home care, for
example, can help avoid treatment in overcrowded acute hospital settings and
improve the experiences and outcomes for both healthcare professionals and
service users. In this information-intensive domain, addressing the
interoperability challenge through standards-based roadmaps is crucial for
enabling effective connections between health and social care services. This
approach facilitates safe and trustworthy data workflows between different
healthcare system providers. In this paper, we present a methodology for
extracting, transforming, and loading data through a semi-automated process
using a Common Semantic Standardized Data Model (CSSDM) to create personalized
healthcare knowledge graph (KG). The CSSDM is grounded in the formal ontology
of ISO 13940 ContSys and incorporates FHIR-based specifications to support
structural attributes for generating KGs. We propose that the CSSDM facilitates
data harmonization and linking, offering an alternative approach to
interoperability. This approach promotes a novel form of collaboration between
companies developing health information systems and cloud-enabled health
services. Consequently, it provides multiple stakeholders with access to
high-quality data and information sharing.","['cs.AI', '68T27', 'I.2.4; I.2.1']",http://arxiv.org/abs/2501.10160v1
Exploring the Impact of Generative Artificial Intelligence in Education: A Thematic Analysis,"The recent advancements in Generative Artificial intelligence (GenAI)
technology have been transformative for the field of education. Large Language
Models (LLMs) such as ChatGPT and Bard can be leveraged to automate boilerplate
tasks, create content for personalised teaching, and handle repetitive tasks to
allow more time for creative thinking. However, it is important to develop
guidelines, policies, and assessment methods in the education sector to ensure
the responsible integration of these tools. In this article, thematic analysis
has been performed on seven essays obtained from professionals in the education
sector to understand the advantages and pitfalls of using GenAI models such as
ChatGPT and Bard in education. Exploratory Data Analysis (EDA) has been
performed on the essays to extract further insights from the text. The study
found several themes which highlight benefits and drawbacks of GenAI tools, as
well as suggestions to overcome these limitations and ensure that students are
using these tools in a responsible and ethical manner.","['cs.AI', 'cs.HC', 'cs.LG']",http://arxiv.org/abs/2501.10134v1
Expression Prompt Collaboration Transformer for Universal Referring Video Object Segmentation,"Audio-guided Video Object Segmentation (A-VOS) and Referring Video Object
Segmentation (R-VOS) are two highly related tasks that both aim to segment
specific objects from video sequences according to expression prompts. However,
due to the challenges of modeling representations for different modalities,
existing methods struggle to strike a balance between interaction flexibility
and localization precision. In this paper, we address this problem from two
perspectives: the alignment of audio and text and the deep interaction among
audio, text, and visual modalities. First, we propose a universal architecture,
the Expression Prompt Collaboration Transformer, herein EPCFormer. Next, we
propose an Expression Alignment (EA) mechanism for audio and text. The proposed
EPCFormer exploits the fact that audio and text prompts referring to the same
objects are semantically equivalent by using contrastive learning for both
types of expressions. Then, to facilitate deep interactions among audio, text,
and visual modalities, we introduce an Expression-Visual Attention (EVA)
module. The knowledge of video object segmentation in terms of the expression
prompts can seamlessly transfer between the two tasks by deeply exploring
complementary cues between text and audio. Experiments on well-recognized
benchmarks demonstrate that our EPCFormer attains state-of-the-art results on
both tasks. The source code will be made publicly available at
https://github.com/lab206/EPCFormer.","['cs.CV', 'eess.AS', 'eess.IV']",http://arxiv.org/abs/2308.04162v2
Continuous Urban Change Detection from Satellite Image Time Series with Temporal Feature Refinement and Multi-Task Integration,"Urbanization advances at unprecedented rates, resulting in negative effects
on the environment and human well-being. Remote sensing has the potential to
mitigate these effects by supporting sustainable development strategies with
accurate information on urban growth. Deep learning-based methods have achieved
promising urban change detection results from optical satellite image pairs
using convolutional neural networks (ConvNets), transformers, and a multi-task
learning setup. However, transformers have not been leveraged for urban change
detection with multi-temporal data, i.e., >2 images, and multi-task learning
methods lack integration approaches that combine change and segmentation
outputs. To fill this research gap, we propose a continuous urban change
detection method that identifies changes in each consecutive image pair of a
satellite image time series (SITS). Specifically, we propose a temporal feature
refinement (TFR) module that utilizes self-attention to improve ConvNet-based
multi-temporal building representations. Furthermore, we propose a multi-task
integration (MTI) module that utilizes Markov networks to find an optimal
building map time series based on segmentation and dense change outputs. The
proposed method effectively identifies urban changes based on high-resolution
SITS acquired by the PlanetScope constellation (F1 score 0.551) and Gaofen-2
(F1 score 0.440). Moreover, our experiments on two challenging datasets
demonstrate the effectiveness of the proposed method compared to bi-temporal
and multi-temporal urban change detection and segmentation methods.",['cs.CV'],http://arxiv.org/abs/2406.17458v2
Mamba2D: A Natively Multi-Dimensional State-Space Model for Vision Tasks,"State-Space Models (SSMs) have recently emerged as a powerful and efficient
alternative to the long-standing transformer architecture. However, existing
SSM conceptualizations retain deeply rooted biases from their roots in natural
language processing. This constrains their ability to appropriately model the
spatially-dependent characteristics of visual inputs. In this paper, we address
these limitations by re-deriving modern selective state-space techniques,
starting from a natively multidimensional formulation. Currently, prior works
attempt to apply natively 1D SSMs to 2D data (i.e. images) by relying on
arbitrary combinations of 1D scan directions to capture spatial dependencies.
In contrast, Mamba2D improves upon this with a single 2D scan direction that
factors in both dimensions of the input natively, effectively modelling spatial
dependencies when constructing hidden states. Mamba2D shows comparable
performance to prior adaptations of SSMs for vision tasks, on standard image
classification evaluations with the ImageNet-1K dataset. Source code is
available at https://github.com/cocoalex00/Mamba2D.",['cs.CV'],http://arxiv.org/abs/2412.16146v2
IncSAR: A Dual Fusion Incremental Learning Framework for SAR Target Recognition,"Deep learning techniques have achieved significant success in Synthetic
Aperture Radar (SAR) target recognition using predefined datasets in static
scenarios. However, real-world applications demand that models incrementally
learn new information without forgetting previously acquired knowledge. The
challenge of catastrophic forgetting, where models lose past knowledge when
adapting to new tasks, remains a critical issue. In this paper, we introduce
IncSAR, an incremental learning framework designed to tackle catastrophic
forgetting in SAR target recognition. IncSAR combines the power of a Vision
Transformer (ViT) and a custom-designed Convolutional Neural Network (CNN) in a
dual-branch architecture, integrated via a late-fusion strategy. Additionally,
we explore the use of TinyViT to reduce computational complexity and propose an
attention mechanism to dynamically enhance feature representation. To mitigate
the speckle noise inherent in SAR images, we employ a denoising module based on
a neural network approximation of Robust Principal Component Analysis (RPCA),
leveraging a simple neural network for efficient noise reduction in SAR
imagery. Moreover, a random projection layer improves the linear separability
of features, and a variant of Linear Discriminant Analysis (LDA) decorrelates
extracted class prototypes for better generalization. Extensive experiments on
the MSTAR, SAR-AIRcraft-1.0, and OpenSARShip benchmark datasets demonstrate
that IncSAR significantly outperforms state-of-the-art approaches, achieving a
99.63\% average accuracy and a 0.33\% performance drop, representing an 89\%
improvement in retention compared to existing techniques. The source code is
available at https://github.com/geokarant/IncSAR.",['cs.CV'],http://arxiv.org/abs/2410.05820v2
Mitigating Sycophancy in Decoder-Only Transformer Architectures: Synthetic Data Intervention,"To address the sycophancy problem caused by reinforcement learning from human
feedback in large language models, this research applies synthetic data
intervention technology to the decoder-only transformer architecture. Based on
the research gaps in the existing literature, the researcher designed an
experimental process to reduce the tendency of models to cater by generating
diversified data, and used GPT4o as an experimental tool for verification. The
experiment used 100 true and false questions, and compared the performance of
the model trained with synthetic data intervention and the original untrained
model on multiple indicators. The results show that the SDI training model
supports the technology in terms of accuracy rate and sycophancy rate and has
significant effectiveness in reducing sycophancy phenomena. Notably, the data
set, experimental process, code and data results have been uploaded to Github,
the link is https://github.com/brucewang123456789/GeniusTrail.git.",['cs.AI'],http://arxiv.org/abs/2411.10156v3
IterL2Norm: Fast Iterative L2-Normalization,"Transformer-based large language models are a memory-bound model whose
operation is based on a large amount of data that are marginally reused. Thus,
the data movement between a host and accelerator likely dictates the total
wall-clock time. Layer normalization is one of the key workloads in the
transformer model, following each of multi-head attention and feed-forward
network blocks. To reduce data movement, layer normalization needs to be
performed on the same chip as the matrix-matrix multiplication engine. To this
end, we introduce an iterative L2-normalization method for 1D input
(IterL2Norm), ensuring fast convergence to the steady-state solution within
five iteration steps and high precision, outperforming the fast inverse square
root algorithm in six out of nine cases for FP32 and five out of nine for
BFloat16 across the embedding lengths used in the OPT models. Implemented in
32/28nm CMOS, the IterL2Norm macro normalizes $d$-dimensional vectors, where
$64 \leq d \leq 1024$, with a latency of 116-227 cycles at 100MHz/1.05V.",['cs.LG'],http://arxiv.org/abs/2412.04778v2
Spatiotemporal Prediction of Secondary Crashes by Rebalancing Dynamic and Static Data with Generative Adversarial Networks,"Data imbalance is a common issue in analyzing and predicting sudden traffic
events. Secondary crashes constitute only a small proportion of all crashes.
These secondary crashes, triggered by primary crashes, significantly exacerbate
traffic congestion and increase the severity of incidents. However, the severe
imbalance of secondary crash data poses significant challenges for prediction
models, affecting their generalization ability and prediction accuracy.
Existing methods fail to fully address the complexity of traffic crash data,
particularly the coexistence of dynamic and static features, and often struggle
to effectively handle data samples of varying lengths. Furthermore, most
current studies predict the occurrence probability and spatiotemporal
distribution of secondary crashes separately, lacking an integrated solution.
To address these challenges, this study proposes a hybrid model named
VarFusiGAN-Transformer, aimed at improving the fidelity of secondary crash data
generation and jointly predicting the occurrence and spatiotemporal
distribution of secondary crashes. The VarFusiGAN-Transformer model employs
Long Short-Term Memory (LSTM) networks to enhance the generation of
multivariate long-time series data, incorporating a static data generator and
an auxiliary discriminator to model the joint distribution of dynamic and
static features. In addition, the model's prediction module achieves
simultaneous prediction of both the occurrence and spatiotemporal distribution
of secondary crashes. Compared to existing methods, the proposed model
demonstrates superior performance in generating high-fidelity data and
improving prediction accuracy.",['cs.AI'],http://arxiv.org/abs/2501.10041v1
Credit Risk Identification in Supply Chains Using Generative Adversarial Networks,"Credit risk management within supply chains has emerged as a critical
research area due to its significant implications for operational stability and
financial sustainability. The intricate interdependencies among supply chain
participants mean that credit risks can propagate across networks, with impacts
varying by industry. This study explores the application of Generative
Adversarial Networks (GANs) to enhance credit risk identification in supply
chains. GANs enable the generation of synthetic credit risk scenarios,
addressing challenges related to data scarcity and imbalanced datasets. By
leveraging GAN-generated data, the model improves predictive accuracy while
effectively capturing dynamic and temporal dependencies in supply chain data.
The research focuses on three representative industries-manufacturing (steel),
distribution (pharmaceuticals), and services (e-commerce) to assess
industry-specific credit risk contagion. Experimental results demonstrate that
the GAN-based model outperforms traditional methods, including logistic
regression, decision trees, and neural networks, achieving superior accuracy,
recall, and F1 scores. The findings underscore the potential of GANs in
proactive risk management, offering robust tools for mitigating financial
disruptions in supply chains. Future research could expand the model by
incorporating external market factors and supplier relationships to further
enhance predictive capabilities. Keywords- Generative Adversarial Networks
(GANs); Supply Chain Risk; Credit Risk Identification; Machine Learning; Data
Augmentation",['cs.LG'],http://arxiv.org/abs/2501.10348v1
VECT-GAN: A variationally encoded generative model for overcoming data scarcity in pharmaceutical science,"Data scarcity in pharmaceutical research has led to reliance on
labour-intensive trial-and-error approaches for development rather than
data-driven methods. While Machine Learning offers a solution, existing
datasets are often small and noisy, limiting their utility. To address this, we
developed a Variationally Encoded Conditional Tabular Generative Adversarial
Network (VECT-GAN), a novel generative model specifically designed for
augmenting small, noisy datasets. We introduce a pipeline where data is
augmented before regression model development and demonstrate that this
consistently and significantly improves performance over other state-of-the-art
tabular generative models. We apply this pipeline across six pharmaceutical
datasets, and highlight its real-world applicability by developing novel
polymers with medically desirable mucoadhesive properties, which we made and
experimentally characterised. Additionally, we pre-train the model on the
ChEMBL database of drug-like molecules, leveraging knowledge distillation to
enhance its generalisability, making it readily available for use on
pharmaceutical datasets containing small molecules, an extremely common
pharmaceutical task. We demonstrate the power of synthetic data for
regularising small tabular datasets, highlighting its potential to become
standard practice in pharmaceutical model development, and make our method,
including VECT-GAN pre-trained on ChEMBL available as a pip package.",['cs.LG'],http://arxiv.org/abs/2501.08995v2
Intelligent Icing Detection Model of Wind Turbine Blades Based on SCADA data,"Diagnosis of ice accretion on wind turbine blades is all the time a hard nut
to crack in condition monitoring of wind farms. Existing methods focus on
mechanism analysis of icing process, deviation degree analysis of feature
engineering. However, there have not been deep researches of neural networks
applied in this field at present. Supervisory control and data acquisition
(SCADA) makes it possible to train networks through continuously providing not
only operation parameters and performance parameters of wind turbines but also
environmental parameters and operation modes. This paper explores the
possibility that using convolutional neural networks (CNNs), generative
adversarial networks (GANs) and domain adaption learning to establish
intelligent diagnosis frameworks under different training scenarios.
Specifically, PGANC and PGANT are proposed for sufficient and non-sufficient
target wind turbine labeled data, respectively. The basic idea is that we
consider a two-stage training with parallel GANs, which are aimed at capturing
intrinsic features for normal and icing samples, followed by classification CNN
or domain adaption module in various training cases. Model validation on three
wind turbine SCADA data shows that two-stage training can effectively improve
the model performance. Besides, if there is no sufficient labeled data for a
target turbine, which is an extremely common phenomenon in real industrial
practices, the addition of domain adaption learning makes the trained model
show better performance. Overall, our proposed intelligent diagnosis frameworks
can achieve more accurate detection on the same wind turbine and more
generalized capability on a new wind turbine, compared with other machine
learning models and conventional CNNs.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2101.07914v2
Diffusion Models in Vision: A Survey,"Denoising diffusion models represent a recent emerging topic in computer
vision, demonstrating remarkable results in the area of generative modeling. A
diffusion model is a deep generative model that is based on two stages, a
forward diffusion stage and a reverse diffusion stage. In the forward diffusion
stage, the input data is gradually perturbed over several steps by adding
Gaussian noise. In the reverse stage, a model is tasked at recovering the
original input data by learning to gradually reverse the diffusion process,
step by step. Diffusion models are widely appreciated for the quality and
diversity of the generated samples, despite their known computational burdens,
i.e. low speeds due to the high number of steps involved during sampling. In
this survey, we provide a comprehensive review of articles on denoising
diffusion models applied in vision, comprising both theoretical and practical
contributions in the field. First, we identify and present three generic
diffusion modeling frameworks, which are based on denoising diffusion
probabilistic models, noise conditioned score networks, and stochastic
differential equations. We further discuss the relations between diffusion
models and other deep generative models, including variational auto-encoders,
generative adversarial networks, energy-based models, autoregressive models and
normalizing flows. Then, we introduce a multi-perspective categorization of
diffusion models applied in computer vision. Finally, we illustrate the current
limitations of diffusion models and envision some interesting directions for
future research.","['cs.CV', 'cs.AI', 'cs.LG']",http://arxiv.org/abs/2209.04747v6
Synthesizing Forestry Images Conditioned on Plant Phenotype Using a Generative Adversarial Network,"Plant phenology and phenotype prediction using remote sensing data are
increasingly gaining attention within the plant science community as a
promising approach to enhance agricultural productivity. This work focuses on
generating synthetic forestry images that satisfy certain phenotypic
attributes, viz. canopy greenness. We harness a Generative Adversarial Network
(GAN) to synthesize biologically plausible and phenotypically stable forestry
images conditioned on the greenness of vegetation (a continuous attribute) over
a specific region of interest, describing a particular vegetation type in a
mixed forest. The training data is based on the automated digital camera
imagery provided by the National Ecological Observatory Network (NEON) and
processed by the PhenoCam Network. Our method helps render the appearance of
forest sites specific to a greenness value. The synthetic images are
subsequently utilized to predict another phenotypic attribute, viz., redness of
plants. The quality of the synthetic images is assessed using the Structural
SIMilarity (SSIM) index and Fr\'echet Inception Distance (FID). Further, the
greenness and redness indices of the synthetic images are compared against
those of the original images using Root Mean Squared Percentage Error (RMSPE)
to evaluate their accuracy and integrity. The generalizability and scalability
of our proposed GAN model are established by effectively transforming it to
generate synthetic images for other forest sites and vegetation types. From a
broader perspective, this approach could be leveraged to visualize forestry
based on different phenotypic attributes in the context of various
environmental parameters.",['cs.CV'],http://arxiv.org/abs/2307.03789v3
Generating Realistic Synthetic Head Rotation Data for Extended Reality using Deep Learning,"Extended Reality is a revolutionary method of delivering multimedia content
to users. A large contributor to its popularity is the sense of immersion and
interactivity enabled by having real-world motion reflected in the virtual
experience accurately and immediately. This user motion, mainly caused by head
rotations, induces several technical challenges. For instance, which content is
generated and transmitted depends heavily on where the user is looking.
Seamless systems, taking user motion into account proactively, will therefore
require accurate predictions of upcoming rotations. Training and evaluating
such predictors requires vast amounts of orientational input data, which is
expensive to gather, as it requires human test subjects. A more feasible
approach is to gather a modest dataset through test subjects, and then extend
it to a more sizeable set using synthetic data generation methods. In this
work, we present a head rotation time series generator based on TimeGAN, an
extension of the well-known Generative Adversarial Network, designed
specifically for generating time series. This approach is able to extend a
dataset of head rotations with new samples closely matching the distribution of
the measured time series.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.09050v1
Investigating Parameter-Efficiency of Hybrid QuGANs Based on Geometric Properties of Generated Sea Route Graphs,"The demand for artificially generated data for the development, training and
testing of new algorithms is omnipresent. Quantum computing (QC), does offer
the hope that its inherent probabilistic functionality can be utilised in this
field of generative artificial intelligence. In this study, we use
quantum-classical hybrid generative adversarial networks (QuGANs) to
artificially generate graphs of shipping routes. We create a training dataset
based on real shipping data and investigate to what extent QuGANs are able to
learn and reproduce inherent distributions and geometric features of this data.
We compare hybrid QuGANs with classical Generative Adversarial Networks (GANs),
with a special focus on their parameter efficiency. Our results indicate that
QuGANs are indeed able to quickly learn and represent underlying geometric
properties and distributions, although they seem to have difficulties in
introducing variance into the sampled data. Compared to classical GANs of
greater size, measured in the number of parameters used, some QuGANs show
similar result quality. Our reference to concrete use cases, such as the
generation of shipping data, provides an illustrative example and demonstrate
the potential and diversity in which QC can be used.","['cs.LG', 'quant-ph']",http://arxiv.org/abs/2501.08678v1
Time series forecasting for multidimensional telemetry data using GAN and BiLSTM in a Digital Twin,"The research related to digital twins has been increasing in recent years.
Besides the mirroring of the physical word into the digital, there is the need
of providing services related to the data collected and transferred to the
virtual world. One of these services is the forecasting of physical part future
behavior, that could lead to applications, like preventing harmful events or
designing improvements to get better performance. One strategy used to predict
any system operation it is the use of time series models like ARIMA or LSTM,
and improvements were implemented using these algorithms. Recently, deep
learning techniques based on generative models such as Generative Adversarial
Networks (GANs) have been proposed to create time series and the use of LSTM
has gained more relevance in time series forecasting, but both have limitations
that restrict the forecasting results. Another issue found in the literature is
the challenge of handling multivariate environments/applications in time series
generation. Therefore, new methods need to be studied in order to fill these
gaps and, consequently, provide better resources for creating useful digital
twins. In this proposal, it is going to be studied the integration of a BiLSTM
layer with a time series obtained by GAN in order to improve the forecasting of
all the features provided by the dataset in terms of accuracy and,
consequently, improving behaviour prediction.","['cs.LG', 'eess.SP']",http://arxiv.org/abs/2501.08464v1
Prediction Interval Construction Method for Electricity Prices,"Accurate prediction of electricity prices plays an essential role in the
electricity market. To reflect the uncertainty of electricity prices, price
intervals are predicted. This paper proposes a novel prediction interval
construction method. A conditional generative adversarial network is first
presented to generate electricity price scenarios, with which the prediction
intervals can be constructed. Then, different generated scenarios are stacked
to obtain the probability densities, which can be applied to accurately reflect
the uncertainty of electricity prices. Furthermore, a reinforced prediction
mechanism based on the volatility level of weather factors is introduced to
address the spikes or volatile prices. A case study is conducted to verify the
effectiveness of the proposed novel prediction interval construction method.
The method can also provide the probability density of each price scenario
within the prediction interval and has the superiority to address the volatile
prices and price spikes with a reinforced prediction mechanism.","['cs.LG', 'cs.SY', 'eess.SY']",http://arxiv.org/abs/2501.07827v1
On the Statistical Capacity of Deep Generative Models,"Deep generative models are routinely used in generating samples from complex,
high-dimensional distributions. Despite their apparent successes, their
statistical properties are not well understood. A common assumption is that
with enough training data and sufficiently large neural networks, deep
generative model samples will have arbitrarily small errors in sampling from
any continuous target distribution. We set up a unifying framework that debunks
this belief. We demonstrate that broad classes of deep generative models,
including variational autoencoders and generative adversarial networks, are not
universal generators. Under the predominant case of Gaussian latent variables,
these models can only generate concentrated samples that exhibit light tails.
Using tools from concentration of measure and convex geometry, we give
analogous results for more general log-concave and strongly log-concave latent
variable distributions. We extend our results to diffusion models via a
reduction argument. We use the Gromov--Levy inequality to give similar
guarantees when the latent variables lie on manifolds with positive Ricci
curvature. These results shed light on the limited capacity of common deep
generative models to handle heavy tails. We illustrate the empirical relevance
of our work with simulations and financial data.","['stat.ML', 'cs.AI', 'cs.LG', 'math.ST', 'stat.TH']",http://arxiv.org/abs/2501.07763v1
Pedestrian Trajectory Prediction Based on Social Interactions Learning With Random Weights,"Pedestrian trajectory prediction is a critical technology in the evolution of
self-driving cars toward complete artificial intelligence. Over recent years,
focusing on the trajectories of pedestrians to model their social interactions
has surged with great interest in more accurate trajectory predictions.
However, existing methods for modeling pedestrian social interactions rely on
pre-defined rules, struggling to capture non-explicit social interactions. In
this work, we propose a novel framework named DTGAN, which extends the
application of Generative Adversarial Networks (GANs) to graph sequence data,
with the primary objective of automatically capturing implicit social
interactions and achieving precise predictions of pedestrian trajectory. DTGAN
innovatively incorporates random weights within each graph to eliminate the
need for pre-defined interaction rules. We further enhance the performance of
DTGAN by exploring diverse task loss functions during adversarial training,
which yields improvements of 16.7\% and 39.3\% on metrics ADE and FDE,
respectively. The effectiveness and accuracy of our framework are verified on
two public datasets. The experimental results show that our proposed DTGAN
achieves superior performance and is well able to understand pedestrians'
intentions.","['cs.CV', 'cs.MM']",http://arxiv.org/abs/2501.07711v1
SFC-GAN: A Generative Adversarial Network for Brain Functional and Structural Connectome Translation,"Modern brain imaging technologies have enabled the detailed reconstruction of
human brain connectomes, capturing structural connectivity (SC) from diffusion
MRI and functional connectivity (FC) from functional MRI. Understanding the
intricate relationships between SC and FC is vital for gaining deeper insights
into the brain's functional and organizational mechanisms. However, obtaining
both SC and FC modalities simultaneously remains challenging, hindering
comprehensive analyses. Existing deep generative models typically focus on
synthesizing a single modality or unidirectional translation between FC and SC,
thereby missing the potential benefits of bi-directional translation,
especially in scenarios where only one connectome is available. Therefore, we
propose Structural-Functional Connectivity GAN (SFC-GAN), a novel framework for
bidirectional translation between SC and FC. This approach leverages the
CycleGAN architecture, incorporating convolutional layers to effectively
capture the spatial structures of brain connectomes. To preserve the
topological integrity of these connectomes, we employ a structure-preserving
loss that guides the model in capturing both global and local connectome
patterns while maintaining symmetry. Our framework demonstrates superior
performance in translating between SC and FC, outperforming baseline models in
similarity and graph property evaluations compared to ground truth data, each
translated modality can be effectively utilized for downstream classification.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2501.07055v1
ODPG: Outfitting Diffusion with Pose Guided Condition,"Virtual Try-On (VTON) technology allows users to visualize how clothes would
look on them without physically trying them on, gaining traction with the rise
of digitalization and online shopping. Traditional VTON methods, often using
Generative Adversarial Networks (GANs) and Diffusion models, face challenges in
achieving high realism and handling dynamic poses. This paper introduces
Outfitting Diffusion with Pose Guided Condition (ODPG), a novel approach that
leverages a latent diffusion model with multiple conditioning inputs during the
denoising process. By transforming garment, pose, and appearance images into
latent features and integrating these features in a UNet-based denoising model,
ODPG achieves non-explicit synthesis of garments on dynamically posed human
images. Our experiments on the FashionTryOn and a subset of the DeepFashion
dataset demonstrate that ODPG generates realistic VTON images with fine-grained
texture details across various poses, utilizing an end-to-end architecture
without the need for explicit garment warping processes. Future work will focus
on generating VTON outputs in video format and on applying our attention
mechanism, as detailed in the Method section, to other domains with limited
data.",['cs.CV'],http://arxiv.org/abs/2501.06769v1
HFMF: Hierarchical Fusion Meets Multi-Stream Models for Deepfake Detection,"The rapid progress in deep generative models has led to the creation of
incredibly realistic synthetic images that are becoming increasingly difficult
to distinguish from real-world data. The widespread use of Variational Models,
Diffusion Models, and Generative Adversarial Networks has made it easier to
generate convincing fake images and videos, which poses significant challenges
for detecting and mitigating the spread of misinformation. As a result,
developing effective methods for detecting AI-generated fakes has become a
pressing concern. In our research, we propose HFMF, a comprehensive two-stage
deepfake detection framework that leverages both hierarchical cross-modal
feature fusion and multi-stream feature extraction to enhance detection
performance against imagery produced by state-of-the-art generative AI models.
The first component of our approach integrates vision Transformers and
convolutional nets through a hierarchical feature fusion mechanism. The second
component of our framework combines object-level information and a fine-tuned
convolutional net model. We then fuse the outputs from both components via an
ensemble deep neural net, enabling robust classification performances. We
demonstrate that our architecture achieves superior performance across diverse
dataset benchmarks while maintaining calibration and interoperability.",['cs.CV'],http://arxiv.org/abs/2501.05631v1
BiasGuard: Guardrailing Fairness in Machine Learning Production Systems,"As machine learning (ML) systems increasingly impact critical sectors such as
hiring, financial risk assessments, and criminal justice, the imperative to
ensure fairness has intensified due to potential negative implications. While
much ML fairness research has focused on enhancing training data and processes,
addressing the outputs of already deployed systems has received less attention.
This paper introduces 'BiasGuard', a novel approach designed to act as a
fairness guardrail in production ML systems. BiasGuard leverages Test-Time
Augmentation (TTA) powered by Conditional Generative Adversarial Network
(CTGAN), a cutting-edge generative AI model, to synthesize data samples
conditioned on inverted protected attribute values, thereby promoting equitable
outcomes across diverse groups. This method aims to provide equal opportunities
for both privileged and unprivileged groups while significantly enhancing the
fairness metrics of deployed systems without the need for retraining. Our
comprehensive experimental analysis across diverse datasets reveals that
BiasGuard enhances fairness by 31% while only reducing accuracy by 0.09%
compared to non-mitigated benchmarks. Additionally, BiasGuard outperforms
existing post-processing methods in improving fairness, positioning it as an
effective tool to safeguard against biases when retraining the model is
impractical.","['cs.LG', 'cs.AI', 'cs.CY']",http://arxiv.org/abs/2501.04142v1
Statistical Error Bounds for GANs with Nonlinear Objective Functionals,"Generative adversarial networks (GANs) are unsupervised learning methods for
training a generator distribution to produce samples that approximate those
drawn from a target distribution. Many such methods can be formulated as
minimization of a metric or divergence between probability distributions.
Recent works have derived statistical error bounds for GANs that are based on
integral probability metrics (IPMs), e.g., WGAN which is based on the
1-Wasserstein metric. In general, IPMs are defined by optimizing a linear
functional (difference of expectations) over a space of discriminators. A much
larger class of GANs, which we here call $(f,\Gamma)$-GANs, can be constructed
using $f$-divergences (e.g., Jensen-Shannon, KL, or $\alpha$-divergences)
together with a regularizing discriminator space $\Gamma$ (e.g., $1$-Lipschitz
functions). These GANs have nonlinear objective functions, depending on the
choice of $f$, and have been shown to exhibit improved performance in a number
of applications. In this work we derive statistical error bounds for
$(f,\Gamma)$-GANs for general classes of $f$ and $\Gamma$ in the form of
finite-sample concentration inequalities. These results prove the statistical
consistency of $(f,\Gamma)$-GANs and reduce to the known results for IPM-GANs
in the appropriate limit. Finally, our results also give new insight into the
performance of GANs for distributions with unbounded support.","['stat.ML', 'cs.LG']",http://arxiv.org/abs/2406.16834v2
Efficient Generative Modeling via Penalized Optimal Transport Network,"The generation of synthetic data with distributions that faithfully emulate
the underlying data-generating mechanism holds paramount significance.
Wasserstein Generative Adversarial Networks (WGANs) have emerged as a prominent
tool for this task; however, due to the delicate equilibrium of the minimax
formulation and the instability of Wasserstein distance in high dimensions,
WGAN often manifests the pathological phenomenon of mode collapse. This results
in generated samples that converge to a restricted set of outputs and fail to
adequately capture the tail behaviors of the true distribution. Such
limitations can lead to serious downstream consequences. To this end, we
propose the Penalized Optimal Transport Network (POTNet), a versatile deep
generative model based on the marginally-penalized Wasserstein (MPW) distance.
Through the MPW distance, POTNet effectively leverages low-dimensional marginal
information to guide the overall alignment of joint distributions. Furthermore,
our primal-based framework enables direct evaluation of the MPW distance, thus
eliminating the need for a critic network. This formulation circumvents
training instabilities inherent in adversarial approaches and avoids the need
for extensive parameter tuning. We derive a non-asymptotic bound on the
generalization error of the MPW loss and establish convergence rates of the
generative distribution learned by POTNet. Our theoretical analysis together
with extensive empirical evaluations demonstrate the superior performance of
POTNet in accurately capturing underlying data structures, including their tail
behaviors and minor modalities. Moreover, our model achieves orders of
magnitude speedup during the sampling stage compared to state-of-the-art
alternatives, which enables computationally efficient large-scale synthetic
data generation.","['stat.ML', 'cs.LG', 'stat.AP', 'stat.ME']",http://arxiv.org/abs/2402.10456v2
Physics-Constrained Generative Artificial Intelligence for Rapid Takeoff Trajectory Design,"To aid urban air mobility (UAM), electric vertical takeoff and landing
(eVTOL) aircraft are being targeted. Conventional multidisciplinary analysis
and optimization (MDAO) can be expensive, while surrogate-based optimization
can struggle with challenging physical constraints. This work proposes
physics-constrained generative adversarial networks (physicsGAN), to
intelligently parameterize the takeoff control profiles of an eVTOL aircraft
and to transform the original design space to a feasible space. Specifically,
the transformed feasible space refers to a space where all designs directly
satisfy all design constraints. The physicsGAN-enabled surrogate-based takeoff
trajectory design framework was demonstrated on the Airbus A3 Vahana. The
physicsGAN generated only feasible control profiles of power and wing angle in
the feasible space with around 98.9% of designs satisfying all constraints. The
proposed design framework obtained 99.6% accuracy compared with
simulation-based optimal design and took only 2.2 seconds, which reduced the
computational time by around 200 times. Meanwhile, data-driven GAN-enabled
surrogate-based optimization took 21.9 seconds using a derivative-free
optimizer, which was around an order of magnitude slower than the proposed
framework. Moreover, the data-driven GAN-based optimization using
gradient-based optimizers could not consistently find the optimal design during
random trials and got stuck in an infeasible region, which is problematic in
real practice. Therefore, the proposed physicsGAN-based design framework
outperformed data-driven GAN-based design to the extent of efficiency (2.2
seconds), optimality (99.6% accurate), and feasibility (100% feasible).
According to the literature review, this is the first physics-constrained
generative artificial intelligence enabled by surrogate models.",['cs.LG'],http://arxiv.org/abs/2501.03445v1
SRAGAN: Saliency Regularized and Attended Generative Adversarial Network for Chinese Ink-wash Painting Generation,"Recent style transfer problems are still largely dominated by Generative
Adversarial Network (GAN) from the perspective of cross-domain image-to-image
(I2I) translation, where the pivotal issue is to learn and transfer
target-domain style patterns onto source-domain content images. This paper
handles the problem of translating real pictures into traditional Chinese
ink-wash paintings, i.e., Chinese ink-wash painting style transfer. Though a
wide range of I2I models tackle this problem, a notable challenge is that the
content details of the source image could be easily erased or corrupted due to
the transfer of ink-wash style elements. To remedy this issue, we propose to
incorporate saliency detection into the unpaired I2I framework to regularize
image content, where the detected saliency map is utilized from two aspects:
(\romannumeral1) we propose saliency IOU (SIOU) loss to explicitly regularize
object content structure by enforcing saliency consistency before and after
image stylization; (\romannumeral2) we propose saliency adaptive normalization
(SANorm) which implicitly enhances object structure integrity of the generated
paintings by dynamically injecting image saliency information into the
generator to guide stylization process. Besides, we also propose saliency
attended discriminator which harnesses image saliency information to focus
generative adversarial attention onto the drawn objects, contributing to
generating more vivid and delicate brush strokes and ink-wash textures.
Extensive qualitative and quantitative experiments demonstrate superiority of
our approach over related advanced image stylization methods in both GAN and
diffusion model paradigms.",['cs.CV'],http://arxiv.org/abs/2404.15743v2
AutoGAN-Distiller: Searching to Compress Generative Adversarial Networks,"The compression of Generative Adversarial Networks (GANs) has lately drawn
attention, due to the increasing demand for deploying GANs into mobile devices
for numerous applications such as image translation, enhancement and editing.
However, compared to the substantial efforts to compressing other deep models,
the research on compressing GANs (usually the generators) remains at its
infancy stage. Existing GAN compression algorithms are limited to handling
specific GAN architectures and losses. Inspired by the recent success of AutoML
in deep compression, we introduce AutoML to GAN compression and develop an
AutoGAN-Distiller (AGD) framework. Starting with a specifically designed
efficient search space, AGD performs an end-to-end discovery for new efficient
generators, given the target computational resource constraints. The search is
guided by the original GAN model via knowledge distillation, therefore
fulfilling the compression. AGD is fully automatic, standalone (i.e., needing
no trained discriminators), and generically applicable to various GAN models.
We evaluate AGD in two representative GAN tasks: image translation and super
resolution. Without bells and whistles, AGD yields remarkably lightweight yet
more competitive compressed models, that largely outperform existing
alternatives. Our codes and pretrained models are available at
https://github.com/TAMU-VITA/AGD.","['cs.CV', 'cs.LG', 'eess.IV']",http://arxiv.org/abs/2006.08198v3
"Generating Multimodal Images with GAN: Integrating Text, Image, and Style","In the field of computer vision, multimodal image generation has become a
research hotspot, especially the task of integrating text, image, and style. In
this study, we propose a multimodal image generation method based on Generative
Adversarial Networks (GAN), capable of effectively combining text descriptions,
reference images, and style information to generate images that meet multimodal
requirements. This method involves the design of a text encoder, an image
feature extractor, and a style integration module, ensuring that the generated
images maintain high quality in terms of visual content and style consistency.
We also introduce multiple loss functions, including adversarial loss,
text-image consistency loss, and style matching loss, to optimize the
generation process. Experimental results show that our method produces images
with high clarity and consistency across multiple public datasets,
demonstrating significant performance improvements compared to existing
methods. The outcomes of this study provide new insights into multimodal image
generation and present broad application prospects.",['cs.CV'],http://arxiv.org/abs/2501.02167v1
MultiPruner: Balanced Structure Removal in Foundation Models,"Recently, state-of-the-art approaches for pruning large pre-trained models
(LPMs) have demonstrated that the training-free removal of non-critical
residual blocks in Transformers is viable for reducing model size, achieving
results that outperform previous training-free pruning approaches. Motivated by
these findings, we extend BlockPruner (Zhong et al., 2024) and propose
MultiPruner, a pruning approach that surpasses recent training-free pruning
methods by adopting a multidimensional, iterative, fine-grained pruning
strategy. In MultiPruner, multidimensional pruning reinstates the structural
balance in block-pruned models by sequentially compressing along three
dimensions: i) residual blocks, ii) channels of multilayer perceptrons (MLP),
and iii) attention heads. This solution enhances zero-shot accuracy on
downstream tasks compared to other techniques while improving model compression
ratios, producing compressed models with fewer computing and memory
requirements. Extensive experiments demonstrate the advantages of the proposed
method across various large pre-trained models. The code and pruning
configurations are available at
https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning.","['cs.LG', 'cs.AI', 'I.2.0']",http://arxiv.org/abs/2501.09949v1
FASP: Fast and Accurate Structured Pruning of Large Language Models,"The rapid increase in the size of large language models (LLMs) has
significantly escalated their computational and memory demands, posing
challenges for efficient deployment, especially on resource-constrained
devices. Structured pruning has emerged as an effective model compression
method that can reduce these demands while preserving performance. In this
paper, we introduce FASP (Fast and Accurate Structured Pruning), a novel
structured pruning framework for LLMs that emphasizes both speed and accuracy.
FASP employs a distinctive pruning structure that interlinks sequential layers,
allowing for the removal of columns in one layer while simultaneously
eliminating corresponding rows in the preceding layer without incurring
additional performance loss. The pruning metric, inspired by Wanda, is
computationally efficient and effectively selects components to prune.
Additionally, we propose a restoration mechanism that enhances model fidelity
by adjusting the remaining weights post-pruning. We evaluate FASP on the OPT
and LLaMA model families, demonstrating superior performance in terms of
perplexity and accuracy on downstream tasks compared to state-of-the-art
methods. Our approach achieves significant speed-ups, pruning models such as
OPT-125M in 17 seconds and LLaMA-30B in 15 minutes on a single NVIDIA RTX 4090
GPU, making it a highly practical solution for optimizing LLMs.",['cs.LG'],http://arxiv.org/abs/2501.09412v1
Soft Knowledge Distillation with Multi-Dimensional Cross-Net Attention for Image Restoration Models Compression,"Transformer-based encoder-decoder models have achieved remarkable success in
image-to-image transfer tasks, particularly in image restoration. However,
their high computational complexity-manifested in elevated FLOPs and parameter
counts-limits their application in real-world scenarios. Existing knowledge
distillation methods in image restoration typically employ lightweight student
models that directly mimic the intermediate features and reconstruction results
of the teacher, overlooking the implicit attention relationships between them.
To address this, we propose a Soft Knowledge Distillation (SKD) strategy that
incorporates a Multi-dimensional Cross-net Attention (MCA) mechanism for
compressing image restoration models. This mechanism facilitates interaction
between the student and teacher across both channel and spatial dimensions,
enabling the student to implicitly learn the attention matrices. Additionally,
we employ a Gaussian kernel function to measure the distance between student
and teacher features in kernel space, ensuring stable and efficient feature
learning. To further enhance the quality of reconstructed images, we replace
the commonly used L1 or KL divergence loss with a contrastive learning loss at
the image level. Experiments on three tasks-image deraining, deblurring, and
denoising-demonstrate that our SKD strategy significantly reduces computational
complexity while maintaining strong image restoration capabilities.",['cs.CV'],http://arxiv.org/abs/2501.09321v1
Knowledge Distillation for Image Restoration : Simultaneous Learning from Degraded and Clean Images,"Model compression through knowledge distillation has seen extensive
application in classification and segmentation tasks. However, its potential in
image-to-image translation, particularly in image restoration, remains
underexplored. To address this gap, we propose a Simultaneous Learning
Knowledge Distillation (SLKD) framework tailored for model compression in image
restoration tasks. SLKD employs a dual-teacher, single-student architecture
with two distinct learning strategies: Degradation Removal Learning (DRL) and
Image Reconstruction Learning (IRL), simultaneously. In DRL, the student
encoder learns from Teacher A to focus on removing degradation factors, guided
by a novel BRISQUE extractor. In IRL, the student decoder learns from Teacher B
to reconstruct clean images, with the assistance of a proposed PIQE extractor.
These strategies enable the student to learn from degraded and clean images
simultaneously, ensuring high-quality compression of image restoration models.
Experimental results across five datasets and three tasks demonstrate that SLKD
achieves substantial reductions in FLOPs and parameters, exceeding 80\%, while
maintaining strong image restoration performance.","['cs.CV', 'eess.IV']",http://arxiv.org/abs/2501.09268v1
SWSC: Shared Weight for Similar Channel in LLM,"Large language models (LLMs) have spurred development in multiple industries.
However, the growing number of their parameters brings substantial storage and
computing burdens, making it essential to explore model compression techniques
for parameter reduction and easier deployment. We propose SWSC, an LLM
compression method based on the concept of Shared Weight for Similar Channel.
It uses the K-Means clustering algorithm to cluster model weights
channel-by-channel, generating clusters with highly similar vectors within
each. A representative vector from each cluster is selected to approximately
replace all vectors in the cluster, significantly reducing the number of model
weight parameters. However, approximate restoration will inevitably cause
damage to the performance of the model. To tackle this issue, we perform
singular value decomposition on the weight error values before and after
compression and retain the larger singular values and their corresponding
singular vectors to compensate for the accuracy. The experimental results show
that our method can effectively ensure the performance of the compressed LLM
even under low-precision conditions.","['cs.LG', 'cs.CL']",http://arxiv.org/abs/2501.08631v1
A Survey on Dynamic Neural Networks: from Computer Vision to Multi-modal Sensor Fusion,"Model compression is essential in the deployment of large Computer Vision
models on embedded devices. However, static optimization techniques (e.g.
pruning, quantization, etc.) neglect the fact that different inputs have
different complexities, thus requiring different amount of computations.
Dynamic Neural Networks allow to condition the number of computations to the
specific input. The current literature on the topic is very extensive and
fragmented. We present a comprehensive survey that synthesizes and unifies
existing Dynamic Neural Networks research in the context of Computer Vision.
Additionally, we provide a logical taxonomy based on which component of the
network is adaptive: the output, the computation graph or the input.
Furthermore, we argue that Dynamic Neural Networks are particularly beneficial
in the context of Sensor Fusion for better adaptivity, noise reduction and
information prioritization. We present preliminary works in this direction.","['cs.CV', '68T45', 'I.4.0; I.2.10']",http://arxiv.org/abs/2501.07451v1
Tiny Models are the Computational Saver for Large Models,"This paper introduces TinySaver, an early-exit-like dynamic model compression
approach which employs tiny models to substitute large models adaptively.
Distinct from traditional compression techniques, dynamic methods like
TinySaver can leverage the difficulty differences to allow certain inputs to
complete their inference processes early, thereby conserving computational
resources. Most existing early exit designs are implemented by attaching
additional network branches to the model's backbone. Our study, however,
reveals that completely independent tiny models can replace a substantial
portion of the larger models' job with minimal impact on performance. Employing
them as the first exit can remarkably enhance computational efficiency. By
searching and employing the most appropriate tiny model as the computational
saver for a given large model, the proposed approaches work as a novel and
generic method to model compression. This finding will help the research
community in exploring new compression methods to address the escalating
computational demands posed by rapidly evolving AI models. Our evaluation of
this approach in ImageNet-1k classification demonstrates its potential to
reduce the number of compute operations by up to 90\%, with only negligible
losses in performance, across various modern vision models.",['cs.AI'],http://arxiv.org/abs/2403.17726v4
CURing Large Models: Compression via CUR Decomposition,"Large deep learning models have achieved remarkable success but are
resource-intensive, posing challenges such as memory usage. We introduce
CURing, a novel model compression method based on CUR matrix decomposition,
which approximates weight matrices as the product of selected columns (C) and
rows (R), and a small linking matrix (U). We apply this decomposition to
weights chosen based on the combined influence of their magnitudes and
activations. By identifying and retaining informative rows and columns, CURing
significantly reduces model size with minimal performance loss. For example, it
reduces Llama3.1-8B's parameters to 7.32B (-9%) in just 129 seconds, over 20
times faster than prior compression methods.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2501.04211v2
Neural Architecture Codesign for Fast Physics Applications,"We develop a pipeline to streamline neural architecture codesign for physics
applications to reduce the need for ML expertise when designing models for
novel tasks. Our method employs neural architecture search and network
compression in a two-stage approach to discover hardware efficient models. This
approach consists of a global search stage that explores a wide range of
architectures while considering hardware constraints, followed by a local
search stage that fine-tunes and compresses the most promising candidates. We
exceed performance on various tasks and show further speedup through model
compression techniques such as quantization-aware-training and neural network
pruning. We synthesize the optimal models to high level synthesis code for FPGA
deployment with the hls4ml library. Additionally, our hierarchical search space
provides greater flexibility in optimization, which can easily extend to other
tasks and domains. We demonstrate this with two case studies: Bragg peak
finding in materials science and jet classification in high energy physics,
achieving models with improved accuracy, smaller latencies, or reduced resource
utilization relative to the baseline models.","['cs.LG', 'cond-mat.mtrl-sci', 'hep-ex', 'physics.ins-det']",http://arxiv.org/abs/2501.05515v1
Generalizing Teacher Networks for Effective Knowledge Distillation Across Student Architectures,"Knowledge distillation (KD) is a model compression method that entails
training a compact student model to emulate the performance of a more complex
teacher model. However, the architectural capacity gap between the two models
limits the effectiveness of knowledge transfer. Addressing this issue, previous
works focused on customizing teacher-student pairs to improve compatibility, a
computationally expensive process that needs to be repeated every time either
model changes. Hence, these methods are impractical when a teacher model has to
be compressed into different student models for deployment on multiple hardware
devices with distinct resource constraints. In this work, we propose Generic
Teacher Network (GTN), a one-off KD-aware training to create a generic teacher
capable of effectively transferring knowledge to any student model sampled from
a given finite pool of architectures. To this end, we represent the student
pool as a weight-sharing supernet and condition our generic teacher to align
with the capacities of various student architectures sampled from this
supernet. Experimental evaluation shows that our method both improves overall
KD effectiveness and amortizes the minimal additional training cost of the
generic teacher across students in the pool.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2407.16040v2
UPAQ: A Framework for Real-Time and Energy-Efficient 3D Object Detection in Autonomous Vehicles,"To enhance perception in autonomous vehicles (AVs), recent efforts are
concentrating on 3D object detectors, which deliver more comprehensive
predictions than traditional 2D object detectors, at the cost of increased
memory footprint and computational resource usage. We present a novel framework
called UPAQ, which leverages semi-structured pattern pruning and quantization
to improve the efficiency of LiDAR point-cloud and camera-based 3D object
detectors on resource-constrained embedded AV platforms. Experimental results
on the Jetson Orin Nano embedded platform indicate that UPAQ achieves up to
5.62x and 5.13x model compression rates, up to 1.97x and 1.86x boost in
inference speed, and up to 2.07x and 1.87x reduction in energy consumption
compared to state-of-the-art model compression frameworks, on the Pointpillar
and SMOKE models respectively.","['cs.CV', 'cs.AI', 'cs.LG']",http://arxiv.org/abs/2501.04213v1
Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic,"Model merging offers an effective strategy to combine the strengths of
multiple finetuned models into a unified model that preserves the specialized
capabilities of each. Existing methods merge models in a global manner,
performing arithmetic operations across all model parameters. However, such
global merging often leads to task interference, degrading the performance of
the merged model. In this work, we introduce Localize-and-Stitch, a novel
approach that merges models in a localized way. Our algorithm works in two
steps: i) Localization: identify tiny ($1\%$ of the total parameters) localized
regions in the finetuned models containing essential skills for the downstream
tasks, and ii) Stitching: reintegrate only these essential regions back into
the pretrained model for task synergy. We demonstrate that our approach
effectively locates sparse regions responsible for finetuned performance, and
the localized regions could be treated as compact and interpretable
representations of the finetuned models (tasks). Empirically, we evaluate our
method on various vision and language benchmarks, showing that it outperforms
existing model merging methods under different data availability scenarios.
Beyond strong empirical performance, our algorithm also facilitates model
compression and preserves pretrained knowledge, enabling flexible and continual
skill composition from multiple finetuned models with minimal storage and
computational overhead. Our code is available at
https://github.com/uiuctml/Localize-and-Stitch.","['cs.LG', 'cs.CL', 'cs.CV']",http://arxiv.org/abs/2408.13656v2
Strategic Fusion Optimizes Transformer Compression,"This study investigates transformer model compression by systematically
pruning its layers. We evaluated 14 pruning strategies across nine diverse
datasets, including 12 strategies based on different signals obtained from
layer activations, mutual information, gradients, weights, and attention. To
address the limitations of single-signal strategies, we introduced two fusion
strategies, linear regression and random forest, which combine individual
strategies (i.e., strategic fusion), for more informed pruning decisions.
Additionally, we applied knowledge distillation to mitigate any accuracy loss
during layer pruning. Our results reveal that random forest strategic fusion
outperforms individual strategies in seven out of nine datasets and achieves
near-optimal performance in the other two. The distilled random forest
surpasses the original accuracy in six datasets and mitigates accuracy drops in
the remaining three. Knowledge distillation also improves the accuracy-to-size
ratio by an average factor of 18.84 across all datasets. Supported by
mathematical foundations and biological analogies, our findings suggest that
strategically combining multiple signals can lead to efficient, high-performing
transformer models for resource-constrained applications.","['cs.LG', 'cs.AI', 'cs.CL']",http://arxiv.org/abs/2501.03273v1
Optimizing Small Language Models for In-Vehicle Function-Calling,"We propose a holistic approach for deploying Small Language Models (SLMs) as
function-calling agents within vehicles as edge devices, offering a more
flexible and robust alternative to traditional rule-based systems. By
leveraging SLMs, we simplify vehicle control mechanisms and enhance the user
experience. Given the in-vehicle hardware constraints, we apply
state-of-the-art model compression techniques, including structured pruning,
healing, and quantization, ensuring that the model fits within the resource
limitations while maintaining acceptable performance. Our work focuses on
optimizing a representative SLM, Microsoft's Phi-3 mini, and outlines best
practices for enabling embedded models, including compression, task-specific
fine-tuning, and vehicle integration. We demonstrate that, despite significant
reduction in model size which removes up to 2 billion parameters from the
original model, our approach preserves the model's ability to handle complex
in-vehicle tasks accurately and efficiently. Furthermore, by executing the
model in a lightweight runtime environment, we achieve a generation speed of 11
tokens per second, making real-time, on-device inference feasible without
hardware acceleration. Our results demonstrate the potential of SLMs to
transform vehicle control systems, enabling more intuitive interactions between
users and their vehicles for an enhanced driving experience.","['cs.LG', 'cs.AI', 'cs.CL', 'cs.CV', 'cs.HC']",http://arxiv.org/abs/2501.02342v1
SymbolNet: Neural Symbolic Regression with Adaptive Dynamic Pruning for Compression,"Compact symbolic expressions have been shown to be more efficient than neural
network models in terms of resource consumption and inference speed when
implemented on custom hardware such as FPGAs, while maintaining comparable
accuracy~\cite{tsoi2023symbolic}. These capabilities are highly valuable in
environments with stringent computational resource constraints, such as
high-energy physics experiments at the CERN Large Hadron Collider. However,
finding compact expressions for high-dimensional datasets remains challenging
due to the inherent limitations of genetic programming, the search algorithm of
most symbolic regression methods. Contrary to genetic programming, the neural
network approach to symbolic regression offers scalability to high-dimensional
inputs and leverages gradient methods for faster equation searching. Common
ways of constraining expression complexity often involve multistage pruning
with fine-tuning, which can result in significant performance loss. In this
work, we propose $\tt{SymbolNet}$, a neural network approach to symbolic
regression specifically designed as a model compression technique, aimed at
enabling low-latency inference for high-dimensional inputs on custom hardware
such as FPGAs. This framework allows dynamic pruning of model weights, input
features, and mathematical operators in a single training process, where both
training loss and expression complexity are optimized simultaneously. We
introduce a sparsity regularization term for each pruning type, which can
adaptively adjust its strength, leading to convergence at a target sparsity
ratio. Unlike most existing symbolic regression methods that struggle with
datasets containing more than $\mathcal{O}(10)$ inputs, we demonstrate the
effectiveness of our model on the LHC jet tagging task (16 inputs), MNIST (784
inputs), and SVHN (3072 inputs).","['cs.LG', 'hep-ex', 'physics.ins-det']",http://arxiv.org/abs/2401.09949v3
Semantics Prompting Data-Free Quantization for Low-Bit Vision Transformers,"Data-free quantization (DFQ), which facilitates model quantization without
real data to address increasing concerns about data security, has garnered
significant attention within the model compression community. Recently, the
unique architecture of vision transformers (ViTs) has driven the development of
specialized DFQ techniques. However, we observe that the synthetic images from
existing methods suffer from the deficient semantics issue compared to real
images, thereby compromising performance. Motivated by this, we propose SPDFQ,
a Semantics Prompting Data-Free Quantization method for ViTs. First, SPDFQ
incorporates Attention Priors Alignment (APA), which uses randomly generated
attention priors to enhance the semantics of synthetic images. Second, SPDFQ
introduces Multi-Semantic Reinforcement (MSR), which utilizes localized patch
optimization to prompt efficient parameterization and diverse semantics in
synthetic images. Finally, SPDFQ employs Softlabel Learning (SL), where soft
learning targets are adapted to encourage more complex semantics and
accommodate images augmented by MSR. Experimental results demonstrate that
SPDFQ significantly outperforms existing methods. For instance, SPDFQ achieves
a 15.52% increase in top-1 accuracy on ImageNet for W4A4 ViT-B",['cs.CV'],http://arxiv.org/abs/2412.16553v2
Lillama: Large Language Models Compression via Low-Rank Feature Distillation,"Current LLM structured pruning methods typically involve two steps: (1)
compression with calibration data and (2) costly continued pretraining on
billions of tokens to recover lost performance. This second step is necessary
as the first significantly impacts model accuracy. Prior research suggests
pretrained Transformer weights aren't inherently low-rank, unlike their
activations, which may explain this drop. Based on this observation, we propose
Lillama, a compression method that locally distills activations with low-rank
weights. Using SVD for initialization and a joint loss combining teacher and
student activations, we accelerate convergence and reduce memory use with local
gradient updates. Lillama compresses Mixtral-8x7B within minutes on a single
A100 GPU, removing 10 billion parameters while retaining over 95% of its
original performance. Phi-2 3B can be compressed by 40% with just 13 million
calibration tokens, resulting in a small model that competes with recent models
of similar size. The method generalizes well to non-transformer architectures,
compressing Mamba-3B by 20% while maintaining 99% performance.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2412.16719v2
Optimization and Scalability of Collaborative Filtering Algorithms in Large Language Models,"With the rapid development of large language models (LLMs) and the growing
demand for personalized content, recommendation systems have become critical in
enhancing user experience and driving engagement. Collaborative filtering
algorithms, being core to many recommendation systems, have garnered
significant attention for their efficiency and interpretability. However,
traditional collaborative filtering approaches face numerous challenges when
integrated into large-scale LLM-based systems, including high computational
costs, severe data sparsity, cold start problems, and lack of scalability. This
paper investigates the optimization and scalability of collaborative filtering
algorithms in large language models, addressing these limitations through
advanced optimization strategies. Firstly, we analyze the fundamental
principles of collaborative filtering algorithms and their limitations when
applied in LLM-based contexts. Next, several optimization techniques such as
matrix factorization, approximate nearest neighbor search, and parallel
computing are proposed to enhance computational efficiency and model accuracy.
Additionally, strategies such as distributed architecture and model compression
are explored to facilitate dynamic updates and scalability in data-intensive
environments.","['cs.AI', 'cs.IR']",http://arxiv.org/abs/2412.18715v1
HTR-JAND: Handwritten Text Recognition with Joint Attention Network and Knowledge Distillation,"Despite significant advances in deep learning, current Handwritten Text
Recognition (HTR) systems struggle with the inherent complexity of historical
documents, including diverse writing styles, degraded text quality, and
computational efficiency requirements across multiple languages and time
periods. This paper introduces HTR-JAND (HTR-JAND: Handwritten Text Recognition
with Joint Attention Network and Knowledge Distillation), an efficient HTR
framework that combines advanced feature extraction with knowledge
distillation. Our architecture incorporates three key components: (1) a CNN
architecture integrating FullGatedConv2d layers with Squeeze-and-Excitation
blocks for adaptive feature extraction, (2) a Combined Attention mechanism
fusing Multi-Head Self-Attention with Proxima Attention for robust sequence
modeling, and (3) a Knowledge Distillation framework enabling efficient model
compression while preserving accuracy through curriculum-based training. The
HTR-JAND framework implements a multi-stage training approach combining
curriculum learning, synthetic data generation, and multi-task learning for
cross-dataset knowledge transfer. We enhance recognition accuracy through
context-aware T5 post-processing, particularly effective for historical
documents. Comprehensive evaluations demonstrate HTR-JAND's effectiveness,
achieving state-of-the-art Character Error Rates (CER) of 1.23\%, 1.02\%, and
2.02\% on IAM, RIMES, and Bentham datasets respectively. Our Student model
achieves a 48\% parameter reduction (0.75M versus 1.5M parameters) while
maintaining competitive performance through efficient knowledge transfer.
Source code and pre-trained models are available at
\href{https://github.com/DocumentRecognitionModels/HTR-JAND}{Github}.",['cs.CV'],http://arxiv.org/abs/2412.18524v1
Singular Value Scaling: Efficient Generative Model Compression via Pruned Weights Refinement,"While pruning methods effectively maintain model performance without extra
training costs, they often focus solely on preserving crucial connections,
overlooking the impact of pruned weights on subsequent fine-tuning or
distillation, leading to inefficiencies. Moreover, most compression techniques
for generative models have been developed primarily for GANs, tailored to
specific architectures like StyleGAN, and research into compressing Diffusion
models has just begun. Even more, these methods are often applicable only to
GANs or Diffusion models, highlighting the need for approaches that work across
both model types. In this paper, we introduce Singular Value Scaling (SVS), a
versatile technique for refining pruned weights, applicable to both model
types. Our analysis reveals that pruned weights often exhibit dominant singular
vectors, hindering fine-tuning efficiency and leading to suboptimal performance
compared to random initialization. Our method enhances weight initialization by
minimizing the disparities between singular values of pruned weights, thereby
improving the fine-tuning process. This approach not only guides the compressed
model toward superior solutions but also significantly speeds up fine-tuning.
Extensive experiments on StyleGAN2, StyleGAN3 and DDPM demonstrate that SVS
improves compression performance across model types without additional training
costs. Our code is available at:
https://github.com/LAIT-CVLab/Singular-Value-Scaling.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2412.17387v2
Understanding Artificial Neural Network's Behavior from Neuron Activation Perspective,"This paper explores the intricate behavior of deep neural networks (DNNs)
through the lens of neuron activation dynamics. We propose a probabilistic
framework that can analyze models' neuron activation patterns as a stochastic
process, uncovering theoretical insights into neural scaling laws, such as
over-parameterization and the power-law decay of loss with respect to dataset
size. By deriving key mathematical relationships, we present that the number of
activated neurons increases in the form of $N(1-(\frac{bN}{D+bN})^b)$, and the
neuron activation should follows power-law distribution. Based on these two
mathematical results, we demonstrate how DNNs maintain generalization
capabilities even under over-parameterization, and we elucidate the phase
transition phenomenon observed in loss curves as dataset size plotted in
log-axis (i.e. the data magnitude increases linearly). Moreover, by combining
the above two phenomenons and the power-law distribution of neuron activation,
we derived the power-law decay of neural network's loss function as the data
size scale increases. Furthermore, our analysis bridges the gap between
empirical observations and theoretical underpinnings, offering experimentally
testable predictions regarding parameter efficiency and model compressibility.
These findings provide a foundation for understanding neural network scaling
and present new directions for optimizing DNN performance.",['cs.AI'],http://arxiv.org/abs/2412.18073v1
CoSurfGS:Collaborative 3D Surface Gaussian Splatting with Distributed Learning for Large Scene Reconstruction,"3D Gaussian Splatting (3DGS) has demonstrated impressive performance in scene
reconstruction. However, most existing GS-based surface reconstruction methods
focus on 3D objects or limited scenes. Directly applying these methods to
large-scale scene reconstruction will pose challenges such as high memory
costs, excessive time consumption, and lack of geometric detail, which makes it
difficult to implement in practical applications. To address these issues, we
propose a multi-agent collaborative fast 3DGS surface reconstruction framework
based on distributed learning for large-scale surface reconstruction.
Specifically, we develop local model compression (LMC) and model aggregation
schemes (MAS) to achieve high-quality surface representation of large scenes
while reducing GPU memory consumption. Extensive experiments on Urban3d,
MegaNeRF, and BlendedMVS demonstrate that our proposed method can achieve fast
and scalable high-fidelity surface reconstruction and photorealistic rendering.
Our project page is available at \url{https://gyy456.github.io/CoSurfGS}.",['cs.CV'],http://arxiv.org/abs/2412.17612v1
GQSA: Group Quantization and Sparsity for Accelerating Large Language Model Inference,"With the rapid growth in the scale and complexity of large language models
(LLMs), the costs of training and inference have risen substantially. Model
compression has emerged as a mainstream solution to reduce memory usage and
computational overhead. This paper presents Group Quantization and Sparse
Acceleration (\textbf{GQSA}), a novel compression technique tailored for LLMs.
Traditional methods typically focus exclusively on either quantization or
sparsification, but relying on a single strategy often results in significant
performance loss at high compression rates. In contrast, GQSA integrates
quantization and sparsification in a tightly coupled manner, leveraging
GPU-friendly structured group sparsity and quantization for efficient
acceleration. The proposed method consists of three key steps. First, GQSA
applies group structured pruning to adhere to GPU-friendly sparse pattern
constraints. Second, a two-stage sparsity-aware training process is employed to
maximize performance retention after compression. Finally, the framework adopts
the Block Sparse Row (BSR) format to enable practical deployment and efficient
execution. Experimental results on the LLaMA model family show that GQSA
achieves an excellent balance between model speed and accuracy. Furthermore, on
the latest LLaMA-3 and LLaMA-3.1 models, GQSA outperforms existing LLM
compression techniques significantly.",['cs.LG'],http://arxiv.org/abs/2412.17560v1
Edge-AI for Agriculture: Lightweight Vision Models for Disease Detection in Resource-Limited Settings,"This research paper presents the development of a lightweight and efficient
computer vision pipeline aimed at assisting farmers in detecting orange
diseases using minimal resources. The proposed system integrates advanced
object detection, classification, and segmentation models, optimized for
deployment on edge devices, ensuring functionality in resource-limited
environments. The study evaluates the performance of various state-of-the-art
models, focusing on their accuracy, computational efficiency, and
generalization capabilities. Notable findings include the Vision Transformer
achieving 96 accuracy in orange species classification and the lightweight
YOLOv8-S model demonstrating exceptional object detection performance with
minimal computational overhead. The research highlights the potential of modern
deep learning architectures to address critical agricultural challenges,
emphasizing the importance of model complexity versus practical utility. Future
work will explore expanding datasets, model compression techniques, and
federated learning to enhance the applicability of these systems in diverse
agricultural contexts, ultimately contributing to more sustainable farming
practices.","['cs.CV', 'cs.AI', 'cs.CY']",http://arxiv.org/abs/2412.18635v1
Lightweight Design and Optimization methods for DCNNs: Progress and Futures,"Lightweight design, as a key approach to mitigate disparity between
computational requirements of deep learning models and hardware performance,
plays a pivotal role in advancing application of deep learning technologies on
mobile and embedded devices, alongside rapid development of smart home,
telemedicine, and autonomous driving. With its outstanding feature extracting
capabilities, Deep Convolutional Neural Networks (DCNNs) have demonstrated
superior performance in computer vision tasks. However, high computational
costs and large network architectures severely limit the widespread application
of DCNNs on resource-constrained hardware platforms such as smartphones,
robots, and IoT devices. This paper reviews lightweight design strategies for
DCNNs and examines recent research progress in both lightweight architectural
design and model compression. Additionally, this paper discusses current
limitations in this field of research and propose prospects for future
directions, aiming to provide valuable guidance and reflection for lightweight
design philosophy on deep neural networks in the field of computer vision.",['cs.CV'],http://arxiv.org/abs/2412.16886v1
TrimLLM: Progressive Layer Dropping for Domain-Specific LLMs,"Specializing large language models (LLMs) for local deployment in
domain-specific use cases is necessary for strong performance while meeting
latency and privacy constraints. However, conventional task-specific adaptation
approaches do not show simultaneous memory saving and inference speedup at
deployment time. Practical compression techniques like quantization and pruning
require dedicated hardware or kernel support to achieve measured inference
speedup. We develop TrimLLM based on the layer-wise specialization phenomenon
we empirically observed and verified on contemporary LLMs. TrimLLM reduces the
depth of LLMs via progressive layer dropping. We show it retains LLMs' capacity
in specific domains and achieves inference speedup irrespective of hardware and
deep learning frameworks. We evaluated TrimLLM on LLMs of various sizes for
inference; models adapted on medical, legal, and financial datasets all
demonstrate $2.1-5.7\times$ inference speedup on consumer GPUs and up to
$3.1\times$ speedup on A100 when compared to state-of-the-art model compression
algorithms, with no loss in accuracy at 50$\sim$60\% model compression ratio.","['cs.LG', 'cs.AI', 'cs.CL']",http://arxiv.org/abs/2412.11242v2
RemoteTrimmer: Adaptive Structural Pruning for Remote Sensing Image Classification,"Since high resolution remote sensing image classification often requires a
relatively high computation complexity, lightweight models tend to be practical
and efficient. Model pruning is an effective method for model compression.
However, existing methods rarely take into account the specificity of remote
sensing images, resulting in significant accuracy loss after pruning. To this
end, we propose an effective structural pruning approach for remote sensing
image classification. Specifically, a pruning strategy that amplifies the
differences in channel importance of the model is introduced. Then an adaptive
mining loss function is designed for the fine-tuning process of the pruned
model. Finally, we conducted experiments on two remote sensing classification
datasets. The experimental results demonstrate that our method achieves minimal
accuracy loss after compressing remote sensing classification models, achieving
state-of-the-art (SoTA) performance.",['cs.CV'],http://arxiv.org/abs/2412.12603v2
Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN,"Large Language Models (LLMs) have achieved remarkable success, yet recent
findings reveal that their deeper layers often contribute minimally and can be
pruned without affecting overall performance. While some view this as an
opportunity for model compression, we identify it as a training shortfall
rooted in the widespread use of Pre-Layer Normalization (Pre-LN). We
demonstrate that Pre-LN, commonly employed in models like GPT and LLaMA, leads
to diminished gradient norms in its deeper layers, reducing their
effectiveness. In contrast, Post-Layer Normalization (Post-LN) preserves larger
gradient norms in deeper layers but suffers from vanishing gradients in earlier
layers. To address this, we introduce Mix-LN, a novel normalization technique
that combines the strengths of Pre-LN and Post-LN within the same model. Mix-LN
applies Post-LN to the earlier layers and Pre-LN to the deeper layers, ensuring
more uniform gradients across layers. This allows all parts of the
network--both shallow and deep layers--to contribute effectively to training.
Extensive experiments with various model sizes from 70M to 7B demonstrate that
Mix-LN consistently outperforms both Pre-LN and Post-LN, promoting more
balanced, healthier gradient norms throughout the network, and enhancing the
overall quality of LLM pre-training. Furthermore, we demonstrate that models
pre-trained with Mix-LN learn better compared to those using Pre-LN or Post-LN
during supervised fine-tuning (SFT) and reinforcement learning from human
feedback (RLHF), highlighting the critical importance of high-quality deep
layers. By effectively addressing the inefficiencies of deep layers in current
LLMs, Mix-LN unlocks their potential, enhancing model capacity without
increasing model size. Our code is available at
https://github.com/pixeli99/MixLN.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2412.13795v1
L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large Language Models,"Due to the high memory and computational costs associated with large language
models (LLMs), model compression techniques such as quantization, which reduces
inference costs, and parameter-efficient fine-tuning (PEFT) methods like
Low-Rank Adaptation (LoRA), which reduce training costs, have gained
significant popularity. This trend has spurred active research into
quantization-aware PEFT techniques, aimed at maintaining model accuracy while
minimizing memory overhead during both inference and training. Previous
quantization-aware PEFT methods typically apply post-training quantization
(PTQ) to pre-trained LLMs, followed by PEFT to recover accuracy loss.
Meanwhile, this approach has limitations in recovering the accuracy loss. In
this paper, we propose L4Q, a method that integrates Quantization-Aware
Training (QAT) with LoRA. By employing a memory-optimized layer design, L4Q
significantly reduces QAT's memory overhead, making its training cost
comparable to LoRA, while preserving the advantage of QAT in producing fully
quantized LLMs with high accuracy. Our experiments demonstrate that this
combined approach to quantization and fine-tuning achieves superior accuracy
compared to decoupled fine-tuning schemes, particularly in 4-bit and 3-bit
quantization, positioning L4Q as an efficient QAT solution. Using the LLaMA and
Mistral models with instructional datasets, we showcase L4Q's capabilities in
language tasks and few-shot learning.","['cs.LG', 'cs.CL']",http://arxiv.org/abs/2402.04902v5
SEE: Sememe Entanglement Encoding for Transformer-bases Models Compression,"Transformer-based large language models exhibit groundbreaking capabilities,
but their storage and computational costs are prohibitively high, limiting
their application in resource-constrained scenarios. An effective approach is
to eliminate redundant model parameters and computational costs while
incorporating efficient expert-derived knowledge structures to achieve a
balance between compression and performance. Therefore, we propose the
\textit{Sememe Entanglement Encoding (SEE)} algorithm. Guided by expert prior
knowledge, the model is compressed through the low-rank approximation idea. In
Entanglement Embedding, basic semantic units such as sememes are represented as
low-dimensional vectors, and then reconstructed into high-dimensional word
embeddings through the combination of generalized quantum entanglement. We
adapt the Sememe Entanglement Encoding algorithm to transformer-based models of
different magnitudes. Experimental results indicate that our approach achieves
stable performance while compressing model parameters and computational costs.","['cs.LG', 'cs.AI', 'cs.CL']",http://arxiv.org/abs/2412.12204v1
BD-KD: Balancing the Divergences for Online Knowledge Distillation,"We address the challenge of producing trustworthy and accurate compact models
for edge devices. While Knowledge Distillation (KD) has improved model
compression in terms of achieving high accuracy performance, calibration of
these compact models has been overlooked. We introduce BD-KD (Balanced
Divergence Knowledge Distillation), a framework for logit-based online KD.
BD-KD enhances both accuracy and model calibration simultaneously, eliminating
the need for post-hoc recalibration techniques, which add computational
overhead to the overall training pipeline and degrade performance. Our method
encourages student-centered training by adjusting the conventional online
distillation loss on both the student and teacher losses, employing sample-wise
weighting of forward and reverse Kullback-Leibler divergence. This strategy
balances student network confidence and boosts performance. Experiments across
CIFAR10, CIFAR100, TinyImageNet, and ImageNet datasets, and various
architectures demonstrate improved calibration and accuracy compared to recent
online KD methods.",['cs.CV'],http://arxiv.org/abs/2212.12965v2
Hyper-Compression: Model Compression via Hyperfunction,"The rapid growth of large models' size has far outpaced that of GPU memory.
To bridge this gap, inspired by the parsimonious relationship between genotype
and phenotype, we turn the model compression problem into the issue of
parameter representation to propose the so-called hyper-compression. The
hyper-compression uses a hyperfunction to represent the parameters of the
target network per ergodic theory, that addresses the following approximation
problem: if a low-dimensional dynamic system can fill the high-dimensional
space eventually. Empirically, the proposed hyper-compression enjoys the
following merits: 1) \textbf{P}referable compression ratio; 2) \textbf{N}o
post-hoc retraining; 3) \textbf{A}ffordable inference time; and 4)
\textbf{S}hort compression time. It compresses LLaMA2-7B in an hour and
achieves close-to-int4-quantization performance, without retraining and with a
performance drop of less than 1\%. Our work can facilitate the harmony between
the scaling law and the stagnation of hardware upgradation in terms of saving
both computation and data. We have open-sourced our
\href{https://github.com/Juntongkuki/Hyper-Compression.git}{code} for readers'
free download and evaluation.","['cs.LG', 'cs.AI', 'cs.ET']",http://arxiv.org/abs/2409.00592v2
Can Students Beyond The Teacher? Distilling Knowledge from Teacher's Bias,"Knowledge distillation (KD) is a model compression technique that transfers
knowledge from a large teacher model to a smaller student model to enhance its
performance. Existing methods often assume that the student model is inherently
inferior to the teacher model. However, we identify that the fundamental issue
affecting student performance is the bias transferred by the teacher. Current
KD frameworks transmit both right and wrong knowledge, introducing bias that
misleads the student model. To address this issue, we propose a novel strategy
to rectify bias and greatly improve the student model's performance. Our
strategy involves three steps: First, we differentiate knowledge and design a
bias elimination method to filter out biases, retaining only the right
knowledge for the student model to learn. Next, we propose a bias rectification
method to rectify the teacher model's wrong predictions, fundamentally
addressing bias interference. The student model learns from both the right
knowledge and the rectified biases, greatly improving its prediction accuracy.
Additionally, we introduce a dynamic learning approach with a loss function
that updates weights dynamically, allowing the student model to quickly learn
right knowledge-based easy tasks initially and tackle hard tasks corresponding
to biases later, greatly enhancing the student model's learning efficiency. To
the best of our knowledge, this is the first strategy enabling the student
model to surpass the teacher model. Experiments demonstrate that our strategy,
as a plug-and-play module, is versatile across various mainstream KD
frameworks. We will release our code after the paper is accepted.",['cs.CV'],http://arxiv.org/abs/2412.09874v1
Activation Sparsity Opportunities for Compressing General Large Language Models,"Deploying local AI models, such as Large Language Models (LLMs), to edge
devices can substantially enhance devices' independent capabilities, alleviate
the server's burden, and lower the response time. Owing to these tremendous
potentials, many big tech companies have released several lightweight Small
Language Models (SLMs) to bridge this gap. However, we still have huge
motivations to deploy more powerful (LLMs) AI models on edge devices and
enhance their smartness level. Unlike the conventional approaches for AI model
compression, we investigate activation sparsity. The activation sparsity method
is orthogonal and combinable with existing techniques to maximize compression
rate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)
components, which typically comprise a large proportion of parameters (around
3/2), ensure that our FFN optimizations would have a better chance of achieving
effective compression. Moreover, our findings are beneficial to general LLMs
and are not restricted to ReLU-based models. This work systematically
investigates the tradeoff between enforcing activation sparsity and perplexity
(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that
we can obtain around 50% of main memory and computing reductions for critical
FFN components with negligible accuracy degradation. This extra 50% sparsity
does not naturally exist in the current LLMs, which require tuning LLMs'
activation outputs by injecting zero-enforcing thresholds. To obtain the
benefits of activation sparsity, we provide a guideline for the system
architect for LLM prediction and prefetching. The success prediction allows the
system to prefetch the necessary weights while omitting the inactive ones and
their successors, therefore lowering cache and memory pollution and reducing
LLM execution time on resource-constrained edge devices.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2412.12178v1
Optimising TinyML with Quantization and Distillation of Transformer and Mamba Models for Indoor Localisation on Edge Devices,"This paper proposes small and efficient machine learning models (TinyML) for
resource-constrained edge devices, specifically for on-device indoor
localisation. Typical approaches for indoor localisation rely on centralised
remote processing of data transmitted from lower powered devices such as
wearables. However, there are several benefits for moving this to the edge
device itself, including increased battery life, enhanced privacy, reduced
latency and lowered operational costs, all of which are key for common
applications such as health monitoring. The work focuses on model compression
techniques, including quantization and knowledge distillation, to significantly
reduce the model size while maintaining high predictive performance. We base
our work on a large state-of-the-art transformer-based model and seek to deploy
it within low-power MCUs. We also propose a state-space-based architecture
using Mamba as a more compact alternative to the transformer. Our results show
that the quantized transformer model performs well within a 64 KB RAM
constraint, achieving an effective balance between model size and localisation
precision. Additionally, the compact Mamba model has strong performance under
even tighter constraints, such as a 32 KB of RAM, without the need for model
compression, making it a viable option for more resource-limited environments.
We demonstrate that, through our framework, it is feasible to deploy advanced
indoor localisation models onto low-power MCUs with restricted memory
limitations. The application of these TinyML models in healthcare has the
potential to revolutionize patient monitoring by providing accurate, real-time
location data while minimizing power consumption, increasing data privacy,
improving latency and reducing infrastructure costs.","['cs.LG', 'cs.SE']",http://arxiv.org/abs/2412.09289v1
ASER: Activation Smoothing and Error Reconstruction for Large Language Model Quantization,"Quantization stands as a pivotal technique for large language model (LLM)
serving, yet it poses significant challenges particularly in achieving
effective low-bit quantization. The limited numerical mapping makes the
quantized model produce a non-trivial error, bringing out intolerable
performance degration. This paper is anchored in the basic idea of model
compression objectives, and delves into the layer-wise error distribution of
LLMs during post-training quantization. Subsequently, we introduce ASER, an
algorithm consisting of (1) Error Reconstruction: low-rank compensation for
quantization error with LoRA-style matrices constructed by whitening SVD; (2)
Activation Smoothing: outlier extraction to gain smooth activation and better
error compensation. ASER is capable of quantizing typical LLMs to low-bit ones,
particularly preserving accuracy even in W4A8 per-channel setup. Experimental
results show that ASER is competitive among the state-of-the-art quantization
algorithms, showing potential to activation quantization, with minor overhead.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2411.07762v2
Low-Rank Correction for Quantized LLMs,"We consider the problem of model compression for Large Language Models (LLMs)
at post-training time, where the task is to compress a well-trained model using
only a small set of calibration input data. In this work, we introduce a new
low-rank approach to correct for quantization errors of \emph{activations} in
LLMs: we propose to add low-rank weight matrices in full precision that act on
the \emph{unquantized} activations. We then solve a joint optimization problem
over the quantized representation of the weights and additional low-rank weight
matrices to quantize both weights and activations. We focus on the case of
4-bit weight-and-activation quantization (W4A4). Using ranks equivalent to 10\%
of the original weight matrix size, our approach reduces the accuracy gap with
the original model by more than 50\%. Using ranks equivalent to 30\% of the
original weight matrix, the accuracy gap is closed completely. We demonstrate
our results on four recent LLMs, namely Llama-2, Llama-3, Phi-3 and Mixtral
models.","['stat.ML', 'cs.LG']",http://arxiv.org/abs/2412.07902v1
VQ4ALL: Efficient Neural Network Representation via a Universal Codebook,"The rapid growth of the big neural network models puts forward new
requirements for lightweight network representation methods. The traditional
methods based on model compression have achieved great success, especially VQ
technology which realizes the high compression ratio of models by sharing code
words. However, because each layer of the network needs to build a code table,
the traditional top-down compression technology lacks attention to the
underlying commonalities, resulting in limited compression rate and frequent
memory access. In this paper, we propose a bottom-up method to share the
universal codebook among multiple neural networks, which not only effectively
reduces the number of codebooks but also further reduces the memory access and
chip area by storing static code tables in the built-in ROM. Specifically, we
introduce VQ4ALL, a VQ-based method that utilizes codewords to enable the
construction of various neural networks and achieve efficient representations.
The core idea of our method is to adopt a kernel density estimation approach to
extract a universal codebook and then progressively construct different low-bit
networks by updating differentiable assignments. Experimental results
demonstrate that VQ4ALL achieves compression rates exceeding 16 $\times$ while
preserving high accuracy across multiple network architectures, highlighting
its effectiveness and versatility.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2412.06875v1
Compression for Better: A General and Stable Lossless Compression Framework,"This work focus on how to stabilize and lossless model compression, aiming to
reduce model complexity and enhance efficiency without sacrificing performance
due to compression errors. A key challenge is effectively leveraging
compression errors and defining the boundaries for lossless compression to
minimize model loss. i.e., compression for better. Currently, there is no
systematic approach to determining this error boundary or understanding its
specific impact on model performance. We propose a general
\textbf{L}oss\textbf{L}ess \textbf{C}ompression theoretical framework
(\textbf{LLC}), which further delineates the compression neighborhood and
higher-order analysis boundaries through the total differential, thereby
specifying the error range within which a model can be compressed without loss.
To verify the effectiveness of LLC, we apply various compression techniques,
including quantization and decomposition. Specifically, for quantization, we
reformulate the classic quantization search problem as a grouped knapsack
problem within the lossless neighborhood, achieving lossless quantization while
improving computational efficiency. For decomposition, LLC addresses the
approximation problem under low-rank constraints, automatically determining the
rank for each layer and producing lossless low-rank models. We conduct
extensive experiments on multiple neural network architectures on different
datasets. The results show that without fancy tricks, LLC can effectively
achieve lossless model compression. Our code will be made publicly.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2412.06868v1
Robust Change Captioning in Remote Sensing: SECOND-CC Dataset and MModalCC Framework,"Remote sensing change captioning (RSICC) aims to describe changes between
bitemporal images in natural language. Existing methods often fail under
challenges like illumination differences, viewpoint changes, blur effects,
leading to inaccuracies, especially in no-change regions. Moreover, the images
acquired at different spatial resolutions and have registration errors tend to
affect the captions. To address these issues, we introduce SECOND-CC, a novel
RSICC dataset featuring high-resolution RGB image pairs, semantic segmentation
maps, and diverse real-world scenarios. SECOND-CC which contains 6,041 pairs of
bitemporal RS images and 30,205 sentences describing the differences between
images. Additionally, we propose MModalCC, a multimodal framework that
integrates semantic and visual data using advanced attention mechanisms,
including Cross-Modal Cross Attention (CMCA) and Multimodal Gated Cross
Attention (MGCA). Detailed ablation studies and attention visualizations
further demonstrate its effectiveness and ability to address RSICC challenges.
Comprehensive experiments show that MModalCC outperforms state-of-the-art RSICC
methods, including RSICCformer, Chg2Cap, and PSNet with +4.6% improvement on
BLEU4 score and +9.6% improvement on CIDEr score. We will make our dataset and
codebase publicly available to facilitate future research at
https://github.com/ChangeCapsInRS/SecondCC","['cs.CV', 'cs.AI', 'cs.LG', 'cs.MM']",http://arxiv.org/abs/2501.10075v1
LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation for Design Space Exploration,"GraphRAG integrates (knowledge) graphs with large language models (LLMs) to
improve reasoning accuracy and contextual relevance. Despite its promising
applications and strong relevance to multiple research communities, such as
databases and natural language processing, GraphRAG currently lacks modular
workflow analysis, systematic solution frameworks, and insightful empirical
studies. To bridge these gaps, we propose LEGO-GraphRAG, a modular framework
that enables: 1) fine-grained decomposition of the GraphRAG workflow, 2)
systematic classification of existing techniques and implemented GraphRAG
instances, and 3) creation of new GraphRAG instances. Our framework facilitates
comprehensive empirical studies of GraphRAG on large-scale real-world graphs
and diverse query sets, revealing insights into balancing reasoning quality,
runtime efficiency, and token or GPU cost, that are essential for building
advanced GraphRAG systems.","['cs.AI', 'cs.CL']",http://arxiv.org/abs/2411.05844v2
Evolving Deeper LLM Thinking,"We explore an evolutionary search strategy for scaling inference time compute
in Large Language Models. The proposed approach, Mind Evolution, uses a
language model to generate, recombine and refine candidate responses. The
proposed approach avoids the need to formalize the underlying inference problem
whenever a solution evaluator is available. Controlling for inference cost, we
find that Mind Evolution significantly outperforms other inference strategies
such as Best-of-N and Sequential Revision in natural language planning tasks.
In the TravelPlanner and Natural Plan benchmarks, Mind Evolution solves more
than 98% of the problem instances using Gemini 1.5 Pro without the use of a
formal solver.",['cs.AI'],http://arxiv.org/abs/2501.09891v1
Enhancing Generalization in Chain of Thought Reasoning for Smaller Models,"Chain-of-Thought (CoT) reasoning in smaller language models is a challenging
natural language process problem yet highly desirable in many real-life
applications. Existing CoT knowledge distillation methods often suffer from
overly conservative memorization in smaller LLMs, leading to low generalization
confidence. As fully preserving the CoT ability of teacher model is impossible,
we hypothesize that adversarial CoT fine-tuning is crucial for developing
smaller LLM with robust CoT generalization. To this end, we propose
\textit{PRompt-Assisted Domain-Adversarial fine-tuning} (PRADA), a principled
fine-tuning framework that integrates diverse CoT domains. Specifically, PRADA
pioneers two CoT improvements in smaller LLM: (1) Recovering the
domain-invariant feature insight which typically lost during distillation with
domain adversarial fine-tuning; (2) Enhancing the domain adaptability of CoT
prompt engineering by employing domain-adversarial approaches. We theoretically
demonstrate the effectiveness of our approach and empirically show that it
significantly outperforms the state of the arts in a wide range of tasks.
Moreover, our empirical findings reveal that the smaller LLM, when leveraging
PRADA, aligns closely with domain knowledge, thereby improving the
explainability of our approach.","['cs.LG', 'cs.AI', 'cs.CL']",http://arxiv.org/abs/2501.09804v1
Generating particle physics Lagrangians with transformers,"In physics, Lagrangians provide a systematic way to describe laws governing
physical systems. In the context of particle physics, they encode the
interactions and behavior of the fundamental building blocks of our universe.
By treating Lagrangians as complex, rule-based constructs similar to linguistic
expressions, we trained a transformer model -- proven to be effective in
natural language tasks -- to predict the Lagrangian corresponding to a given
list of particles. We report on the transformer's performance in constructing
Lagrangians respecting the Standard Model $\mathrm{SU}(3)\times
\mathrm{SU}(2)\times \mathrm{U}(1)$ gauge symmetries. The resulting model is
shown to achieve high accuracies (over 90\%) with Lagrangians up to six matter
fields, with the capacity to generalize beyond the training distribution,
albeit within architectural constraints. We show through an analysis of input
embeddings that the model has internalized concepts such as group
representations and conjugation operations as it learned to generate
Lagrangians. We make the model and training datasets available to the
community. An interactive demonstration can be found at:
\url{https://huggingface.co/spaces/JoseEliel/generate-lagrangians}.","['cs.LG', 'cs.SC', 'hep-ph', 'hep-th']",http://arxiv.org/abs/2501.09729v1
The Goofus & Gallant Story Corpus for Practical Value Alignment,"Values or principles are key elements of human society that influence people
to behave and function according to an accepted standard set of social rules to
maintain social order. As AI systems are becoming ubiquitous in human society,
it is a major concern that they could violate these norms or values and
potentially cause harm. Thus, to prevent intentional or unintentional harm, AI
systems are expected to take actions that align with these principles. Training
systems to exhibit this type of behavior is difficult and often requires a
specialized dataset. This work presents a multi-modal dataset illustrating
normative and non-normative behavior in real-life situations described through
natural language and artistic images. This training set contains curated sets
of images that are designed to teach young children about social principles. We
argue that this is an ideal dataset to use for training socially normative
agents given this fact.",['cs.AI'],http://arxiv.org/abs/2501.09707v1
A Comprehensive Survey of Foundation Models in Medicine,"Foundation models (FMs) are large-scale deep learning models trained on
massive datasets, often using self-supervised learning techniques. These models
serve as a versatile base for a wide range of downstream tasks, including those
in medicine and healthcare. FMs have demonstrated remarkable success across
multiple healthcare domains. However, existing surveys in this field do not
comprehensively cover all areas where FMs have made significant strides. In
this survey, we present a comprehensive review of FMs in medicine, focusing on
their evolution, learning strategies, flagship models, applications, and
associated challenges. We examine how prominent FMs, such as the BERT and GPT
families, are transforming various aspects of healthcare, including clinical
large language models, medical image analysis, and omics research.
Additionally, we provide a detailed taxonomy of FM-enabled healthcare
applications, spanning clinical natural language processing, medical computer
vision, graph learning, and other biology- and omics- related tasks. Despite
the transformative potentials of FMs, they also pose unique challenges. This
survey delves into these challenges and highlights open research questions and
lessons learned to guide researchers and practitioners. Our goal is to provide
valuable insights into the capabilities of FMs in health, facilitating
responsible deployment and mitigating associated risks.","['cs.LG', 'cs.AI', 'cs.CV']",http://arxiv.org/abs/2406.10729v3
Confidence Estimation for Error Detection in Text-to-SQL Systems,"Text-to-SQL enables users to interact with databases through natural
language, simplifying the retrieval and synthesis of information. Despite the
success of large language models (LLMs) in converting natural language
questions into SQL queries, their broader adoption is limited by two main
challenges: achieving robust generalization across diverse queries and ensuring
interpretative confidence in their predictions. To tackle these issues, our
research investigates the integration of selective classifiers into Text-to-SQL
systems. We analyse the trade-off between coverage and risk using entropy based
confidence estimation with selective classifiers and assess its impact on the
overall performance of Text-to-SQL models. Additionally, we explore the models'
initial calibration and improve it with calibration techniques for better model
alignment between confidence and accuracy. Our experimental results show that
encoder-decoder T5 is better calibrated than in-context-learning GPT 4 and
decoder-only Llama 3, thus the designated external entropy-based selective
classifier has better performance. The study also reveal that, in terms of
error detection, selective classifier with a higher probability detects errors
associated with irrelevant questions rather than incorrect query generations.","['cs.LG', 'cs.CL']",http://arxiv.org/abs/2501.09527v1
AugRefer: Advancing 3D Visual Grounding via Cross-Modal Augmentation and Spatial Relation-based Referring,"3D visual grounding (3DVG), which aims to correlate a natural language
description with the target object within a 3D scene, is a significant yet
challenging task. Despite recent advancements in this domain, existing
approaches commonly encounter a shortage: a limited amount and diversity of
text3D pairs available for training. Moreover, they fall short in effectively
leveraging different contextual clues (e.g., rich spatial relations within the
3D visual space) for grounding. To address these limitations, we propose
AugRefer, a novel approach for advancing 3D visual grounding. AugRefer
introduces cross-modal augmentation designed to extensively generate diverse
text-3D pairs by placing objects into 3D scenes and creating accurate and
semantically rich descriptions using foundation models. Notably, the resulting
pairs can be utilized by any existing 3DVG methods for enriching their training
data. Additionally, AugRefer presents a language-spatial adaptive decoder that
effectively adapts the potential referring objects based on the language
description and various 3D spatial relations. Extensive experiments on three
benchmark datasets clearly validate the effectiveness of AugRefer.",['cs.CV'],http://arxiv.org/abs/2501.09428v1
Vision-Language Models Do Not Understand Negation,"Many practical vision-language applications require models that understand
negation, e.g., when using natural language to retrieve images which contain
certain objects but not others. Despite advancements in vision-language models
(VLMs) through large-scale training, their ability to comprehend negation
remains underexplored. This study addresses the question: how well do current
VLMs understand negation? We introduce NegBench, a new benchmark designed to
evaluate negation understanding across 18 task variations and 79k examples
spanning image, video, and medical datasets. The benchmark consists of two core
tasks designed to evaluate negation understanding in diverse multimodal
settings: Retrieval with Negation and Multiple Choice Questions with Negated
Captions. Our evaluation reveals that modern VLMs struggle significantly with
negation, often performing at chance level. To address these shortcomings, we
explore a data-centric approach wherein we finetune CLIP models on large-scale
synthetic datasets containing millions of negated captions. We show that this
approach can result in a 10% increase in recall on negated queries and a 40%
boost in accuracy on multiple-choice questions with negated captions.","['cs.CV', 'cs.CL']",http://arxiv.org/abs/2501.09425v1
SOP-Agent: Empower General Purpose AI Agent with Domain-Specific SOPs,"Despite significant advancements in general-purpose AI agents, several
challenges still hinder their practical application in real-world scenarios.
First, the limited planning capabilities of Large Language Models (LLM)
restrict AI agents from effectively solving complex tasks that require
long-horizon planning. Second, general-purpose AI agents struggle to
efficiently utilize domain-specific knowledge and human expertise. In this
paper, we introduce the Standard Operational Procedure-guided Agent
(SOP-agent), a novel framework for constructing domain-specific agents through
pseudocode-style Standard Operational Procedures (SOPs) written in natural
language. Formally, we represent a SOP as a decision graph, which is traversed
to guide the agent in completing tasks specified by the SOP. We conduct
extensive experiments across tasks in multiple domains, including
decision-making, search and reasoning, code generation, data cleaning, and
grounded customer service. The SOP-agent demonstrates excellent versatility,
achieving performance superior to general-purpose agent frameworks and
comparable to domain-specific agent systems. Additionally, we introduce the
Grounded Customer Service Benchmark, the first benchmark designed to evaluate
the grounded decision-making capabilities of AI agents in customer service
scenarios based on SOPs.",['cs.AI'],http://arxiv.org/abs/2501.09316v1
Text Semantics to Flexible Design: A Residential Layout Generation Method Based on Stable Diffusion Model,"Flexibility in the AI-based residential layout design remains a significant
challenge, as traditional methods like rule-based heuristics and graph-based
generation often lack flexibility and require substantial design knowledge from
users. To address these limitations, we propose a cross-modal design approach
based on the Stable Diffusion model for generating flexible residential
layouts. The method offers multiple input types for learning objectives,
allowing users to specify both boundaries and layouts. It incorporates natural
language as design constraints and introduces ControlNet to enable stable
layout generation through two distinct pathways. We also present a scheme that
encapsulates design expertise within a knowledge graph and translates it into
natural language, providing an interpretable representation of design
knowledge. This comprehensibility and diversity of input options enable
professionals and non-professionals to directly express design requirements,
enhancing flexibility and controllability. Finally, experiments verify the
flexibility of the proposed methods under multimodal constraints better than
state-of-the-art models, even when specific semantic information about room
areas or connections is incomplete.",['cs.AI'],http://arxiv.org/abs/2501.09279v1
Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG,"Large Language Models (LLMs) have revolutionized artificial intelligence (AI)
by enabling human like text generation and natural language understanding.
However, their reliance on static training data limits their ability to respond
to dynamic, real time queries, resulting in outdated or inaccurate outputs.
Retrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs
by integrating real time data retrieval to provide contextually relevant and
up-to-date responses. Despite its promise, traditional RAG systems are
constrained by static workflows and lack the adaptability required for
multistep reasoning and complex task management.
  Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these
limitations by embedding autonomous AI agents into the RAG pipeline. These
agents leverage agentic design patterns reflection, planning, tool use, and
multiagent collaboration to dynamically manage retrieval strategies,
iteratively refine contextual understanding, and adapt workflows to meet
complex task requirements. This integration enables Agentic RAG systems to
deliver unparalleled flexibility, scalability, and context awareness across
diverse applications.
  This survey provides a comprehensive exploration of Agentic RAG, beginning
with its foundational principles and the evolution of RAG paradigms. It
presents a detailed taxonomy of Agentic RAG architectures, highlights key
applications in industries such as healthcare, finance, and education, and
examines practical implementation strategies. Additionally, it addresses
challenges in scaling these systems, ensuring ethical decision making, and
optimizing performance for real-world applications, while providing detailed
insights into frameworks and tools for implementing Agentic RAG","['cs.AI', 'cs.CL', 'cs.IR']",http://arxiv.org/abs/2501.09136v1
TextSleuth: Towards Explainable Tampered Text Detection,"Recently, tampered text detection has attracted increasing attention due to
its essential role in information security. Although existing methods can
detect the tampered text region, the interpretation of such detection remains
unclear, making the prediction unreliable. To address this problem, we propose
to explain the basis of tampered text detection with natural language via large
multimodal models. To fill the data gap for this task, we propose a
large-scale, comprehensive dataset, ETTD, which contains both pixel-level
annotations for tampered text region and natural language annotations
describing the anomaly of the tampered text. Multiple methods are employed to
improve the quality of the proposed data. For example, elaborate queries are
introduced to generate high-quality anomaly descriptions with GPT4o. A fused
mask prompt is proposed to reduce confusion when querying GPT4o to generate
anomaly descriptions. To automatically filter out low-quality annotations, we
also propose to prompt GPT4o to recognize tampered texts before describing the
anomaly, and to filter out the responses with low OCR accuracy. To further
improve explainable tampered text detection, we propose a simple yet effective
model called TextSleuth, which achieves improved fine-grained perception and
cross-domain generalization by focusing on the suspected region, with a
two-stage analysis paradigm and an auxiliary grounding prompt. Extensive
experiments on both the ETTD dataset and the public dataset have verified the
effectiveness of the proposed methods. In-depth analysis is also provided to
inspire further research. Our dataset and code will be open-source.",['cs.CV'],http://arxiv.org/abs/2412.14816v3
High-Rank Irreducible Cartesian Tensor Decomposition and Bases of Equivariant Spaces,"Irreducible Cartesian tensors (ICTs) play a crucial role in the design of
equivariant graph neural networks, as well as in theoretical chemistry and
chemical physics. Meanwhile, the design space of available linear operations on
tensors that preserve symmetry presents a significant challenge. The ICT
decomposition and a basis of this equivariant space are difficult to obtain for
high-rank tensors. After decades of research, Bonvicini (2024) recently
achieves an explicit ICT decomposition for $n=5$ with factorial time/space
complexity. In this work we, for the first time, obtains decomposition matrices
for ICTs up to rank $n=9$ with reduced and affordable complexity, by
constructing what we call path matrices. The path matrices are obtained via
performing chain-like contractions with Clebsch-Gordan matrices following the
parentage scheme. We prove and leverage that the concatenation of path matrices
is an orthonormal change-of-basis matrix between the Cartesian tensor product
space and the spherical direct sum spaces. Furthermore, we identify a complete
orthogonal basis for the equivariant space, rather than a spanning set
(Pearce-Crump, 2023), through this path matrices technique. To the best of our
knowledge, this is also the first analytic, rather than numerical, method for
theoretically obtaining arbitrary rank orthogonal ICT decomposition matrices
and orthogonal equivariant bases. We further extend our result to the arbitrary
tensor product and direct sum spaces, enabling free design between different
spaces while keeping symmetry. The Python code is available at
https://github.com/ShihaoShao-GH/ICT-decomposition-and-equivariant-bases, where
the $n=6,\dots,9$ ICT decomposition matrices are obtained in 1s, 3s, 11s, and
4m32s on 28-cores Intel(R) Xeon(R) Gold 6330 CPU @ 2.00GHz, respectively.","['cs.LG', 'math-ph', 'math.MP', 'physics.chem-ph', 'physics.comp-ph', 'quant-ph']",http://arxiv.org/abs/2412.18263v4
SpaceTime: Causal Discovery from Non-Stationary Time Series,"Understanding causality is challenging and often complicated by changing
causal relationships over time and across environments. Climate patterns, for
example, shift over time with recurring seasonal trends, while also depending
on geographical characteristics such as ecosystem variability. Existing methods
for discovering causal graphs from time series either assume stationarity, do
not permit both temporal and spatial distribution changes, or are unaware of
locations with the same causal relationships. In this work, we therefore unify
the three tasks of causal graph discovery in the non-stationary multi-context
setting, of reconstructing temporal regimes, and of partitioning datasets and
time intervals into those where invariant causal relationships hold. To
construct a consistent score that forms the basis of our method, we employ the
Minimum Description Length principle. Our resulting algorithm SPACETIME
simultaneously accounts for heterogeneity across space and non-stationarity
over time. Given multiple time series, it discovers regime changepoints and a
temporal causal graph using non-parametric functional modeling and kernelized
discrepancy testing. We also show that our method provides insights into
real-world phenomena such as river-runoff measured at different catchments and
biosphere-atmosphere interactions across ecosystems.",['cs.LG'],http://arxiv.org/abs/2501.10235v1
Temporal Graph MLP Mixer for Spatio-Temporal Forecasting,"Spatiotemporal forecasting is critical in applications such as traffic
prediction, climate modeling, and environmental monitoring. However, the
prevalence of missing data in real-world sensor networks significantly
complicates this task. In this paper, we introduce the Temporal Graph MLP-Mixer
(T-GMM), a novel architecture designed to address these challenges. The model
combines node-level processing with patch-level subgraph encoding to capture
localized spatial dependencies while leveraging a three-dimensional MLP-Mixer
to handle temporal, spatial, and feature-based dependencies. Experiments on the
AQI, ENGRAD, PV-US and METR-LA datasets demonstrate the model's ability to
effectively forecast even in the presence of significant missing data. While
not surpassing state-of-the-art models in all scenarios, the T-GMM exhibits
strong learning capabilities, particularly in capturing long-range
dependencies. These results highlight its potential for robust, scalable
spatiotemporal forecasting.",['cs.LG'],http://arxiv.org/abs/2501.10214v1
Structure-guided Deep Multi-View Clustering,"Deep multi-view clustering seeks to utilize the abundant information from
multiple views to improve clustering performance. However, most of the existing
clustering methods often neglect to fully mine multi-view structural
information and fail to explore the distribution of multi-view data, limiting
clustering performance. To address these limitations, we propose a
structure-guided deep multi-view clustering model. Specifically, we introduce a
positive sample selection strategy based on neighborhood relationships, coupled
with a corresponding loss function. This strategy constructs multi-view nearest
neighbor graphs to dynamically redefine positive sample pairs, enabling the
mining of local structural information within multi-view data and enhancing the
reliability of positive sample selection. Additionally, we introduce a Gaussian
distribution model to uncover latent structural information and introduce a
loss function to reduce discrepancies between view embeddings. These two
strategies explore multi-view structural information and data distribution from
different perspectives, enhancing consistency across views and increasing
intra-cluster compactness. Experimental evaluations demonstrate the efficacy of
our method, showing significant improvements in clustering performance on
multiple benchmark datasets compared to state-of-the-art multi-view clustering
approaches.",['cs.CV'],http://arxiv.org/abs/2501.10157v1
Topology-Driven Attribute Recovery for Attribute Missing Graph Learning in Social Internet of Things,"With the advancement of information technology, the Social Internet of Things
(SIoT) has fostered the integration of physical devices and social networks,
deepening the study of complex interaction patterns. Text Attribute Graphs
(TAGs) capture both topological structures and semantic attributes, enhancing
the analysis of complex interactions within the SIoT. However, existing graph
learning methods are typically designed for complete attributed graphs, and the
common issue of missing attributes in Attribute Missing Graphs (AMGs) increases
the difficulty of analysis tasks. To address this, we propose the
Topology-Driven Attribute Recovery (TDAR) framework, which leverages
topological data for AMG learning. TDAR introduces an improved pre-filling
method for initial attribute recovery using native graph topology.
Additionally, it dynamically adjusts propagation weights and incorporates
homogeneity strategies within the embedding space to suit AMGs' unique
topological structures, effectively reducing noise during information
propagation. Extensive experiments on public datasets demonstrate that TDAR
significantly outperforms state-of-the-art methods in attribute reconstruction
and downstream tasks, offering a robust solution to the challenges posed by
AMGs. The code is available at https://github.com/limengran98/TDAR.",['cs.AI'],http://arxiv.org/abs/2501.10151v1
Spatio-temporal Graph Learning on Adaptive Mined Key Frames for High-performance Multi-Object Tracking,"In the realm of multi-object tracking, the challenge of accurately capturing
the spatial and temporal relationships between objects in video sequences
remains a significant hurdle. This is further complicated by frequent
occurrences of mutual occlusions among objects, which can lead to tracking
errors and reduced performance in existing methods. Motivated by these
challenges, we propose a novel adaptive key frame mining strategy that
addresses the limitations of current tracking approaches. Specifically, we
introduce a Key Frame Extraction (KFE) module that leverages reinforcement
learning to adaptively segment videos, thereby guiding the tracker to exploit
the intrinsic logic of the video content. This approach allows us to capture
structured spatial relationships between different objects as well as the
temporal relationships of objects across frames. To tackle the issue of object
occlusions, we have developed an Intra-Frame Feature Fusion (IFF) module.
Unlike traditional graph-based methods that primarily focus on inter-frame
feature fusion, our IFF module uses a Graph Convolutional Network (GCN) to
facilitate information exchange between the target and surrounding objects
within a frame. This innovation significantly enhances target
distinguishability and mitigates tracking loss and appearance similarity due to
occlusions. By combining the strengths of both long and short trajectories and
considering the spatial relationships between objects, our proposed tracker
achieves impressive results on the MOT17 dataset, i.e., 68.6 HOTA, 81.0 IDF1,
66.6 AssA, and 893 IDS, proving its effectiveness and accuracy.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.10129v1
Few-shot Structure-Informed Machinery Part Segmentation with Foundation Models and Graph Neural Networks,"This paper proposes a novel approach to few-shot semantic segmentation for
machinery with multiple parts that exhibit spatial and hierarchical
relationships. Our method integrates the foundation models CLIPSeg and Segment
Anything Model (SAM) with the interest point detector SuperPoint and a graph
convolutional network (GCN) to accurately segment machinery parts. By providing
1 to 25 annotated samples, our model, evaluated on a purely synthetic dataset
depicting a truck-mounted loading crane, achieves effective segmentation across
various levels of detail. Training times are kept under five minutes on
consumer GPUs. The model demonstrates robust generalization to real data,
achieving a qualitative synthetic-to-real generalization with a $J\&F$ score of
92.2 on real data using 10 synthetic support samples. When benchmarked on the
DAVIS 2017 dataset, it achieves a $J\&F$ score of 71.5 in semi-supervised video
segmentation with three support samples. This method's fast training times and
effective generalization to real data make it a valuable tool for autonomous
systems interacting with machinery and infrastructure, and illustrate the
potential of combined and orchestrated foundation models for few-shot
segmentation tasks.",['cs.CV'],http://arxiv.org/abs/2501.10080v1
Virtual Nodes Improve Long-term Traffic Prediction,"Effective traffic prediction is a cornerstone of intelligent transportation
systems, enabling precise forecasts of traffic flow, speed, and congestion.
While traditional spatio-temporal graph neural networks (ST-GNNs) have achieved
notable success in short-term traffic forecasting, their performance in
long-term predictions remains limited. This challenge arises from
over-squashing problem, where bottlenecks and limited receptive fields restrict
information flow and hinder the modeling of global dependencies. To address
these challenges, this study introduces a novel framework that incorporates
virtual nodes, which are additional nodes added to the graph and connected to
existing nodes, in order to aggregate information across the entire graph
within a single GNN layer. Our proposed model incorporates virtual nodes by
constructing a semi-adaptive adjacency matrix. This matrix integrates
distance-based and adaptive adjacency matrices, allowing the model to leverage
geographical information while also learning task-specific features from data.
Experimental results demonstrate that the inclusion of virtual nodes
significantly enhances long-term prediction accuracy while also improving
layer-wise sensitivity to mitigate the over-squashing problem. Virtual nodes
also offer enhanced explainability by focusing on key intersections and
high-traffic areas, as shown by the visualization of their adjacency matrix
weights on road network heat maps. Our advanced approach enhances the
understanding and management of urban traffic systems, making it particularly
well-suited for real-world applications.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2501.10048v1
Adaptive Spatiotemporal Augmentation for Improving Dynamic Graph Learning,"Dynamic graph augmentation is used to improve the performance of dynamic
GNNs. Most methods assume temporal locality, meaning that recent edges are more
influential than earlier edges. However, for temporal changes in edges caused
by random noise, overemphasizing recent edges while neglecting earlier ones may
lead to the model capturing noise. To address this issue, we propose STAA
(SpatioTemporal Activity-Aware Random Walk Diffusion). STAA identifies nodes
likely to have noisy edges in spatiotemporal dimensions. Spatially, it analyzes
critical topological positions through graph wavelet coefficients. Temporally,
it analyzes edge evolution through graph wavelet coefficient change rates.
Then, random walks are used to reduce the weights of noisy edges, deriving a
diffusion matrix containing spatiotemporal information as an augmented
adjacency matrix for dynamic GNN learning. Experiments on multiple datasets
show that STAA outperforms other dynamic graph augmentation methods in node
classification and link prediction tasks.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2501.10010v1
MoRe: Class Patch Attention Needs Regularization for Weakly Supervised Semantic Segmentation,"Weakly Supervised Semantic Segmentation (WSSS) with image-level labels
typically uses Class Activation Maps (CAM) to achieve dense predictions.
Recently, Vision Transformer (ViT) has provided an alternative to generate
localization maps from class-patch attention. However, due to insufficient
constraints on modeling such attention, we observe that the Localization
Attention Maps (LAM) often struggle with the artifact issue, i.e., patch
regions with minimal semantic relevance are falsely activated by class tokens.
In this work, we propose MoRe to address this issue and further explore the
potential of LAM. Our findings suggest that imposing additional regularization
on class-patch attention is necessary. To this end, we first view the attention
as a novel directed graph and propose the Graph Category Representation module
to implicitly regularize the interaction among class-patch entities. It ensures
that class tokens dynamically condense the related patch information and
suppress unrelated artifacts at a graph level. Second, motivated by the
observation that CAM from classification weights maintains smooth localization
of objects, we devise the Localization-informed Regularization module to
explicitly regularize the class-patch attention. It directly mines the token
relations from CAM and further supervises the consistency between class and
patch tokens in a learnable manner. Extensive experiments are conducted on
PASCAL VOC and MS COCO, validating that MoRe effectively addresses the artifact
issue and achieves state-of-the-art performance, surpassing recent single-stage
and even multi-stage methods. Code is available at
https://github.com/zwyang6/MoRe.",['cs.CV'],http://arxiv.org/abs/2412.11076v3
Study on a Fast Solver for Combined Field Integral Equations of 3D Conducting Bodies Based on Graph Neural Networks,"In this paper, we present a graph neural networks (GNNs)-based fast solver
(GraphSolver) for solving combined field integral equations (CFIEs) of 3D
conducting bodies. Rao-Wilton-Glisson (RWG) basis functions are employed to
discretely and accurately represent the geometry of 3D conducting bodies. A
concise and informative graph representation is then constructed by treating
each RWG function as a node in the graph, enabling the flow of current between
nodes. With the transformed graphs, GraphSolver is developed to directly
predict real and imaginary parts of the x, y and z components of the surface
current densities at each node (RWG function). Numerical results demonstrate
the efficacy of GraphSolver in solving CFIEs for 3D conducting bodies with
varying levels of geometric complexity, including basic 3D targets,
missile-shaped targets, and airplane-shaped targets.","['cs.LG', 'cs.AI', 'cs.NA', 'math.NA', '65M22', 'I.2']",http://arxiv.org/abs/2501.09923v1
MECD+: Unlocking Event-Level Causal Graph Discovery for Video Reasoning,"Video causal reasoning aims to achieve a high-level understanding of videos
from a causal perspective. However, it exhibits limitations in its scope,
primarily executed in a question-answering paradigm and focusing on brief video
segments containing isolated events and basic causal relations, lacking
comprehensive and structured causality analysis for videos with multiple
interconnected events. To fill this gap, we introduce a new task and dataset,
Multi-Event Causal Discovery (MECD). It aims to uncover the causal relations
between events distributed chronologically across long videos. Given visual
segments and textual descriptions of events, MECD identifies the causal
associations between these events to derive a comprehensive and structured
event-level video causal graph explaining why and how the result event
occurred. To address the challenges of MECD, we devise a novel framework
inspired by the Granger Causality method, incorporating an efficient mask-based
event prediction model to perform an Event Granger Test. It estimates causality
by comparing the predicted result event when premise events are masked versus
unmasked. Furthermore, we integrate causal inference techniques such as
front-door adjustment and counterfactual inference to mitigate challenges in
MECD like causality confounding and illusory causality. Additionally, context
chain reasoning is introduced to conduct more robust and generalized reasoning.
Experiments validate the effectiveness of our framework in reasoning complete
causal relations, outperforming GPT-4o and VideoChat2 by 5.77% and 2.70%,
respectively. Further experiments demonstrate that causal relation graphs can
also contribute to downstream video understanding tasks such as video question
answering and video event prediction.",['cs.CV'],http://arxiv.org/abs/2501.07227v3
LayerAnimate: Layer-specific Control for Animation,"Animated video separates foreground and background elements into layers, with
distinct processes for sketching, refining, coloring, and in-betweening.
Existing video generation methods typically treat animation as a monolithic
data domain, lacking fine-grained control over individual layers. In this
paper, we introduce LayerAnimate, a novel architectural approach that enhances
fine-grained control over individual animation layers within a video diffusion
model, allowing users to independently manipulate foreground and background
elements in distinct layers. To address the challenge of limited layer-specific
data, we propose a data curation pipeline that features automated element
segmentation, motion-state hierarchical merging, and motion coherence
refinement. Through quantitative and qualitative comparisons, and user study,
we demonstrate that LayerAnimate outperforms current methods in terms of
animation quality, control precision, and usability, making it an ideal tool
for both professional animators and amateur enthusiasts. This framework opens
up new possibilities for layer-specific animation applications and creative
flexibility. Our code is available at https://layeranimate.github.io.",['cs.CV'],http://arxiv.org/abs/2501.08295v2
VanGogh: A Unified Multimodal Diffusion-based Framework for Video Colorization,"Video colorization aims to transform grayscale videos into vivid color
representations while maintaining temporal consistency and structural
integrity. Existing video colorization methods often suffer from color bleeding
and lack comprehensive control, particularly under complex motion or diverse
semantic cues. To this end, we introduce VanGogh, a unified multimodal
diffusion-based framework for video colorization. VanGogh tackles these
challenges using a Dual Qformer to align and fuse features from multiple
modalities, complemented by a depth-guided generation process and an optical
flow loss, which help reduce color overflow. Additionally, a color injection
strategy and luma channel replacement are implemented to improve generalization
and mitigate flickering artifacts. Thanks to this design, users can exercise
both global and local control over the generation process, resulting in
higher-quality colorized videos. Extensive qualitative and quantitative
evaluations, and user studies, demonstrate that VanGogh achieves superior
temporal consistency and color fidelity.Project page:
https://becauseimbatman0.github.io/VanGogh.",['cs.CV'],http://arxiv.org/abs/2501.09499v1
Towards an End-to-End (E2E) Adversarial Learning and Application in the Physical World,"The traditional learning process of patch-based adversarial attacks,
conducted in the digital domain and then applied in the physical domain (e.g.,
via printed stickers), may suffer from reduced performance due to adversarial
patches' limited transferability from the digital domain to the physical
domain. Given that previous studies have considered using projectors to apply
adversarial attacks, we raise the following question: can adversarial learning
(i.e., patch generation) be performed entirely in the physical domain with a
projector? In this work, we propose the Physical-domain Adversarial Patch
Learning Augmentation (PAPLA) framework, a novel end-to-end (E2E) framework
that converts adversarial learning from the digital domain to the physical
domain using a projector. We evaluate PAPLA across multiple scenarios,
including controlled laboratory settings and realistic outdoor environments,
demonstrating its ability to ensure attack success compared to conventional
digital learning-physical application (DL-PA) methods. We also analyze the
impact of environmental factors, such as projection surface color, projector
strength, ambient light, distance, and angle of the target object relative to
the camera, on the effectiveness of projected patches. Finally, we demonstrate
the feasibility of the attack against a parked car and a stop sign in a
real-world outdoor environment. Our results show that under specific
conditions, E2E adversarial learning in the physical domain eliminates the
transferability issue and ensures evasion by object detectors. Finally, we
provide insights into the challenges and opportunities of applying adversarial
learning in the physical domain and explain where such an approach is more
effective than using a sticker.","['cs.CV', 'cs.CR']",http://arxiv.org/abs/2501.08258v2
OpticFusion: Multi-Modal Neural Implicit 3D Reconstruction of Microstructures by Fusing White Light Interferometry and Optical Microscopy,"White Light Interferometry (WLI) is a precise optical tool for measuring the
3D topography of microstructures. However, conventional WLI cannot capture the
natural color of a sample's surface, which is essential for many microscale
research applications that require both 3D geometry and color information.
Previous methods have attempted to overcome this limitation by modifying WLI
hardware and analysis software, but these solutions are often costly. In this
work, we address this challenge from a computer vision multi-modal
reconstruction perspective for the first time. We introduce OpticFusion, a
novel approach that uses an additional digital optical microscope (OM) to
achieve 3D reconstruction with natural color textures using multi-view WLI and
OM images. Our method employs a two-step data association process to obtain the
poses of WLI and OM data. By leveraging the neural implicit representation, we
fuse multi-modal data and apply color decomposition technology to extract the
sample's natural color. Tested on our multi-modal dataset of various microscale
samples, OpticFusion achieves detailed 3D reconstructions with color textures.
Our method provides an effective tool for practical applications across
numerous microscale research fields. The source code and our real-world dataset
are available at https://github.com/zju3dv/OpticFusion.","['cs.CV', 'physics.app-ph', 'physics.ins-det', 'physics.optics']",http://arxiv.org/abs/2501.09259v1
Unified Few-shot Crack Segmentation and its Precise 3D Automatic Measurement in Concrete Structures,"Visual-Spatial Systems has become increasingly essential in concrete crack
inspection. However, existing methods often lacks adaptability to diverse
scenarios, exhibits limited robustness in image-based approaches, and struggles
with curved or complex geometries. To address these limitations, an innovative
framework for two-dimensional (2D) crack detection, three-dimensional (3D)
reconstruction, and 3D automatic crack measurement was proposed by integrating
computer vision technologies and multi-modal Simultaneous localization and
mapping (SLAM) in this study. Firstly, building on a base DeepLabv3+
segmentation model, and incorporating specific refinements utilizing foundation
model Segment Anything Model (SAM), we developed a crack segmentation method
with strong generalization across unfamiliar scenarios, enabling the generation
of precise 2D crack masks. To enhance the accuracy and robustness of 3D
reconstruction, Light Detection and Ranging (LiDAR) point clouds were utilized
together with image data and segmentation masks. By leveraging both image- and
LiDAR-SLAM, we developed a multi-frame and multi-modal fusion framework that
produces dense, colorized point clouds, effectively capturing crack semantics
at a 3D real-world scale. Furthermore, the crack geometric attributions were
measured automatically and directly within 3D dense point cloud space,
surpassing the limitations of conventional 2D image-based measurements. This
advancement makes the method suitable for structural components with curved and
complex 3D geometries. Experimental results across various concrete structures
highlight the significant improvements and unique advantages of the proposed
method, demonstrating its effectiveness, accuracy, and robustness in real-world
applications.","['cs.CV', 'cs.RO']",http://arxiv.org/abs/2501.09203v1
RoHan: Robust Hand Detection in Operation Room,"Hand-specific localization has garnered significant interest within the
computer vision community. Although there are numerous datasets with hand
annotations from various angles and settings, domain transfer techniques
frequently struggle in surgical environments. This is mainly due to the limited
availability of gloved hand instances and the unique challenges of operating
rooms (ORs). Thus, hand-detection models tailored to OR settings require
extensive training and expensive annotation processes. To overcome these
challenges, we present ""RoHan"" - a novel approach for robust hand detection in
the OR, leveraging advanced semi-supervised domain adaptation techniques to
tackle the challenges of varying recording conditions, diverse glove colors,
and occlusions common in surgical settings. Our methodology encompasses two
main stages: (1) data augmentation strategy that utilizes ""Artificial Gloves,""
a method for augmenting publicly available hand datasets with synthetic images
of hands-wearing gloves; (2) semi-supervised domain adaptation pipeline that
improves detection performance in real-world OR settings through iterative
prediction refinement and efficient frame filtering. We evaluate our method
using two datasets: simulated enterotomy repair and saphenous vein graft
harvesting. ""RoHan"" substantially reduces the need for extensive labeling and
model training, paving the way for the practical implementation of hand
detection technologies in medical settings.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2501.08115v2
Make-A-Character 2: Animatable 3D Character Generation From a Single Image,"This report introduces Make-A-Character 2, an advanced system for generating
high-quality 3D characters from single portrait photographs, ideal for game
development and digital human applications. Make-A-Character 2 builds upon its
predecessor by incorporating several significant improvements for image-based
head generation. We utilize the IC-Light method to correct non-ideal
illumination in input photos and apply neural network-based color correction to
harmonize skin tones between the photos and game engine renders. We also employ
the Hierarchical Representation Network to capture high-frequency facial
structures and conduct adaptive skeleton calibration for accurate and
expressive facial animations. The entire image-to-3D-character generation
process takes less than 2 minutes. Furthermore, we leverage transformer
architecture to generate co-speech facial and gesture actions, enabling
real-time conversation with the generated character. These technologies have
been integrated into our conversational AI avatar products.",['cs.CV'],http://arxiv.org/abs/2501.07870v2
MangaNinja: Line Art Colorization with Precise Reference Following,"Derived from diffusion models, MangaNinjia specializes in the task of
reference-guided line art colorization. We incorporate two thoughtful designs
to ensure precise character detail transcription, including a patch shuffling
module to facilitate correspondence learning between the reference color image
and the target line art, and a point-driven control scheme to enable
fine-grained color matching. Experiments on a self-collected benchmark
demonstrate the superiority of our model over current solutions in terms of
precise colorization. We further showcase the potential of the proposed
interactive point control in handling challenging cases, cross-character
colorization, multi-reference harmonization, beyond the reach of existing
algorithms.",['cs.CV'],http://arxiv.org/abs/2501.08332v1
GAC-Net_Geometric and attention-based Network for Depth Completion,"Depth completion is a key task in autonomous driving, aiming to complete
sparse LiDAR depth measurements into high-quality dense depth maps through
image guidance. However, existing methods usually treat depth maps as an
additional channel of color images, or directly perform convolution on sparse
data, failing to fully exploit the 3D geometric information in depth maps,
especially with limited performance in complex boundaries and sparse areas. To
address these issues, this paper proposes a depth completion network combining
channel attention mechanism and 3D global feature perception (CGA-Net). The
main innovations include: 1) Utilizing PointNet++ to extract global 3D
geometric features from sparse depth maps, enhancing the scene perception
ability of low-line LiDAR data; 2) Designing a channel-attention-based
multimodal feature fusion module to efficiently integrate sparse depth, RGB
images, and 3D geometric features; 3) Combining residual learning with CSPN++
to optimize the depth refinement stage, further improving the completion
quality in edge areas and complex scenes. Experiments on the KITTI depth
completion dataset show that CGA-Net can significantly improve the prediction
accuracy of dense depth maps, achieving a new state-of-the-art (SOTA), and
demonstrating strong robustness to sparse and complex scenes.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.07988v1
Pre-trained Vision-Language Models Learn Discoverable Visual Concepts,"Do vision-language models (VLMs) pre-trained to caption an image of a
""durian"" learn visual concepts such as ""brown"" (color) and ""spiky"" (texture) at
the same time? We aim to answer this question as visual concepts learned ""for
free"" would enable wide applications such as neuro-symbolic reasoning or
human-interpretable object classification. We assume that the visual concepts,
if captured by pre-trained VLMs, can be extracted by their vision-language
interface with text-based concept prompts. We observe that recent works
prompting VLMs with concepts often differ in their strategies to define and
evaluate the visual concepts, leading to conflicting conclusions. We propose a
new concept definition strategy based on two observations: First, certain
concept prompts include shortcuts that recognize correct concepts for wrong
reasons; Second, multimodal information (e.g. visual discriminativeness, and
textual knowledge) should be leveraged when selecting the concepts. Our
proposed concept discovery and learning (CDL) framework is thus designed to
identify a diverse list of generic visual concepts (e.g. ""spiky"" as opposed to
""spiky durian""), which are ranked and selected based on visual and language
mutual information. We carefully design quantitative and human evaluations of
the discovered concepts on six diverse visual recognition datasets, which
confirm that pre-trained VLMs do learn visual concepts that provide accurate
and thorough descriptions for the recognized objects. All code and models are
publicly released.","['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG']",http://arxiv.org/abs/2404.12652v2
Zero-Shot Monocular Scene Flow Estimation in the Wild,"Large models have shown generalization across datasets for many low-level
vision tasks, like depth estimation, but no such general models exist for scene
flow. Even though scene flow has wide potential use, it is not used in practice
because current predictive models do not generalize well. We identify three key
challenges and propose solutions for each.First, we create a method that
jointly estimates geometry and motion for accurate prediction. Second, we
alleviate scene flow data scarcity with a data recipe that affords us 1M
annotated training samples across diverse synthetic scenes. Third, we evaluate
different parameterizations for scene flow prediction and adopt a natural and
effective parameterization. Our resulting model outperforms existing methods as
well as baselines built on large-scale models in terms of 3D end-point error,
and shows zero-shot generalization to the casually captured videos from DAVIS
and the robotic manipulation scenes from RoboTAP. Overall, our approach makes
scene flow prediction more practical in-the-wild.",['cs.CV'],http://arxiv.org/abs/2501.10357v1
Multi-Modal Attention Networks for Enhanced Segmentation and Depth Estimation of Subsurface Defects in Pulse Thermography,"AI-driven pulse thermography (PT) has become a crucial tool in
non-destructive testing (NDT), enabling automatic detection of hidden anomalies
in various industrial components. Current state-of-the-art techniques feed
segmentation and depth estimation networks compressed PT sequences using either
Principal Component Analysis (PCA) or Thermographic Signal Reconstruction
(TSR). However, treating these two modalities independently constrains the
performance of PT inspection models as these representations possess
complementary semantic features. To address this limitation, this work proposes
PT-Fusion, a multi-modal attention-based fusion network that fuses both PCA and
TSR modalities for defect segmentation and depth estimation of subsurface
defects in PT setups. PT-Fusion introduces novel feature fusion modules,
Encoder Attention Fusion Gate (EAFG) and Attention Enhanced Decoding Block
(AEDB), to fuse PCA and TSR features for enhanced segmentation and depth
estimation of subsurface defects. In addition, a novel data augmentation
technique is proposed based on random data sampling from thermographic
sequences to alleviate the scarcity of PT datasets. The proposed method is
benchmarked against state-of-the-art PT inspection models, including U-Net,
attention U-Net, and 3D-CNN on the Universit\'e Laval IRT-PVC dataset. The
results demonstrate that PT-Fusion outperforms the aforementioned models in
defect segmentation and depth estimation accuracies with a margin of 10%.","['cs.CV', 'cs.AI', 'eess.IV']",http://arxiv.org/abs/2501.09994v1
FoundationStereo: Zero-Shot Stereo Matching,"Tremendous progress has been made in deep stereo matching to excel on
benchmark datasets through per-domain fine-tuning. However, achieving strong
zero-shot generalization - a hallmark of foundation models in other computer
vision tasks - remains challenging for stereo matching. We introduce
FoundationStereo, a foundation model for stereo depth estimation designed to
achieve strong zero-shot generalization. To this end, we first construct a
large-scale (1M stereo pairs) synthetic training dataset featuring large
diversity and high photorealism, followed by an automatic self-curation
pipeline to remove ambiguous samples. We then design a number of network
architecture components to enhance scalability, including a side-tuning feature
backbone that adapts rich monocular priors from vision foundation models to
mitigate the sim-to-real gap, and long-range context reasoning for effective
cost volume filtering. Together, these components lead to strong robustness and
accuracy across domains, establishing a new standard in zero-shot stereo depth
estimation.","['cs.CV', 'cs.LG', 'cs.RO']",http://arxiv.org/abs/2501.09898v1
FutureDepth: Learning to Predict the Future Improves Video Depth Estimation,"In this paper, we propose a novel video depth estimation approach,
FutureDepth, which enables the model to implicitly leverage multi-frame and
motion cues to improve depth estimation by making it learn to predict the
future at training. More specifically, we propose a future prediction network,
F-Net, which takes the features of multiple consecutive frames and is trained
to predict multi-frame features one time step ahead iteratively. In this way,
F-Net learns the underlying motion and correspondence information, and we
incorporate its features into the depth decoding process. Additionally, to
enrich the learning of multiframe correspondence cues, we further leverage a
reconstruction network, R-Net, which is trained via adaptively masked
auto-encoding of multiframe feature volumes. At inference time, both F-Net and
R-Net are used to produce queries to work with the depth decoder, as well as a
final refinement network. Through extensive experiments on several benchmarks,
i.e., NYUDv2, KITTI, DDAD, and Sintel, which cover indoor, driving, and
open-domain scenarios, we show that FutureDepth significantly improves upon
baseline models, outperforms existing video depth estimation methods, and sets
new state-of-the-art (SOTA) accuracy. Furthermore, FutureDepth is more
efficient than existing SOTA video depth estimation models and has similar
latencies when comparing to monocular models",['cs.CV'],http://arxiv.org/abs/2403.12953v2
MAMo: Leveraging Memory and Attention for Monocular Video Depth Estimation,"We propose MAMo, a novel memory and attention frame-work for monocular video
depth estimation. MAMo can augment and improve any single-image depth
estimation networks into video depth estimation models, enabling them to take
advantage of the temporal information to predict more accurate depth. In MAMo,
we augment model with memory which aids the depth prediction as the model
streams through the video. Specifically, the memory stores learned visual and
displacement tokens of the previous time instances. This allows the depth
network to cross-reference relevant features from the past when predicting
depth on the current frame. We introduce a novel scheme to continuously update
the memory, optimizing it to keep tokens that correspond with both the past and
the present visual information. We adopt attention-based approach to process
memory features where we first learn the spatio-temporal relation among the
resultant visual and displacement memory tokens using self-attention module.
Further, the output features of self-attention are aggregated with the current
visual features through cross-attention. The cross-attended features are
finally given to a decoder to predict depth on the current frame. Through
extensive experiments on several benchmarks, including KITTI, NYU-Depth V2, and
DDAD, we show that MAMo consistently improves monocular depth estimation
networks and sets new state-of-the-art (SOTA) accuracy. Notably, our MAMo video
depth estimation provides higher accuracy with lower latency, when omparing to
SOTA cost-volume-based video depth models.",['cs.CV'],http://arxiv.org/abs/2307.14336v3
A Comparative Study on Multi-task Uncertainty Quantification in Semantic Segmentation and Monocular Depth Estimation,"Deep neural networks excel in perception tasks such as semantic segmentation
and monocular depth estimation, making them indispensable in safety-critical
applications like autonomous driving and industrial inspection. However, they
often suffer from overconfidence and poor explainability, especially for
out-of-domain data. While uncertainty quantification has emerged as a promising
solution to these challenges, multi-task settings have yet to be explored. In
an effort to shed light on this, we evaluate Monte Carlo Dropout, Deep
Sub-Ensembles, and Deep Ensembles for joint semantic segmentation and monocular
depth estimation. Thereby, we reveal that Deep Ensembles stand out as the
preferred choice, particularly in out-of-domain scenarios, and show the
potential benefit of multi-task learning with regard to the uncertainty quality
in comparison to solving both tasks separately. Additionally, we highlight the
impact of employing different uncertainty thresholds to classify pixels as
certain or uncertain, with the median uncertainty emerging as a robust default.","['cs.CV', 'cs.AI', 'cs.LG']",http://arxiv.org/abs/2405.17097v2
DEFOM-Stereo: Depth Foundation Model Based Stereo Matching,"Stereo matching is a key technique for metric depth estimation in computer
vision and robotics. Real-world challenges like occlusion and non-texture
hinder accurate disparity estimation from binocular matching cues. Recently,
monocular relative depth estimation has shown remarkable generalization using
vision foundation models. Thus, to facilitate robust stereo matching with
monocular depth cues, we incorporate a robust monocular relative depth model
into the recurrent stereo-matching framework, building a new framework for
depth foundation model-based stereo-matching, DEFOM-Stereo. In the feature
extraction stage, we construct the combined context and matching feature
encoder by integrating features from conventional CNNs and DEFOM. In the update
stage, we use the depth predicted by DEFOM to initialize the recurrent
disparity and introduce a scale update module to refine the disparity at the
correct scale. DEFOM-Stereo is verified to have comparable performance on the
Scene Flow dataset with state-of-the-art (SOTA) methods and notably shows much
stronger zero-shot generalization. Moreover, DEFOM-Stereo achieves SOTA
performance on the KITTI 2012, KITTI 2015, Middlebury, and ETH3D benchmarks,
ranking 1st on many metrics. In the joint evaluation under the robust vision
challenge, our model simultaneously outperforms previous models on the
individual benchmarks. Both results demonstrate the outstanding capabilities of
the proposed model.",['cs.CV'],http://arxiv.org/abs/2501.09466v1
MonSter: Marry Monodepth to Stereo Unleashes Power,"Stereo matching recovers depth from image correspondences. Existing methods
struggle to handle ill-posed regions with limited matching cues, such as
occlusions and textureless areas. To address this, we propose MonSter, a novel
method that leverages the complementary strengths of monocular depth estimation
and stereo matching. MonSter integrates monocular depth and stereo matching
into a dual-branch architecture to iteratively improve each other.
Confidence-based guidance adaptively selects reliable stereo cues for monodepth
scale-shift recovery. The refined monodepth is in turn guides stereo
effectively at ill-posed regions. Such iterative mutual enhancement enables
MonSter to evolve monodepth priors from coarse object-level structures to
pixel-level geometry, fully unlocking the potential of stereo matching. As
shown in Fig.1, MonSter ranks 1st across five most commonly used leaderboards
-- SceneFlow, KITTI 2012, KITTI 2015, Middlebury, and ETH3D. Achieving up to
49.5% improvements (Bad 1.0 on ETH3D) over the previous best method.
Comprehensive analysis verifies the effectiveness of MonSter in ill-posed
regions. In terms of zero-shot generalization, MonSter significantly and
consistently outperforms state-of-the-art across the board. The code is
publicly available at: https://github.com/Junda24/MonSter.",['cs.CV'],http://arxiv.org/abs/2501.08643v1
A Critical Synthesis of Uncertainty Quantification and Foundation Models in Monocular Depth Estimation,"While recent foundation models have enabled significant breakthroughs in
monocular depth estimation, a clear path towards safe and reliable deployment
in the real-world remains elusive. Metric depth estimation, which involves
predicting absolute distances, poses particular challenges, as even the most
advanced foundation models remain prone to critical errors. Since quantifying
the uncertainty has emerged as a promising endeavor to address these
limitations and enable trustworthy deployment, we fuse five different
uncertainty quantification methods with the current state-of-the-art
DepthAnythingV2 foundation model. To cover a wide range of metric depth
domains, we evaluate their performance on four diverse datasets. Our findings
identify fine-tuning with the Gaussian Negative Log-Likelihood Loss (GNLL) as a
particularly promising approach, offering reliable uncertainty estimates while
maintaining predictive performance and computational efficiency on par with the
baseline, encompassing both training and inference time. By fusing uncertainty
quantification and foundation models within the context of monocular depth
estimation, this paper lays a critical foundation for future research aimed at
improving not only model performance but also its explainability. Extending
this critical synthesis of uncertainty quantification and foundation models
into other crucial tasks, such as semantic segmentation and pose estimation,
presents exciting opportunities for safer and more reliable machine vision
systems.","['cs.CV', 'cs.AI', 'cs.LG']",http://arxiv.org/abs/2501.08188v1
Revisiting Birds Eye View Perception Models with Frozen Foundation Models: DINOv2 and Metric3Dv2,"Birds Eye View perception models require extensive data to perform and
generalize effectively. While traditional datasets often provide abundant
driving scenes from diverse locations, this is not always the case. It is
crucial to maximize the utility of the available training data. With the advent
of large foundation models such as DINOv2 and Metric3Dv2, a pertinent question
arises: can these models be integrated into existing model architectures to not
only reduce the required training data but surpass the performance of current
models? We choose two model architectures in the vehicle segmentation domain to
alter: Lift-Splat-Shoot, and Simple-BEV. For Lift-Splat-Shoot, we explore the
implementation of frozen DINOv2 for feature extraction and Metric3Dv2 for depth
estimation, where we greatly exceed the baseline results by 7.4 IoU while
utilizing only half the training data and iterations. Furthermore, we introduce
an innovative application of Metric3Dv2's depth information as a PseudoLiDAR
point cloud incorporated into the Simple-BEV architecture, replacing
traditional LiDAR. This integration results in a +3 IoU improvement compared to
the Camera-only model.",['cs.CV'],http://arxiv.org/abs/2501.08118v1
RGB-D Indiscernible Object Counting in Underwater Scenes,"Recently, indiscernible/camouflaged scene understanding has attracted lots of
research attention in the vision community. We further advance the frontier of
this field by systematically studying a new challenge named indiscernible
object counting (IOC), the goal of which is to count objects that are blended
with respect to their surroundings. Due to a lack of appropriate IOC datasets,
we present a large-scale dataset IOCfish5K which contains a total of 5,637
high-resolution images and 659,024 annotated center points. Our dataset
consists of a large number of indiscernible objects (mainly fish) in underwater
scenes, making the annotation process all the more challenging. IOCfish5K is
superior to existing datasets with indiscernible scenes because of its larger
scale, higher image resolutions, more annotations, and denser scenes. All these
aspects make it the most challenging dataset for IOC so far, supporting
progress in this area. Benefiting from the recent advancements of depth
estimation foundation models, we construct high-quality depth maps for
IOCfish5K by generating pseudo labels using the Depth Anything V2 model. The
RGB-D version of IOCfish5K is named IOCfish5K-D. For benchmarking purposes on
IOCfish5K, we select 14 mainstream methods for object counting and carefully
evaluate them. For multimodal IOCfish5K-D, we evaluate other 4 popular
multimodal counting methods. Furthermore, we propose IOCFormer, a new strong
baseline that combines density and regression branches in a unified framework
and can effectively tackle object counting under concealed scenes. We also
propose IOCFormer-D to enable the effective usage of depth modality in helping
detect and count objects hidden in their environments. Experiments show that
IOCFormer and IOCFormer-D achieve state-of-the-art scores on IOCfish5K and
IOCfish5K-D, respectively.",['cs.CV'],http://arxiv.org/abs/2304.11677v2
Matching Free Depth Recovery from Structured Light,"We present a novel approach for depth estimation from images captured by
structured light systems. Unlike many previous methods that rely on image
matching process, our approach uses a density voxel grid to represent scene
geometry, which is trained via self-supervised differentiable volume rendering.
Our method leverages color fields derived from projected patterns in structured
light systems during the rendering process, enabling the isolated optimization
of the geometry field. This contributes to faster convergence and high-quality
output. Additionally, we incorporate normalized device coordinates (NDC), a
distortion loss, and a novel surface-based color loss to enhance geometric
fidelity. Experimental results demonstrate that our method outperforms existing
matching-based techniques in geometric performance for few-shot scenarios,
achieving approximately a 60% reduction in average estimated depth errors on
synthetic scenes and about 30% on real-world captured scenes. Furthermore, our
approach delivers fast training, with a speed roughly three times faster than
previous matching-free methods that employ implicit representations.",['cs.CV'],http://arxiv.org/abs/2501.07113v1
ViPOcc: Leveraging Visual Priors from Vision Foundation Models for Single-View 3D Occupancy Prediction,"Inferring the 3D structure of a scene from a single image is an ill-posed and
challenging problem in the field of vision-centric autonomous driving. Existing
methods usually employ neural radiance fields to produce voxelized 3D
occupancy, lacking instance-level semantic reasoning and temporal photometric
consistency. In this paper, we propose ViPOcc, which leverages the visual
priors from vision foundation models (VFMs) for fine-grained 3D occupancy
prediction. Unlike previous works that solely employ volume rendering for RGB
and depth image reconstruction, we introduce a metric depth estimation branch,
in which an inverse depth alignment module is proposed to bridge the domain gap
in depth distribution between VFM predictions and the ground truth. The
recovered metric depth is then utilized in temporal photometric alignment and
spatial geometric alignment to ensure accurate and consistent 3D occupancy
prediction. Additionally, we also propose a semantic-guided non-overlapping
Gaussian mixture sampler for efficient, instance-aware ray sampling, which
addresses the redundant and imbalanced sampling issue that still exists in
previous state-of-the-art methods. Extensive experiments demonstrate the
superior performance of ViPOcc in both 3D occupancy prediction and depth
estimation tasks on the KITTI-360 and KITTI Raw datasets. Our code is available
at: \url{https://mias.group/ViPOcc}.",['cs.CV'],http://arxiv.org/abs/2412.11210v2
Relative Pose Estimation through Affine Corrections of Monocular Depth Priors,"Monocular depth estimation (MDE) models have undergone significant
advancements over recent years. Many MDE models aim to predict affine-invariant
relative depth from monocular images, while recent developments in large-scale
training and vision foundation models enable reasonable estimation of metric
(absolute) depth. However, effectively leveraging these predictions for
geometric vision tasks, in particular relative pose estimation, remains
relatively under explored. While depths provide rich constraints for cross-view
image alignment, the intrinsic noise and ambiguity from the monocular depth
priors present practical challenges to improving upon classic keypoint-based
solutions. In this paper, we develop three solvers for relative pose estimation
that explicitly account for independent affine (scale and shift) ambiguities,
covering both calibrated and uncalibrated conditions. We further propose a
hybrid estimation pipeline that combines our proposed solvers with classic
point-based solvers and epipolar constraints. We find that the affine
correction modeling is beneficial to not only the relative depth priors but
also, surprisingly, the ``metric"" ones. Results across multiple datasets
demonstrate large improvements of our approach over classic keypoint-based
baselines and PnP-based solutions, under both calibrated and uncalibrated
setups. We also show that our method improves consistently with different
feature matchers and MDE models, and can further benefit from very recent
advances on both modules. Code is available at
https://github.com/MarkYu98/madpose.",['cs.CV'],http://arxiv.org/abs/2501.05446v1
$DPF^*$: improved Depth Potential Function for scale-invariant sulcal depth estimation,"The shape of human brain is complex and highly variable, with interactions
between brain size, cortical folding, and age well-documented in the
literature. However, few studies have explored how global brain size influences
geometric features of the cortical surface derived from anatomical MRI. In this
work, we focus on sulcal depth, an imaging phenotype that has gained
significant attention in both basic research and clinical applications. We make
key contributions to the field by: 1) providing the first quantitative analysis
of how brain size affects sulcal depth measurements; 2) introducing a novel,
scale-invariant method for sulcal depth estimation based on an original
formalization of the problem; 3) presenting a validation framework and sharing
our code and benchmark data with the community; and 4) demonstrating the
biological relevance of our new sulcal depth measure using a large sample of
1,987 subjects spanning the developmental period from 26 weeks post-conception
to adulthood.",['cs.CV'],http://arxiv.org/abs/2501.05436v1
A Systematic Literature Review on Deep Learning-based Depth Estimation in Computer Vision,"Depth estimation (DE) provides spatial information about a scene and enables
tasks such as 3D reconstruction, object detection, and scene understanding.
Recently, there has been an increasing interest in using deep learning
(DL)-based methods for DE. Traditional techniques rely on handcrafted features
that often struggle to generalise to diverse scenes and require extensive
manual tuning. However, DL models for DE can automatically extract relevant
features from input data, adapt to various scene conditions, and generalise
well to unseen environments. Numerous DL-based methods have been developed,
making it necessary to survey and synthesize the state-of-the-art (SOTA).
Previous reviews on DE have mainly focused on either monocular or stereo-based
techniques, rather than comprehensively reviewing DE. Furthermore, to the best
of our knowledge, there is no systematic literature review (SLR) that
comprehensively focuses on DE. Therefore, this SLR study is being conducted.
Initially, electronic databases were searched for relevant publications,
resulting in 1284 publications. Using defined exclusion and quality criteria,
128 publications were shortlisted and further filtered to select 59
high-quality primary studies. These studies were analysed to extract data and
answer defined research questions. Based on the results, DL methods were
developed for mainly three different types of DE: monocular, stereo, and
multi-view. 20 publicly available datasets were used to train, test, and
evaluate DL models for DE, with KITTI, NYU Depth V2, and Make 3D being the most
used datasets. 29 evaluation metrics were used to assess the performance of DE.
35 base models were reported in the primary studies, and the top five most-used
base models were ResNet-50, ResNet-18, ResNet-101, U-Net, and VGG-16. Finally,
the lack of ground truth data was among the most significant challenges
reported by primary studies.","['cs.CV', 'cs.AI', 'cs.RO']",http://arxiv.org/abs/2501.05147v1
AuxDepthNet: Real-Time Monocular 3D Object Detection with Depth-Sensitive Features,"Monocular 3D object detection is a challenging task in autonomous systems due
to the lack of explicit depth information in single-view images. Existing
methods often depend on external depth estimators or expensive sensors, which
increase computational complexity and hinder real-time performance. To overcome
these limitations, we propose AuxDepthNet, an efficient framework for real-time
monocular 3D object detection that eliminates the reliance on external depth
maps or pre-trained depth models. AuxDepthNet introduces two key components:
the Auxiliary Depth Feature (ADF) module, which implicitly learns
depth-sensitive features to improve spatial reasoning and computational
efficiency, and the Depth Position Mapping (DPM) module, which embeds depth
positional information directly into the detection process to enable accurate
object localization and 3D bounding box regression. Leveraging the DepthFusion
Transformer architecture, AuxDepthNet globally integrates visual and
depth-sensitive features through depth-guided interactions, ensuring robust and
efficient detection. Extensive experiments on the KITTI dataset show that
AuxDepthNet achieves state-of-the-art performance, with $\text{AP}_{3D}$ scores
of 24.72\% (Easy), 18.63\% (Moderate), and 15.31\% (Hard), and
$\text{AP}_{\text{BEV}}$ scores of 34.11\% (Easy), 25.18\% (Moderate), and
21.90\% (Hard) at an IoU threshold of 0.7.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.03700v1
EndoOmni: Zero-Shot Cross-Dataset Depth Estimation in Endoscopy by Robust Self-Learning from Noisy Labels,"Single-image depth estimation is essential for endoscopy tasks such as
localization, reconstruction, and augmented reality. Most existing methods in
surgical scenes focus on in-domain depth estimation, limiting their real-world
applicability. This constraint stems from the scarcity and inferior labeling
quality of medical data for training. In this work, we present EndoOmni, the
first foundation model for zero-shot cross-domain depth estimation for
endoscopy. To harness the potential of diverse training data, we refine the
advanced self-learning paradigm that employs a teacher model to generate
pseudo-labels, guiding a student model trained on large-scale labeled and
unlabeled data. To address training disturbance caused by inherent noise in
depth labels, we propose a robust training framework that leverages both depth
labels and estimated confidence from the teacher model to jointly guide the
student model training. Moreover, we propose a weighted scale-and-shift
invariant loss to adaptively adjust learning weights based on label confidence,
thus imposing learning bias towards cleaner label pixels while reducing the
influence of highly noisy pixels. Experiments on zero-shot relative depth
estimation show that our EndoOmni improves state-of-the-art methods in medical
imaging for 33\% and existing foundation models for 34\% in terms of absolute
relative error on specific datasets. Furthermore, our model provides strong
initialization for fine-tuning metric depth estimation, maintaining superior
performance in both in-domain and out-of-domain scenarios. The source code is
publicly available at https://github.com/TianCuteQY/EndoOmni.",['cs.CV'],http://arxiv.org/abs/2409.05442v4
DepthMaster: Taming Diffusion Models for Monocular Depth Estimation,"Monocular depth estimation within the diffusion-denoising paradigm
demonstrates impressive generalization ability but suffers from low inference
speed. Recent methods adopt a single-step deterministic paradigm to improve
inference efficiency while maintaining comparable performance. However, they
overlook the gap between generative and discriminative features, leading to
suboptimal results. In this work, we propose DepthMaster, a single-step
diffusion model designed to adapt generative features for the discriminative
depth estimation task. First, to mitigate overfitting to texture details
introduced by generative features, we propose a Feature Alignment module, which
incorporates high-quality semantic features to enhance the denoising network's
representation capability. Second, to address the lack of fine-grained details
in the single-step deterministic framework, we propose a Fourier Enhancement
module to adaptively balance low-frequency structure and high-frequency
details. We adopt a two-stage training strategy to fully leverage the potential
of the two modules. In the first stage, we focus on learning the global scene
structure with the Feature Alignment module, while in the second stage, we
exploit the Fourier Enhancement module to improve the visual quality. Through
these efforts, our model achieves state-of-the-art performance in terms of
generalization and detail preservation, outperforming other diffusion-based
methods across various datasets. Our project page can be found at
https://indu1ge.github.io/DepthMaster_page.",['cs.CV'],http://arxiv.org/abs/2501.02576v1
Depth Any Camera: Zero-Shot Metric Depth Estimation from Any Camera,"While recent depth estimation methods exhibit strong zero-shot
generalization, achieving accurate metric depth across diverse camera
types-particularly those with large fields of view (FoV) such as fisheye and
360-degree cameras-remains a significant challenge. This paper presents Depth
Any Camera (DAC), a powerful zero-shot metric depth estimation framework that
extends a perspective-trained model to effectively handle cameras with varying
FoVs. The framework is designed to ensure that all existing 3D data can be
leveraged, regardless of the specific camera types used in new applications.
Remarkably, DAC is trained exclusively on perspective images but generalizes
seamlessly to fisheye and 360-degree cameras without the need for specialized
training data. DAC employs Equi-Rectangular Projection (ERP) as a unified image
representation, enabling consistent processing of images with diverse FoVs. Its
key components include a pitch-aware Image-to-ERP conversion for efficient
online augmentation in ERP space, a FoV alignment operation to support
effective training across a wide range of FoVs, and multi-resolution data
augmentation to address resolution disparities between training and testing.
DAC achieves state-of-the-art zero-shot metric depth estimation, improving
delta-1 ($\delta_1$) accuracy by up to 50% on multiple fisheye and 360-degree
datasets compared to prior metric depth foundation models, demonstrating robust
generalization across camera types.","['cs.CV', 'cs.AI', 'cs.RO']",http://arxiv.org/abs/2501.02464v1
SafeAug: Safety-Critical Driving Data Augmentation from Naturalistic Datasets,"Safety-critical driving data is crucial for developing safe and trustworthy
self-driving algorithms. Due to the scarcity of safety-critical data in
naturalistic datasets, current approaches primarily utilize simulated or
artificially generated images. However, there remains a gap in authenticity
between these generated images and naturalistic ones. We propose a novel
framework to augment the safety-critical driving data from the naturalistic
dataset to address this issue. In this framework, we first detect vehicles
using YOLOv5, followed by depth estimation and 3D transformation to simulate
vehicle proximity and critical driving scenarios better. This allows for
targeted modification of vehicle dynamics data to reflect potentially hazardous
situations. Compared to the simulated or artificially generated data, our
augmentation methods can generate safety-critical driving data with minimal
compromise on image authenticity. Experiments using KITTI datasets demonstrate
that a downstream self-driving algorithm trained on this augmented dataset
performs superiorly compared to the baselines, which include SMOGN and
importance sampling.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2501.02143v1
Metric3Dv2: A Versatile Monocular Geometric Foundation Model for Zero-shot Metric Depth and Surface Normal Estimation,"We introduce Metric3D v2, a geometric foundation model for zero-shot metric
depth and surface normal estimation from a single image, which is crucial for
metric 3D recovery. While depth and normal are geometrically related and highly
complimentary, they present distinct challenges. SoTA monocular depth methods
achieve zero-shot generalization by learning affine-invariant depths, which
cannot recover real-world metrics. Meanwhile, SoTA normal estimation methods
have limited zero-shot performance due to the lack of large-scale labeled data.
To tackle these issues, we propose solutions for both metric depth estimation
and surface normal estimation. For metric depth estimation, we show that the
key to a zero-shot single-view model lies in resolving the metric ambiguity
from various camera models and large-scale data training. We propose a
canonical camera space transformation module, which explicitly addresses the
ambiguity problem and can be effortlessly plugged into existing monocular
models. For surface normal estimation, we propose a joint depth-normal
optimization module to distill diverse data knowledge from metric depth,
enabling normal estimators to learn beyond normal labels. Equipped with these
modules, our depth-normal models can be stably trained with over 16 million of
images from thousands of camera models with different-type annotations,
resulting in zero-shot generalization to in-the-wild images with unseen camera
settings. Our method enables the accurate recovery of metric 3D structures on
randomly collected internet images, paving the way for plausible single-image
metrology. Our project page is at https://JUGGHM.github.io/Metric3Dv2.",['cs.CV'],http://arxiv.org/abs/2404.15506v4
IGAF: Incremental Guided Attention Fusion for Depth Super-Resolution,"Accurate depth estimation is crucial for many fields, including robotics,
navigation, and medical imaging. However, conventional depth sensors often
produce low-resolution (LR) depth maps, making detailed scene perception
challenging. To address this, enhancing LR depth maps to high-resolution (HR)
ones has become essential, guided by HR-structured inputs like RGB or grayscale
images. We propose a novel sensor fusion methodology for guided depth
super-resolution (GDSR), a technique that combines LR depth maps with HR images
to estimate detailed HR depth maps. Our key contribution is the Incremental
guided attention fusion (IGAF) module, which effectively learns to fuse
features from RGB images and LR depth maps, producing accurate HR depth maps.
Using IGAF, we build a robust super-resolution model and evaluate it on
multiple benchmark datasets. Our model achieves state-of-the-art results
compared to all baseline models on the NYU v2 dataset for $\times 4$, $\times
8$, and $\times 16$ upsampling. It also outperforms all baselines in a
zero-shot setting on the Middlebury, Lu, and RGB-D-D datasets. Code,
environments, and models are available on GitHub.",['cs.CV'],http://arxiv.org/abs/2501.01723v1
TexAVi: Generating Stereoscopic VR Video Clips from Text Descriptions,"While generative models such as text-to-image, large language models and
text-to-video have seen significant progress, the extension to
text-to-virtual-reality remains largely unexplored, due to a deficit in
training data and the complexity of achieving realistic depth and motion in
virtual environments. This paper proposes an approach to coalesce existing
generative systems to form a stereoscopic virtual reality video from text.
  Carried out in three main stages, we start with a base text-to-image model
that captures context from an input text. We then employ Stable Diffusion on
the rudimentary image produced, to generate frames with enhanced realism and
overall quality. These frames are processed with depth estimation algorithms to
create left-eye and right-eye views, which are stitched side-by-side to create
an immersive viewing experience. Such systems would be highly beneficial in
virtual reality production, since filming and scene building often require
extensive hours of work and post-production effort.
  We utilize image evaluation techniques, specifically Fr\'echet Inception
Distance and CLIP Score, to assess the visual quality of frames produced for
the video. These quantitative measures establish the proficiency of the
proposed method.
  Our work highlights the exciting possibilities of using natural
language-driven graphics in fields like virtual reality simulations.","['cs.CV', 'cs.AI', 'cs.LG', 'I.2']",http://arxiv.org/abs/2501.01156v1
PatchRefiner V2: Fast and Lightweight Real-Domain High-Resolution Metric Depth Estimation,"While current high-resolution depth estimation methods achieve strong
results, they often suffer from computational inefficiencies due to reliance on
heavyweight models and multiple inference steps, increasing inference time. To
address this, we introduce PatchRefiner V2 (PRV2), which replaces heavy refiner
models with lightweight encoders. This reduces model size and inference time
but introduces noisy features. To overcome this, we propose a Coarse-to-Fine
(C2F) module with a Guided Denoising Unit for refining and denoising the
refiner features and a Noisy Pretraining strategy to pretrain the refiner
branch to fully exploit the potential of the lightweight refiner branch.
Additionally, we introduce a Scale-and-Shift Invariant Gradient Matching
(SSIGM) loss to enhance synthetic-to-real domain transfer. PRV2 outperforms
state-of-the-art depth estimation methods on UnrealStereo4K in both accuracy
and speed, using fewer parameters and faster inference. It also shows improved
depth boundary delineation on real-world datasets like CityScape, ScanNet++,
and KITTI, demonstrating its versatility across domains.",['cs.CV'],http://arxiv.org/abs/2501.01121v1
FPGA-based Acceleration of Neural Network for Image Classification using Vitis AI,"In recent years, Convolutional Neural Networks (CNNs) have been widely
adopted in computer vision. Complex CNN architecture running on CPU or GPU has
either insufficient throughput or prohibitive power consumption. Hence, there
is a need to have dedicated hardware to accelerate the computation workload to
solve these limitations. In this paper, we accelerate a CNN for image
classification with the CIFAR-10 dataset using Vitis-AI on Xilinx Zynq
UltraScale+ MPSoC ZCU104 FPGA evaluation board. The work achieves 3.33-5.82x
higher throughput and 3.39-6.30x higher energy efficiency than CPU and GPU
baselines. It shows the potential to extract 2D features for downstream tasks,
such as depth estimation and 3D reconstruction.","['cs.CV', 'eess.IV']",http://arxiv.org/abs/2412.20974v1
AttEntropy: On the Generalization Ability of Supervised Semantic Segmentation Transformers to New Objects in New Domains,"In addition to impressive performance, vision transformers have demonstrated
remarkable abilities to encode information they were not trained to extract.
For example, this information can be used to perform segmentation or
single-view depth estimation even though the networks were only trained for
image recognition. We show that a similar phenomenon occurs when explicitly
training transformers for semantic segmentation in a supervised manner for a
set of categories: Once trained, they provide valuable information even about
categories absent from the training set. This information can be used to
segment objects from these never-seen-before classes in domains as varied as
road obstacles, aircraft parked at a terminal, lunar rocks, and maritime
hazards.","['cs.CV', 'I.4.6; I.4.8; I.5.4']",http://arxiv.org/abs/2212.14397v3
MetricDepth: Enhancing Monocular Depth Estimation with Deep Metric Learning,"Deep metric learning aims to learn features relying on the consistency or
divergence of class labels. However, in monocular depth estimation, the absence
of a natural definition of class poses challenges in the leveraging of deep
metric learning. Addressing this gap, this paper introduces MetricDepth, a
novel method that integrates deep metric learning to enhance the performance of
monocular depth estimation. To overcome the inapplicability of the class-based
sample identification in previous deep metric learning methods to monocular
depth estimation task, we design the differential-based sample identification.
This innovative approach identifies feature samples as different sample types
by their depth differentials relative to anchor, laying a foundation for
feature regularizing in monocular depth estimation models. Building upon this
advancement, we then address another critical problem caused by the vast range
and the continuity of depth annotations in monocular depth estimation. The
extensive and continuous annotations lead to the diverse differentials of
negative samples to anchor feature, representing the varied impact of negative
samples during feature regularizing. Recognizing the inadequacy of the uniform
strategy in previous deep metric learning methods for handling negative samples
in monocular depth estimation task, we propose the multi-range strategy.
Through further distinction on negative samples according to depth differential
ranges and implementation of diverse regularizing, our multi-range strategy
facilitates differentiated regularization interactions between anchor feature
and its negative samples. Experiments across various datasets and model types
demonstrate the effectiveness and versatility of MetricDepth,confirming its
potential for performance enhancement in monocular depth estimation task.",['cs.CV'],http://arxiv.org/abs/2412.20390v1
Multi-Modality Driven LoRA for Adverse Condition Depth Estimation,"The autonomous driving community is increasingly focused on addressing corner
case problems, particularly those related to ensuring driving safety under
adverse conditions (e.g., nighttime, fog, rain). To this end, the task of
Adverse Condition Depth Estimation (ACDE) has gained significant attention.
Previous approaches in ACDE have primarily relied on generative models, which
necessitate additional target images to convert the sunny condition into
adverse weather, or learnable parameters for feature augmentation to adapt
domain gaps, resulting in increased model complexity and tuning efforts.
Furthermore, unlike CLIP-based methods where textual and visual features have
been pre-aligned, depth estimation models lack sufficient alignment between
multimodal features, hindering coherent understanding under adverse conditions.
To address these limitations, we propose Multi-Modality Driven LoRA (MMD-LoRA),
which leverages low-rank adaptation matrices for efficient fine-tuning from
source-domain to target-domain. It consists of two core components: Prompt
Driven Domain Alignment (PDDA) and Visual-Text Consistent Contrastive
Learning(VTCCL). During PDDA, the image encoder with MMD-LoRA generates
target-domain visual representations, supervised by alignment loss that the
source-target difference between language and image should be equal. Meanwhile,
VTCCL bridges the gap between textual features from CLIP and visual features
from diffusion model, pushing apart different weather representations (vision
and text) and bringing together similar ones. Through extensive experiments,
the proposed method achieves state-of-the-art performance on the nuScenes and
Oxford RobotCar datasets, underscoring robustness and efficiency in adapting to
varied adverse environments.",['cs.CV'],http://arxiv.org/abs/2412.20162v1
DepthMamba with Adaptive Fusion,"Multi-view depth estimation has achieved impressive performance over various
benchmarks. However, almost all current multi-view systems rely on given ideal
camera poses, which are unavailable in many real-world scenarios, such as
autonomous driving. In this work, we propose a new robustness benchmark to
evaluate the depth estimation system under various noisy pose settings.
Surprisingly, we find current multi-view depth estimation methods or
single-view and multi-view fusion methods will fail when given noisy pose
settings. To tackle this challenge, we propose a two-branch network
architecture which fuses the depth estimation results of single-view and
multi-view branch. In specific, we introduced mamba to serve as feature
extraction backbone and propose an attention-based fusion methods which
adaptively select the most robust estimation results between the two branches.
Thus, the proposed method can perform well on some challenging scenes including
dynamic objects, texture-less regions, etc. Ablation studies prove the
effectiveness of the backbone and fusion method, while evaluation experiments
on challenging benchmarks (KITTI and DDAD) show that the proposed method
achieves a competitive performance compared to the state-of-the-art methods.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2412.19964v1
LiRCDepth: Lightweight Radar-Camera Depth Estimation via Knowledge Distillation and Uncertainty Guidance,"Recently, radar-camera fusion algorithms have gained significant attention as
radar sensors provide geometric information that complements the limitations of
cameras. However, most existing radar-camera depth estimation algorithms focus
solely on improving performance, often neglecting computational efficiency. To
address this gap, we propose LiRCDepth, a lightweight radar-camera depth
estimation model. We incorporate knowledge distillation to enhance the training
process, transferring critical information from a complex teacher model to our
lightweight student model in three key domains. Firstly, low-level and
high-level features are transferred by incorporating pixel-wise and pair-wise
distillation. Additionally, we introduce an uncertainty-aware inter-depth
distillation loss to refine intermediate depth maps during decoding. Leveraging
our proposed knowledge distillation scheme, the lightweight model achieves a
6.6% improvement in MAE on the nuScenes dataset compared to the model trained
without distillation. Code: https://github.com/harborsarah/LiRCDepth","['cs.CV', 'eess.IV']",http://arxiv.org/abs/2412.16380v2
An End-to-End Depth-Based Pipeline for Selfie Image Rectification,"Portraits or selfie images taken from a close distance typically suffer from
perspective distortion. In this paper, we propose an end-to-end deep
learning-based rectification pipeline to mitigate the effects of perspective
distortion. We learn to predict the facial depth by training a deep CNN. The
estimated depth is utilized to adjust the camera-to-subject distance by moving
the camera farther, increasing the camera focal length, and reprojecting the 3D
image features to the new perspective. The reprojected features are then fed to
an inpainting module to fill in the missing pixels. We leverage a
differentiable renderer to enable end-to-end training of our depth estimation
and feature extraction nets to improve the rectified outputs. To boost the
results of the inpainting module, we incorporate an auxiliary module to predict
the horizontal movement of the camera which decreases the area that requires
hallucination of challenging face parts such as ears. Unlike previous works, we
process the full-frame input image at once without cropping the subject's face
and processing it separately from the rest of the body, eliminating the need
for complex post-processing steps to attach the face back to the subject's
body. To train our network, we utilize the popular game engine Unreal Engine to
generate a large synthetic face dataset containing various subjects, head
poses, expressions, eyewear, clothes, and lighting. Quantitative and
qualitative results show that our rectification pipeline outperforms previous
methods, and produces comparable results with a time-consuming 3D GAN-based
method while being more than 260 times faster.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2412.19189v1
Revisiting Monocular 3D Object Detection from Scene-Level Depth Retargeting to Instance-Level Spatial Refinement,"Monocular 3D object detection is challenging due to the lack of accurate
depth. However, existing depth-assisted solutions still exhibit inferior
performance, whose reason is universally acknowledged as the unsatisfactory
accuracy of monocular depth estimation models. In this paper, we revisit
monocular 3D object detection from the depth perspective and formulate an
additional issue as the limited 3D structure-aware capability of existing depth
representations (\textit{e.g.}, depth one-hot encoding or depth distribution).
To address this issue, we propose a novel depth-adapted monocular 3D object
detection network, termed \textbf{RD3D}, that mainly comprises a Scene-Level
Depth Retargeting (SDR) module and an Instance-Level Spatial Refinement (ISR)
module. The former incorporates the scene-level perception of 3D structures,
retargeting traditional depth representations to a new formulation:
\textbf{Depth Thickness Field}. The latter refines the voxel spatial
representation with the guidance of instances, eliminating the ambiguity of 3D
occupation and thus improving detection accuracy. Extensive experiments on the
KITTI and Waymo datasets demonstrate our superiority to existing
state-of-the-art (SoTA) methods and the universality when equipped with
different depth estimation models. The code will be available.",['cs.CV'],http://arxiv.org/abs/2412.19165v1
MVS-GS: High-Quality 3D Gaussian Splatting Mapping via Online Multi-View Stereo,"This study addresses the challenge of online 3D model generation for neural
rendering using an RGB image stream. Previous research has tackled this issue
by incorporating Neural Radiance Fields (NeRF) or 3D Gaussian Splatting (3DGS)
as scene representations within dense SLAM methods. However, most studies focus
primarily on estimating coarse 3D scenes rather than achieving detailed
reconstructions. Moreover, depth estimation based solely on images is often
ambiguous, resulting in low-quality 3D models that lead to inaccurate
renderings. To overcome these limitations, we propose a novel framework for
high-quality 3DGS modeling that leverages an online multi-view stereo (MVS)
approach. Our method estimates MVS depth using sequential frames from a local
time window and applies comprehensive depth refinement techniques to filter out
outliers, enabling accurate initialization of Gaussians in 3DGS. Furthermore,
we introduce a parallelized backend module that optimizes the 3DGS model
efficiently, ensuring timely updates with each new keyframe. Experimental
results demonstrate that our method outperforms state-of-the-art dense SLAM
methods, particularly excelling in challenging outdoor environments.",['cs.CV'],http://arxiv.org/abs/2412.19130v1
Learning Monocular Depth from Events via Egomotion Compensation,"Event cameras are neuromorphically inspired sensors that sparsely and
asynchronously report brightness changes. Their unique characteristics of high
temporal resolution, high dynamic range, and low power consumption make them
well-suited for addressing challenges in monocular depth estimation (e.g.,
high-speed or low-lighting conditions). However, current existing methods
primarily treat event streams as black-box learning systems without
incorporating prior physical principles, thus becoming over-parameterized and
failing to fully exploit the rich temporal information inherent in event camera
data. To address this limitation, we incorporate physical motion principles to
propose an interpretable monocular depth estimation framework, where the
likelihood of various depth hypotheses is explicitly determined by the effect
of motion compensation. To achieve this, we propose a Focus Cost Discrimination
(FCD) module that measures the clarity of edges as an essential indicator of
focus level and integrates spatial surroundings to facilitate cost estimation.
Furthermore, we analyze the noise patterns within our framework and improve it
with the newly introduced Inter-Hypotheses Cost Aggregation (IHCA) module,
where the cost volume is refined through cost trend prediction and multi-scale
cost consistency constraints. Extensive experiments on real-world and synthetic
datasets demonstrate that our proposed framework outperforms cutting-edge
methods by up to 10\% in terms of the absolute relative error metric, revealing
superior performance in predicting accuracy.","['cs.CV', 'cs.LG', 'cs.RO']",http://arxiv.org/abs/2412.19067v1
HybridDepth: Robust Metric Depth Fusion by Leveraging Depth from Focus and Single-Image Priors,"We propose HYBRIDDEPTH, a robust depth estimation pipeline that addresses key
challenges in depth estimation,including scale ambiguity, hardware
heterogeneity, and generalizability. HYBRIDDEPTH leverages focal stack, data
conveniently accessible in common mobile devices, to produce accurate metric
depth maps. By incorporating depth priors afforded by recent advances in
singleimage depth estimation, our model achieves a higher level of structural
detail compared to existing methods. We test our pipeline as an end-to-end
system, with a newly developed mobile client to capture focal stacks, which are
then sent to a GPU-powered server for depth estimation. Comprehensive
quantitative and qualitative analyses demonstrate that HYBRIDDEPTH outperforms
state-of-the-art(SOTA) models on common datasets such as DDFF12 and NYU Depth
V2. HYBRIDDEPTH also shows strong zero-shot generalization. When trained on NYU
Depth V2, HYBRIDDEPTH surpasses SOTA models in zero-shot performance on
ARKitScenes and delivers more structurally accurate depth maps on Mobile Depth.
The code is available at https://github.com/cake-lab/HybridDepth/.",['cs.CV'],http://arxiv.org/abs/2407.18443v3
RSGaussian:3D Gaussian Splatting with LiDAR for Aerial Remote Sensing Novel View Synthesis,"This study presents RSGaussian, an innovative novel view synthesis (NVS)
method for aerial remote sensing scenes that incorporate LiDAR point cloud as
constraints into the 3D Gaussian Splatting method, which ensures that Gaussians
grow and split along geometric benchmarks, addressing the overgrowth and
floaters issues occurs. Additionally, the approach introduces coordinate
transformations with distortion parameters for camera models to achieve
pixel-level alignment between LiDAR point clouds and 2D images, facilitating
heterogeneous data fusion and achieving the high-precision geo-alignment
required in aerial remote sensing. Depth and plane consistency losses are
incorporated into the loss function to guide Gaussians towards real depth and
plane representations, significantly improving depth estimation accuracy.
Experimental results indicate that our approach has achieved novel view
synthesis that balances photo-realistic visual quality and high-precision
geometric estimation under aerial remote sensing datasets. Finally, we have
also established and open-sourced a dense LiDAR point cloud dataset along with
its corresponding aerial multi-view images, AIR-LONGYAN.","['cs.CV', 'cs.GR']",http://arxiv.org/abs/2412.18380v1
Revisiting 360 Depth Estimation with PanoGabor: A New Fusion Perspective,"Depth estimation from a monocular 360 image is important to the perception of
the entire 3D environment. However, the inherent distortion and large field of
view (FoV) in 360 images pose great challenges for this task. To this end,
existing mainstream solutions typically introduce additional perspective-based
360 representations (\textit{e.g.}, Cubemap) to achieve effective feature
extraction. Nevertheless, regardless of the introduced representations, they
eventually need to be unified into the equirectangular projection (ERP) format
for the subsequent depth estimation, which inevitably reintroduces the
troublesome distortions. In this work, we propose an oriented distortion-aware
Gabor Fusion framework (PGFuse) to address the above challenges. First, we
introduce Gabor filters that analyze texture in the frequency domain, thereby
extending the receptive fields and enhancing depth cues. To address the
reintroduced distortions, we design a linear latitude-aware distortion
representation method to generate customized, distortion-aware Gabor filters
(PanoGabor filters). Furthermore, we design a channel-wise and spatial-wise
unidirectional fusion module (CS-UFM) that integrates the proposed PanoGabor
filters to unify other representations into the ERP format, delivering
effective and distortion-free features. Considering the orientation sensitivity
of the Gabor transform, we introduce a spherical gradient constraint to
stabilize this sensitivity. Experimental results on three popular indoor 360
benchmarks demonstrate the superiority of the proposed PGFuse to existing
state-of-the-art solutions. Code can be available upon acceptance.",['cs.CV'],http://arxiv.org/abs/2408.16227v3
Flowing from Words to Pixels: A Framework for Cross-Modality Evolution,"Diffusion models, and their generalization, flow matching, have had a
remarkable impact on the field of media generation. Here, the conventional
approach is to learn the complex mapping from a simple source distribution of
Gaussian noise to the target media distribution. For cross-modal tasks such as
text-to-image generation, this same mapping from noise to image is learnt
whilst including a conditioning mechanism in the model. One key and thus far
relatively unexplored feature of flow matching is that, unlike Diffusion
models, they are not constrained for the source distribution to be noise.
Hence, in this paper, we propose a paradigm shift, and ask the question of
whether we can instead train flow matching models to learn a direct mapping
from the distribution of one modality to the distribution of another, thus
obviating the need for both the noise distribution and conditioning mechanism.
We present a general and simple framework, CrossFlow, for cross-modal flow
matching. We show the importance of applying Variational Encoders to the input
data, and introduce a method to enable Classifier-free guidance. Surprisingly,
for text-to-image, CrossFlow with a vanilla transformer without cross attention
slightly outperforms standard flow matching, and we show that it scales better
with training steps and model size, while also allowing for interesting latent
arithmetic which results in semantically meaningful edits in the output space.
To demonstrate the generalizability of our approach, we also show that
CrossFlow is on par with or outperforms the state-of-the-art for various
cross-modal / intra-modal mapping tasks, viz. image captioning, depth
estimation, and image super-resolution. We hope this paper contributes to
accelerating progress in cross-modal media generation.",['cs.CV'],http://arxiv.org/abs/2412.15213v1
Scaling 4D Representations,"Scaling has not yet been convincingly demonstrated for pure self-supervised
learning from video. However, prior work has focused evaluations on
semantic-related tasks $\unicode{x2013}$ action classification, ImageNet
classification, etc. In this paper we focus on evaluating self-supervised
learning on non-semantic vision tasks that are more spatial (3D) and temporal
(+1D = 4D), such as camera pose estimation, point and object tracking, and
depth estimation. We show that by learning from very large video datasets,
masked auto-encoding (MAE) with transformer video models actually scales,
consistently improving performance on these 4D tasks, as model size increases
from 20M all the way to the largest by far reported self-supervised video model
$\unicode{x2013}$ 22B parameters. Rigorous apples-to-apples comparison with
many recent image and video models demonstrates the benefits of scaling 4D
representations.","['cs.CV', 'cs.AI', 'cs.LG']",http://arxiv.org/abs/2412.15212v1
DepthFM: Fast Monocular Depth Estimation with Flow Matching,"Current discriminative depth estimation methods often produce blurry
artifacts, while generative approaches suffer from slow sampling due to
curvatures in the noise-to-depth transport. Our method addresses these
challenges by framing depth estimation as a direct transport between image and
depth distributions. We are the first to explore flow matching in this field,
and we demonstrate that its interpolation trajectories enhance both training
and sampling efficiency while preserving high performance. While generative
models typically require extensive training data, we mitigate this dependency
by integrating external knowledge from a pre-trained image diffusion model,
enabling effective transfer even across differing objectives. To further boost
our model performance, we employ synthetic data and utilize image-depth pairs
generated by a discriminative model on an in-the-wild image dataset. As a
generative model, our model can reliably estimate depth confidence, which
provides an additional advantage. Our approach achieves competitive zero-shot
performance on standard benchmarks of complex natural scenes while improving
sampling efficiency and only requiring minimal synthetic data for training.",['cs.CV'],http://arxiv.org/abs/2403.13788v2
MonoPCC: Photometric-invariant Cycle Constraint for Monocular Depth Estimation of Endoscopic Images,"Photometric constraint is indispensable for self-supervised monocular depth
estimation. It involves warping a source image onto a target view using
estimated depth&pose, and then minimizing the difference between the warped and
target images. However, the endoscopic built-in light causes significant
brightness fluctuations, and thus makes the photometric constraint unreliable.
Previous efforts only mitigate this relying on extra models to calibrate image
brightness. In this paper, we propose MonoPCC to address the brightness
inconsistency radically by reshaping the photometric constraint into a cycle
form. Instead of only warping the source image, MonoPCC constructs a closed
loop consisting of two opposite forward-backward warping paths: from target to
source and then back to target. Thus, the target image finally receives an
image cycle-warped from itself, which naturally makes the constraint invariant
to brightness changes. Moreover, MonoPCC transplants the source image's
phase-frequency into the intermediate warped image to avoid structure lost, and
also stabilizes the training via an exponential moving average (EMA) strategy
to avoid frequent changes in the forward warping. The comprehensive and
extensive experimental results on four endoscopic datasets demonstrate that our
proposed MonoPCC shows a great robustness to the brightness inconsistency, and
exceeds other state-of-the-arts by reducing the absolute relative error by at
least 7.27%, 9.38%, 9.90% and 3.17%, respectively.",['cs.CV'],http://arxiv.org/abs/2404.16571v4
Dyn-HaMR: Recovering 4D Interacting Hand Motion from a Dynamic Camera,"We propose Dyn-HaMR, to the best of our knowledge, the first approach to
reconstruct 4D global hand motion from monocular videos recorded by dynamic
cameras in the wild. Reconstructing accurate 3D hand meshes from monocular
videos is a crucial task for understanding human behaviour, with significant
applications in augmented and virtual reality (AR/VR). However, existing
methods for monocular hand reconstruction typically rely on a weak perspective
camera model, which simulates hand motion within a limited camera frustum. As a
result, these approaches struggle to recover the full 3D global trajectory and
often produce noisy or incorrect depth estimations, particularly when the video
is captured by dynamic or moving cameras, which is common in egocentric
scenarios. Our Dyn-HaMR consists of a multi-stage, multi-objective optimization
pipeline, that factors in (i) simultaneous localization and mapping (SLAM) to
robustly estimate relative camera motion, (ii) an interacting-hand prior for
generative infilling and to refine the interaction dynamics, ensuring plausible
recovery under (self-)occlusions, and (iii) hierarchical initialization through
a combination of state-of-the-art hand tracking methods. Through extensive
evaluations on both in-the-wild and indoor datasets, we show that our approach
significantly outperforms state-of-the-art methods in terms of 4D global mesh
recovery. This establishes a new benchmark for hand motion reconstruction from
monocular video with moving cameras. Our project page is at
https://dyn-hamr.github.io/.",['cs.CV'],http://arxiv.org/abs/2412.12861v2
Foundation Models Meet Low-Cost Sensors: Test-Time Adaptation for Rescaling Disparity for Zero-Shot Metric Depth Estimation,"The recent development of foundation models for monocular depth estimation
such as Depth Anything paved the way to zero-shot monocular depth estimation.
Since it returns an affine-invariant disparity map, the favored technique to
recover the metric depth consists in fine-tuning the model. However, this stage
is costly to perform because of the training but also due to the creation of
the dataset. It must contain images captured by the camera that will be used at
test time and the corresponding ground truth. Moreover, the fine-tuning may
also degrade the generalizing capacity of the original model. Instead, we
propose in this paper a new method to rescale Depth Anything predictions using
3D points provided by low-cost sensors or techniques such as low-resolution
LiDAR, stereo camera, structure-from-motion where poses are given by an IMU.
Thus, this approach avoids fine-tuning and preserves the generalizing power of
the original depth estimation model while being robust to the noise of the
sensor or of the depth model. Our experiments highlight improvements relative
to other metric depth estimation methods and competitive results compared to
fine-tuned approaches. Code available at
https://gitlab.ensta.fr/ssh/monocular-depth-rescaling.",['cs.CV'],http://arxiv.org/abs/2412.14103v1
CLIP-PCQA: Exploring Subjective-Aligned Vision-Language Modeling for Point Cloud Quality Assessment,"In recent years, No-Reference Point Cloud Quality Assessment (NR-PCQA)
research has achieved significant progress. However, existing methods mostly
seek a direct mapping function from visual data to the Mean Opinion Score
(MOS), which is contradictory to the mechanism of practical subjective
evaluation. To address this, we propose a novel language-driven PCQA method
named CLIP-PCQA. Considering that human beings prefer to describe visual
quality using discrete quality descriptions (e.g., ""excellent"" and ""poor"")
rather than specific scores, we adopt a retrieval-based mapping strategy to
simulate the process of subjective assessment. More specifically, based on the
philosophy of CLIP, we calculate the cosine similarity between the visual
features and multiple textual features corresponding to different quality
descriptions, in which process an effective contrastive loss and learnable
prompts are introduced to enhance the feature extraction. Meanwhile, given the
personal limitations and bias in subjective experiments, we further covert the
feature similarities into probabilities and consider the Opinion Score
Distribution (OSD) rather than a single MOS as the final target. Experimental
results show that our CLIP-PCQA outperforms other State-Of-The-Art (SOTA)
approaches.","['cs.CV', 'cs.MM']",http://arxiv.org/abs/2501.10071v1
DehazeGS: Seeing Through Fog with 3D Gaussian Splatting,"Current novel view synthesis tasks primarily rely on high-quality and clear
images. However, in foggy scenes, scattering and attenuation can significantly
degrade the reconstruction and rendering quality. Although NeRF-based dehazing
reconstruction algorithms have been developed, their use of deep fully
connected neural networks and per-ray sampling strategies leads to high
computational costs. Moreover, NeRF's implicit representation struggles to
recover fine details from hazy scenes. In contrast, recent advancements in 3D
Gaussian Splatting achieve high-quality 3D scene reconstruction by explicitly
modeling point clouds into 3D Gaussians. In this paper, we propose leveraging
the explicit Gaussian representation to explain the foggy image formation
process through a physically accurate forward rendering process. We introduce
DehazeGS, a method capable of decomposing and rendering a fog-free background
from participating media using only muti-view foggy images as input. We model
the transmission within each Gaussian distribution to simulate the formation of
fog. During this process, we jointly learn the atmospheric light and scattering
coefficient while optimizing the Gaussian representation of the hazy scene. In
the inference stage, we eliminate the effects of scattering and attenuation on
the Gaussians and directly project them onto a 2D plane to obtain a clear view.
Experiments on both synthetic and real-world foggy datasets demonstrate that
DehazeGS achieves state-of-the-art performance in terms of both rendering
quality and computational efficiency. visualizations are available at
https://dehazegs.github.io/",['cs.CV'],http://arxiv.org/abs/2501.03659v3
Point-PRC: A Prompt Learning Based Regulation Framework for Generalizable Point Cloud Analysis,"This paper investigates the 3D domain generalization (3DDG) ability of large
3D models based on prevalent prompt learning. Recent works demonstrate the
performances of 3D point cloud recognition can be boosted remarkably by
parameter-efficient prompt tuning. However, we observe that the improvement on
downstream tasks comes at the expense of a severe drop in 3D domain
generalization. To resolve this challenge, we present a comprehensive
regulation framework that allows the learnable prompts to actively interact
with the well-learned general knowledge in large 3D models to maintain good
generalization. Specifically, the proposed framework imposes multiple explicit
constraints on the prompt learning trajectory by maximizing the mutual
agreement between task-specific predictions and task-agnostic knowledge. We
design the regulation framework as a plug-and-play module to embed into
existing representative large 3D models. Surprisingly, our method not only
realizes consistently increasing generalization ability but also enhances
task-specific 3D recognition performances across various 3DDG benchmarks by a
clear margin. Considering the lack of study and evaluation on 3DDG, we also
create three new benchmarks, namely base-to-new, cross-dataset and few-shot
generalization benchmarks, to enrich the field and inspire future research.
Code and benchmarks are available at
\url{https://github.com/auniquesun/Point-PRC}.",['cs.CV'],http://arxiv.org/abs/2410.20406v3
SA-MLP: A Low-Power Multiplication-Free Deep Network for 3D Point Cloud Classification in Resource-Constrained Environments,"Point cloud classification plays a crucial role in the processing and
analysis of data from 3D sensors such as LiDAR, which are commonly used in
applications like autonomous vehicles, robotics, and environmental monitoring.
However, traditional neural networks, which rely heavily on multiplication
operations, often face challenges in terms of high computational costs and
energy consumption. This study presents a novel family of efficient MLP-based
architectures designed to improve the computational efficiency of point cloud
classification tasks in sensor systems. The baseline model, Mul-MLP, utilizes
conventional multiplication operations, while Add-MLP and Shift-MLP replace
multiplications with addition and shift operations, respectively. These
replacements leverage more sensor-friendly operations that can significantly
reduce computational overhead, making them particularly suitable for
resource-constrained sensor platforms. To further enhance performance, we
propose SA-MLP, a hybrid architecture that alternates between shift and adder
layers, preserving the network depth while optimizing computational efficiency.
Unlike previous approaches such as ShiftAddNet, which increase the layer count
and limit representational capacity by freezing shift weights, SA-MLP fully
exploits the complementary advantages of shift and adder layers by employing
distinct learning rates and optimizers. Experimental results show that Add-MLP
and Shift-MLP achieve competitive performance compared to Mul-MLP, while SA-MLP
surpasses the baseline, delivering results comparable to state-of-the-art MLP
models in terms of both classification accuracy and computational efficiency.
This work offers a promising, energy-efficient solution for sensor-driven
applications requiring real-time point cloud classification, particularly in
environments with limited computational resources.",['cs.CV'],http://arxiv.org/abs/2409.01998v2
Enhancing Performance of Point Cloud Completion Networks with Consistency Loss,"Point cloud completion networks are conventionally trained to minimize the
disparities between the completed point cloud and the ground-truth counterpart.
However, an incomplete object-level point cloud can have multiple valid
completion solutions when it is examined in isolation. This one-to-many mapping
issue can cause contradictory supervision signals to the network because the
loss function may produce different values for identical input-output pairs of
the network. In many cases, this issue could adversely affect the network
optimization process. In this work, we propose to enhance the conventional
learning objective using a novel completion consistency loss to mitigate the
one-to-many mapping problem. Specifically, the proposed consistency loss ensure
that a point cloud completion network generates a coherent completion solution
for incomplete objects originating from the same source point cloud.
Experimental results across multiple well-established datasets and benchmarks
demonstrated the proposed completion consistency loss have excellent capability
to enhance the completion performance of various existing networks without any
modification to the design of the networks. The proposed consistency loss
enhances the performance of the point completion network without affecting the
inference speed, thereby increasing the accuracy of point cloud completion.
Notably, a state-of-the-art point completion network trained with the proposed
consistency loss can achieve state-of-the-art accuracy on the challenging new
MVP dataset. The code and result of experiment various point completion models
using proposed consistency loss will be available at:
https://github.com/kaist-avelab/ConsistencyLoss .","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2410.07298v3
SplatMAP: Online Dense Monocular SLAM with 3D Gaussian Splatting,"Achieving high-fidelity 3D reconstruction from monocular video remains
challenging due to the inherent limitations of traditional methods like
Structure-from-Motion (SfM) and monocular SLAM in accurately capturing scene
details. While differentiable rendering techniques such as Neural Radiance
Fields (NeRF) address some of these challenges, their high computational costs
make them unsuitable for real-time applications. Additionally, existing 3D
Gaussian Splatting (3DGS) methods often focus on photometric consistency,
neglecting geometric accuracy and failing to exploit SLAM's dynamic depth and
pose updates for scene refinement. We propose a framework integrating dense
SLAM with 3DGS for real-time, high-fidelity dense reconstruction. Our approach
introduces SLAM-Informed Adaptive Densification, which dynamically updates and
densifies the Gaussian model by leveraging dense point clouds from SLAM.
Additionally, we incorporate Geometry-Guided Optimization, which combines
edge-aware geometric constraints and photometric consistency to jointly
optimize the appearance and geometry of the 3DGS scene representation, enabling
detailed and accurate SLAM mapping reconstruction. Experiments on the Replica
and TUM-RGBD datasets demonstrate the effectiveness of our approach, achieving
state-of-the-art results among monocular systems. Specifically, our method
achieves a PSNR of 36.864, SSIM of 0.985, and LPIPS of 0.040 on Replica,
representing improvements of 10.7%, 6.4%, and 49.4%, respectively, over the
previous SOTA. On TUM-RGBD, our method outperforms the closest baseline by
10.2%, 6.6%, and 34.7% in the same metrics. These results highlight the
potential of our framework in bridging the gap between photometric and
geometric dense 3D scene representations, paving the way for practical and
efficient monocular dense reconstruction.","['cs.CV', 'I.4']",http://arxiv.org/abs/2501.07015v2
Automated Detection and Analysis of Minor Deformations in Flat Walls Due to Railway Vibrations Using LiDAR and Machine Learning,"This study introduces an advanced methodology for automatically identifying
minor deformations in flat walls caused by vibrations from nearby railway
tracks. It leverages high-density Terrestrial Laser Scanner (TLS) LiDAR surveys
and AI/ML techniques to collect and analyze data. The scan data is processed
into a detailed point cloud, which is segmented to distinguish ground points,
trees, buildings, and other objects. The analysis focuses on identifying
sections along flat walls and estimating their deformations relative to the
ground orientation.
  Findings from the study, conducted at the RGIPT campus, reveal significant
deformations in walls close to the railway corridor, with the highest
deformations ranging from 7 to 8 cm and an average of 3 to 4 cm. In contrast,
walls further from the corridor show negligible deformations. The developed
automated process for feature extraction and deformation monitoring
demonstrates potential for structural health monitoring. By integrating LiDAR
data with machine learning, the methodology provides an efficient system for
identifying and analyzing structural deformations, highlighting the importance
of continuous monitoring for ensuring structural integrity and public safety in
urban infrastructure. This approach represents a substantial advancement in
automated feature extraction and deformation analysis, contributing to more
effective management of urban infrastructure.",['cs.LG'],http://arxiv.org/abs/2501.06457v2
Synthesis and Analysis of Data as Probability Measures with Entropy-Regularized Optimal Transport,"We consider synthesis and analysis of probability measures using the
entropy-regularized Wasserstein-2 cost and its unbiased version, the Sinkhorn
divergence. The synthesis problem consists of computing the barycenter, with
respect to these costs, of $m$ reference measures given a set of coefficients
belonging to the $m$-dimensional simplex. The analysis problem consists of
finding the coefficients for the closest barycenter in the Wasserstein-2
distance to a given measure $\mu$. Under the weakest assumptions on the
measures thus far in the literature, we compute the derivative of the
entropy-regularized Wasserstein-2 cost. We leverage this to establish a
characterization of regularized barycenters as solutions to a fixed-point
equation for the average of the entropic maps from the barycenter to the
reference measures. This characterization yields a finite-dimensional, convex,
quadratic program for solving the analysis problem when $\mu$ is a barycenter.
It is shown that these coordinates, as well as the value of the barycenter
functional, can be estimated from samples with dimension-independent rates of
convergence, a hallmark of entropy-regularized optimal transport, and we verify
these rates experimentally. We also establish that barycentric coordinates are
stable with respect to perturbations in the Wasserstein-2 metric, suggesting a
robustness of these coefficients to corruptions. We employ the barycentric
coefficients as features for classification of corrupted point cloud data, and
show that compared to neural network baselines, our approach is more efficient
in small training data regimes.","['stat.ML', 'cs.LG']",http://arxiv.org/abs/2501.07446v2
3UR-LLM: An End-to-End Multimodal Large Language Model for 3D Scene Understanding,"Multi-modal Large Language Models (MLLMs) exhibit impressive capabilities in
2D tasks, yet encounter challenges in discerning the spatial positions,
interrelations, and causal logic in scenes when transitioning from 2D to 3D
representations. We find that the limitations mainly lie in: i) the high
annotation cost restricting the scale-up of volumes of 3D scene data, and ii)
the lack of a straightforward and effective way to perceive 3D information
which results in prolonged training durations and complicates the streamlined
framework. To this end, we develop pipeline based on open-source 2D MLLMs and
LLMs to generate high-quality 3D-text pairs and construct 3DS-160K , to enhance
the pre-training process. Leveraging this high-quality pre-training data, we
introduce the 3UR-LLM model, an end-to-end 3D MLLM designed for precise
interpretation of 3D scenes, showcasing exceptional capability in navigating
the complexities of the physical world. 3UR-LLM directly receives 3D point
cloud as input and project 3D features fused with text instructions into a
manageable set of tokens. Considering the computation burden derived from these
hybrid tokens, we design a 3D compressor module to cohesively compress the 3D
spatial cues and textual narrative. 3UR-LLM achieves promising performance with
respect to the previous SOTAs, for instance, 3UR-LLM exceeds its counterparts
by 7.1\% CIDEr on ScanQA, while utilizing fewer training resources. The code
and model weights for 3UR-LLM and the 3DS-160K benchmark are available at
3UR-LLM.",['cs.CV'],http://arxiv.org/abs/2501.07819v1
PSReg: Prior-guided Sparse Mixture of Experts for Point Cloud Registration,"The discriminative feature is crucial for point cloud registration. Recent
methods improve the feature discriminative by distinguishing between
non-overlapping and overlapping region points. However, they still face
challenges in distinguishing the ambiguous structures in the overlapping
regions. Therefore, the ambiguous features they extracted resulted in a
significant number of outlier matches from overlapping regions. To solve this
problem, we propose a prior-guided SMoE-based registration method to improve
the feature distinctiveness by dispatching the potential correspondences to the
same experts. Specifically, we propose a prior-guided SMoE module by fusing
prior overlap and potential correspondence embeddings for routing, assigning
tokens to the most suitable experts for processing. In addition, we propose a
registration framework by a specific combination of Transformer layer and
prior-guided SMoE module. The proposed method not only pays attention to the
importance of locating the overlapping areas of point clouds, but also commits
to finding more accurate correspondences in overlapping areas. Our extensive
experiments demonstrate the effectiveness of our method, achieving
state-of-the-art registration recall (95.7\%/79.3\%) on the 3DMatch/3DLoMatch
benchmark. Moreover, we also test the performance on ModelNet40 and demonstrate
excellent performance.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.07762v1
Extracting Manifold Information from Point Clouds,"A kernel based method is proposed for the construction of signature
(defining) functions of subsets of $\mathbb{R}^d$. The subsets can range from
full dimensional manifolds (open subsets) to point clouds (a finite number of
points) and include bounded smooth manifolds of any codimension. The
interpolation and analysis of point clouds are the main application. Two
extreme cases in terms of regularity are considered, where the data set is
interpolated by an analytic surface, at the one extreme, and by a H\""older
continuous surface, at the other. The signature function can be computed as a
linear combination of translated kernels, the coefficients of which are the
solution of a finite dimensional linear problem. Once it is obtained, it can be
used to estimate the dimension as well as the normal and the curvatures of the
interpolated surface. The method is global and does not require explicit
knowledge of local neighborhoods or any other structure present in the data
set. It admits a variational formulation with a natural ``regularized''
counterpart, that proves to be useful in dealing with data sets corrupted by
numerical error or noise. The underlying analytical structure of the approach
is presented in general before it is applied to the case of point clouds.","['cs.CV', 'cs.CG', 'cs.NA', 'math.NA', '65D05, 65D10, 65D18, 68T10, 68U05', 'G.1.1; I.2.10; I.3.5; I.5.1']",http://arxiv.org/abs/2404.00427v2
UnCommon Objects in 3D,"We introduce Uncommon Objects in 3D (uCO3D), a new object-centric dataset for
3D deep learning and 3D generative AI. uCO3D is the largest publicly-available
collection of high-resolution videos of objects with 3D annotations that
ensures full-360$^{\circ}$ coverage. uCO3D is significantly more diverse than
MVImgNet and CO3Dv2, covering more than 1,000 object categories. It is also of
higher quality, due to extensive quality checks of both the collected videos
and the 3D annotations. Similar to analogous datasets, uCO3D contains
annotations for 3D camera poses, depth maps and sparse point clouds. In
addition, each object is equipped with a caption and a 3D Gaussian Splat
reconstruction. We train several large 3D models on MVImgNet, CO3Dv2, and uCO3D
and obtain superior results using the latter, showing that uCO3D is better for
learning applications.","['cs.CV', 'cs.AI', 'cs.GR']",http://arxiv.org/abs/2501.07574v1
Point-JEPA: A Joint Embedding Predictive Architecture for Self-Supervised Learning on Point Cloud,"Recent advancements in self-supervised learning in the point cloud domain
have demonstrated significant potential. However, these methods often suffer
from drawbacks, including lengthy pre-training time, the necessity of
reconstruction in the input space, or the necessity of additional modalities.
In order to address these issues, we introduce Point-JEPA, a joint embedding
predictive architecture designed specifically for point cloud data. To this
end, we introduce a sequencer that orders point cloud patch embeddings to
efficiently compute and utilize their proximity based on the indices during
target and context selection. The sequencer also allows shared computations of
the patch embeddings' proximity between context and target selection, further
improving the efficiency. Experimentally, our method achieves competitive
results with state-of-the-art methods while avoiding the reconstruction in the
input space or additional modality.",['cs.CV'],http://arxiv.org/abs/2404.16432v5
Robust Single Object Tracking in LiDAR Point Clouds under Adverse Weather Conditions,"3D single object tracking (3DSOT) in LiDAR point clouds is a critical task
for outdoor perception, enabling real-time perception of object location,
orientation, and motion. Despite the impressive performance of current 3DSOT
methods, evaluating them on clean datasets inadequately reflects their
comprehensive performance, as the adverse weather conditions in real-world
surroundings has not been considered. One of the main obstacles is the lack of
adverse weather benchmarks for the evaluation of 3DSOT. To this end, this work
proposes a challenging benchmark for LiDAR-based 3DSOT in adverse weather,
which comprises two synthetic datasets (KITTI-A and nuScenes-A) and one
real-world dataset (CADC-SOT) spanning three weather types: rain, fog, and
snow. Based on this benchmark, five representative 3D trackers from different
tracking frameworks conducted robustness evaluation, resulting in significant
performance degradations. This prompts the question: What are the factors that
cause current advanced methods to fail on such adverse weather samples?
Consequently, we explore the impacts of adverse weather and answer the above
question from three perspectives: 1) target distance; 2) template shape
corruption; and 3) target shape corruption. Finally, based on domain
randomization and contrastive learning, we designed a dual-branch tracking
framework for adverse weather, named DRCT, achieving excellent performance in
benchmarks.",['cs.CV'],http://arxiv.org/abs/2501.07133v1
Pamba: Enhancing Global Interaction in Point Clouds via State Space Model,"Transformers have demonstrated impressive results for 3D point cloud semantic
segmentation. However, the quadratic complexity of transformer makes
computation costs high, limiting the number of points that can be processed
simultaneously and impeding the modeling of long-range dependencies between
objects in a single scene. Drawing inspiration from the great potential of
recent state space models (SSM) for long sequence modeling, we introduce Mamba,
an SSM-based architecture, to the point cloud domain and propose Pamba, a novel
architecture with strong global modeling capability under linear complexity.
Specifically, to make the disorderness of point clouds fit in with the causal
nature of Mamba, we propose a multi-path serialization strategy applicable to
point clouds. Besides, we propose the ConvMamba block to compensate for the
shortcomings of Mamba in modeling local geometries and in unidirectional
modeling. Pamba obtains state-of-the-art results on several 3D point cloud
segmentation tasks, including ScanNet v2, ScanNet200, S3DIS and nuScenes, while
its effectiveness is validated by extensive experiments.",['cs.CV'],http://arxiv.org/abs/2406.17442v3
CULTURE3D: Cultural Landmarks and Terrain Dataset for 3D Applications,"In this paper, we present a large-scale fine-grained dataset using
high-resolution images captured from locations worldwide. Compared to existing
datasets, our dataset offers a significantly larger size and includes a higher
level of detail, making it uniquely suited for fine-grained 3D applications.
Notably, our dataset is built using drone-captured aerial imagery, which
provides a more accurate perspective for capturing real-world site layouts and
architectural structures. By reconstructing environments with these detailed
images, our dataset supports applications such as the COLMAP format for
Gaussian Splatting and the Structure-from-Motion (SfM) method. It is compatible
with widely-used techniques including SLAM, Multi-View Stereo, and Neural
Radiance Fields (NeRF), enabling accurate 3D reconstructions and point clouds.
This makes it a benchmark for reconstruction and segmentation tasks. The
dataset enables seamless integration with multi-modal data, supporting a range
of 3D applications, from architectural reconstruction to virtual tourism. Its
flexibility promotes innovation, facilitating breakthroughs in 3D modeling and
analysis.",['cs.CV'],http://arxiv.org/abs/2501.06927v1
An End-to-End Robust Point Cloud Semantic Segmentation Network with Single-Step Conditional Diffusion Models,"Existing conditional Denoising Diffusion Probabilistic Models (DDPMs) with a
Noise-Conditional Framework (NCF) remain challenging for 3D scene understanding
tasks, as the complex geometric details in scenes increase the difficulty of
fitting the gradients of the data distribution (the scores) from semantic
labels. This also results in longer training and inference time for DDPMs
compared to non-DDPMs. From a different perspective, we delve deeply into the
model paradigm dominated by the Conditional Network. In this paper, we propose
an end-to-end robust semantic Segmentation Network based on a Conditional-Noise
Framework (CNF) of DDPMs, named CDSegNet. Specifically, CDSegNet models the
Noise Network (NN) as a learnable noise-feature generator. This enables the
Conditional Network (CN) to understand 3D scene semantics under multi-level
feature perturbations, enhancing the generalization in unseen scenes.
Meanwhile, benefiting from the noise system of DDPMs, CDSegNet exhibits strong
noise and sparsity robustness in experiments. Moreover, thanks to CNF, CDSegNet
can generate the semantic labels in a single-step inference like non-DDPMs, due
to avoiding directly fitting the scores from semantic labels in the dominant
network of CDSegNet. On public indoor and outdoor benchmarks, CDSegNet
significantly outperforms existing methods, achieving state-of-the-art
performance.",['cs.CV'],http://arxiv.org/abs/2411.16308v3
Training-Free Point Cloud Recognition Based on Geometric and Semantic Information Fusion,"The trend of employing training-free methods for point cloud recognition is
becoming increasingly popular due to its significant reduction in computational
resources and time costs. However, existing approaches are limited as they
typically extract either geometric or semantic features. To address this
limitation, we are the first to propose a novel training-free method that
integrates both geometric and semantic features. For the geometric branch, we
adopt a non-parametric strategy to extract geometric features. In the semantic
branch, we leverage a model aligned with text features to obtain semantic
features. Additionally, we introduce the GFE module to complement the geometric
information of point clouds and the MFF module to improve performance in
few-shot settings. Experimental results demonstrate that our method outperforms
existing state-of-the-art training-free approaches on mainstream benchmark
datasets, including ModelNet and ScanObiectNN.",['cs.CV'],http://arxiv.org/abs/2409.04760v5
PGSR: Planar-based Gaussian Splatting for Efficient and High-Fidelity Surface Reconstruction,"Recently, 3D Gaussian Splatting (3DGS) has attracted widespread attention due
to its high-quality rendering, and ultra-fast training and rendering speed.
However, due to the unstructured and irregular nature of Gaussian point clouds,
it is difficult to guarantee geometric reconstruction accuracy and multi-view
consistency simply by relying on image reconstruction loss. Although many
studies on surface reconstruction based on 3DGS have emerged recently, the
quality of their meshes is generally unsatisfactory. To address this problem,
we propose a fast planar-based Gaussian splatting reconstruction representation
(PGSR) to achieve high-fidelity surface reconstruction while ensuring
high-quality rendering. Specifically, we first introduce an unbiased depth
rendering method, which directly renders the distance from the camera origin to
the Gaussian plane and the corresponding normal map based on the Gaussian
distribution of the point cloud, and divides the two to obtain the unbiased
depth. We then introduce single-view geometric, multi-view photometric, and
geometric regularization to preserve global geometric accuracy. We also propose
a camera exposure compensation model to cope with scenes with large
illumination variations. Experiments on indoor and outdoor scenes show that our
method achieves fast training and rendering while maintaining high-fidelity
rendering and geometric reconstruction, outperforming 3DGS-based and NeRF-based
methods.",['cs.CV'],http://arxiv.org/abs/2406.06521v2
LPRnet: A self-supervised registration network for LiDAR and photogrammetric point clouds,"LiDAR and photogrammetry are active and passive remote sensing techniques for
point cloud acquisition, respectively, offering complementary advantages and
heterogeneous. Due to the fundamental differences in sensing mechanisms,
spatial distributions and coordinate systems, their point clouds exhibit
significant discrepancies in density, precision, noise, and overlap. Coupled
with the lack of ground truth for large-scale scenes, integrating the
heterogeneous point clouds is a highly challenging task. This paper proposes a
self-supervised registration network based on a masked autoencoder, focusing on
heterogeneous LiDAR and photogrammetric point clouds. At its core, the method
introduces a multi-scale masked training strategy to extract robust features
from heterogeneous point clouds under self-supervision. To further enhance
registration performance, a rotation-translation embedding module is designed
to effectively capture the key features essential for accurate rigid
transformations. Building upon the robust representations, a transformer-based
architecture seamlessly integrates local and global features, fostering precise
alignment across diverse point cloud datasets. The proposed method demonstrates
strong feature extraction capabilities for both LiDAR and photogrammetric point
clouds, addressing the challenges of acquiring ground truth at the scene level.
Experiments conducted on two real-world datasets validate the effectiveness of
the proposed method in solving heterogeneous point cloud registration problems.","['cs.CV', 'eess.IV']",http://arxiv.org/abs/2501.05669v1
GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models,"In recent years, 2D Vision-Language Models (VLMs) have made significant
strides in image-text understanding tasks. However, their performance in 3D
spatial comprehension, which is critical for embodied intelligence, remains
limited. Recent advances have leveraged 3D point clouds and multi-view images
as inputs, yielding promising results. However, we propose exploring a purely
vision-based solution inspired by human perception, which merely relies on
visual cues for 3D spatial understanding. This paper empirically investigates
the limitations of VLMs in 3D spatial knowledge, revealing that their primary
shortcoming lies in the lack of global-local correspondence between the scene
and individual frames. To address this, we introduce GPT4Scene, a novel visual
prompting paradigm in VLM training and inference that helps build the
global-local relationship, significantly improving the 3D spatial understanding
of indoor scenes. Specifically, GPT4Scene constructs a 3D Bird's Eye View (BEV)
image from the video and marks consistent object IDs across both frames and the
BEV image. The model then inputs the concatenated BEV image and video frames
with markers. In zero-shot evaluations, GPT4Scene improves performance over
closed-source VLMs like GPT-4o. Additionally, we prepare a processed video
dataset consisting of 165K text annotation to fine-tune open-source VLMs,
achieving state-of-the-art performance on all 3D understanding tasks.
Surprisingly, after training with the GPT4Scene paradigm, VLMs consistently
improve during inference, even without visual prompting and BEV image as
explicit correspondence. It demonstrates that the proposed paradigm helps VLMs
develop an intrinsic ability to understand 3D scenes, which paves the way for a
noninvasive approach to extending pre-trained VLMs for 3D scene understanding.",['cs.CV'],http://arxiv.org/abs/2501.01428v3
Voxel-Aggregated Feature Synthesis: Efficient Dense Mapping for Simulated 3D Reasoning,"We address the issue of the exploding computational requirements of recent
State-of-the-art (SOTA) open set multimodel 3D mapping (dense 3D mapping)
algorithms and present Voxel-Aggregated Feature Synthesis (VAFS), a novel
approach to dense 3D mapping in simulation. Dense 3D mapping involves
segmenting and embedding sequential RGBD frames which are then fused into 3D.
This leads to redundant computation as the differences between frames are small
but all are individually segmented and embedded. This makes dense 3D mapping
impractical for research involving embodied agents in which the environment,
and thus the mapping, must be modified with regularity. VAFS drastically
reduces this computation by using the segmented point cloud computed by a
simulator's physics engine and synthesizing views of each region. This reduces
the number of features to embed from the number of captured RGBD frames to the
number of objects in the scene, effectively allowing a ""ground truth"" semantic
map to be computed an order of magnitude faster than traditional methods. We
test the resulting representation by assessing the IoU scores of semantic
queries for different objects in the simulated scene, and find that VAFS
exceeds the accuracy and speed of prior dense 3D mapping techniques.",['cs.CV'],http://arxiv.org/abs/2411.10616v2
CoE: Deep Coupled Embedding for Non-Rigid Point Cloud Correspondences,"The interest in matching non-rigidly deformed shapes represented as raw point
clouds is rising due to the proliferation of low-cost 3D sensors. Yet, the task
is challenging since point clouds are irregular and there is a lack of
intrinsic shape information. We propose to tackle these challenges by learning
a new shape representation -- a per-point high dimensional embedding, in an
embedding space where semantically similar points share similar embeddings. The
learned embedding has multiple beneficial properties: it is aware of the
underlying shape geometry and is robust to shape deformations and various shape
artefacts, such as noise and partiality. Consequently, this embedding can be
directly employed to retrieve high-quality dense correspondences through a
simple nearest neighbor search in the embedding space. Extensive experiments
demonstrate new state-of-the-art results and robustness in numerous challenging
non-rigid shape matching benchmarks and show its great potential in other shape
analysis tasks, such as segmentation.",['cs.CV'],http://arxiv.org/abs/2412.05557v2
Advancing ALS Applications with Large-Scale Pre-training: Dataset Development and Downstream Assessment,"The pre-training and fine-tuning paradigm has revolutionized satellite remote
sensing applications. However, this approach remains largely underexplored for
airborne laser scanning (ALS), an important technology for applications such as
forest management and urban planning. In this study, we address this gap by
constructing a large-scale ALS point cloud dataset and evaluating its impact on
downstream applications. Our dataset comprises ALS point clouds collected
across the contiguous United States, provided by the United States Geological
Survey's 3D Elevation Program. To ensure efficient data collection while
capturing diverse land cover and terrain types, we introduce a geospatial
sampling method that selects point cloud tiles based on land cover maps and
digital elevation models. As a baseline self-supervised learning model, we
adopt BEV-MAE, a state-of-the-art masked autoencoder for 3D outdoor point
clouds, and pre-train it on the constructed dataset. The pre-trained models are
subsequently fine-tuned for downstream tasks, including tree species
classification, terrain scene recognition, and point cloud semantic
segmentation. Our results show that the pre-trained models significantly
outperform their scratch counterparts across all downstream tasks,
demonstrating the transferability of the representations learned from the
proposed dataset. Furthermore, we observe that scaling the dataset using our
geospatial sampling method consistently enhances performance, whereas
pre-training on datasets constructed with random sampling fails to achieve
similar improvements. These findings highlight the utility of the constructed
dataset and the effectiveness of our sampling strategy in the pre-training and
fine-tuning paradigm. The source code and pre-trained models will be made
publicly available at \url{https://github.com/martianxiu/ALS_pretraining}.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.05095v1
OneLLM: One Framework to Align All Modalities with Language,"Multimodal large language models (MLLMs) have gained significant attention
due to their strong multimodal understanding capability. However, existing
works rely heavily on modality-specific encoders, which usually differ in
architecture and are limited to common modalities. In this paper, we present
OneLLM, an MLLM that aligns eight modalities to language using a unified
framework. We achieve this through a unified multimodal encoder and a
progressive multimodal alignment pipeline. In detail, we first train an image
projection module to connect a vision encoder with LLM. Then, we build a
universal projection module (UPM) by mixing multiple image projection modules
and dynamic routing. Finally, we progressively align more modalities to LLM
with the UPM. To fully leverage the potential of OneLLM in following
instructions, we also curated a comprehensive multimodal instruction dataset,
including 2M items from image, audio, video, point cloud, depth/normal map, IMU
and fMRI brain activity. OneLLM is evaluated on 25 diverse benchmarks,
encompassing tasks such as multimodal captioning, question answering and
reasoning, where it delivers excellent performance. Code, data, model and
online demo are available at https://github.com/csuhan/OneLLM","['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG', 'cs.MM']",http://arxiv.org/abs/2312.03700v2
Enhancing Graph Self-Supervised Learning with Graph Interplay,"Graph self-supervised learning (GSSL) has emerged as a compelling framework
for extracting informative representations from graph-structured data without
extensive reliance on labeled inputs. In this study, we introduce Graph
Interplay (GIP), an innovative and versatile approach that significantly
enhances the performance equipped with various existing GSSL methods. To this
end, GIP advocates direct graph-level communications by introducing random
inter-graph edges within standard batches. Against GIP's simplicity, we further
theoretically show that \textsc{GIP} essentially performs a principled manifold
separation via combining inter-graph message passing and GSSL, bringing about
more structured embedding manifolds and thus benefits a series of downstream
tasks. Our empirical study demonstrates that GIP surpasses the performance of
prevailing GSSL methods across multiple benchmarks by significant margins,
highlighting its potential as a breakthrough approach. Besides, GIP can be
readily integrated into a series of GSSL methods and consistently offers
additional performance gain. This advancement not only amplifies the capability
of GSSL but also potentially sets the stage for a novel graph learning paradigm
in a broader sense.","['cs.LG', 'cs.AI', 'stat.ML']",http://arxiv.org/abs/2410.04061v3
Likelihood Training of Cascaded Diffusion Models via Hierarchical Volume-preserving Maps,"Cascaded models are multi-scale generative models with a marked capacity for
producing perceptually impressive samples at high resolutions. In this work, we
show that they can also be excellent likelihood models, so long as we overcome
a fundamental difficulty with probabilistic multi-scale models: the
intractability of the likelihood function. Chiefly, in cascaded models each
intermediary scale introduces extraneous variables that cannot be tractably
marginalized out for likelihood evaluation. This issue vanishes by modeling the
diffusion process on latent spaces induced by a class of transformations we
call hierarchical volume-preserving maps, which decompose spatially structured
data in a hierarchical fashion without introducing local distortions in the
latent space. We demonstrate that two such maps are well-known in the
literature for multiscale modeling: Laplacian pyramids and wavelet transforms.
Not only do such reparameterizations allow the likelihood function to be
directly expressed as a joint likelihood over the scales, we show that the
Laplacian pyramid and wavelet transform also produces significant improvements
to the state-of-the-art on a selection of benchmarks in likelihood modeling,
including density estimation, lossless compression, and out-of-distribution
detection. Investigating the theoretical basis of our empirical gains we
uncover deep connections to score matching under the Earth Mover's Distance
(EMD), which is a well-known surrogate for perceptual similarity. Code can be
found at \href{https://github.com/lihenryhfl/pcdm}{this https url}.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2501.06999v1
DeltaGNN: Graph Neural Network with Information Flow Control,"Graph Neural Networks (GNNs) are popular deep learning models designed to
process graph-structured data through recursive neighborhood aggregations in
the message passing process. When applied to semi-supervised node
classification, the message-passing enables GNNs to understand short-range
spatial interactions, but also causes them to suffer from over-smoothing and
over-squashing. These challenges hinder model expressiveness and prevent the
use of deeper models to capture long-range node interactions (LRIs) within the
graph. Popular solutions for LRIs detection are either too expensive to process
large graphs due to high time complexity or fail to generalize across diverse
graph structures. To address these limitations, we propose a mechanism called
\emph{information flow control}, which leverages a novel connectivity measure,
called \emph{information flow score}, to address over-smoothing and
over-squashing with linear computational overhead, supported by theoretical
evidence. Finally, to prove the efficacy of our methodology we design DeltaGNN,
the first scalable and generalizable approach for detecting long-range and
short-range interactions. We benchmark our model across 10 real-world datasets,
including graphs with varying sizes, topologies, densities, and homophilic
ratios, showing superior performance with limited computational complexity. The
implementation of the proposed methods are publicly available at
https://github.com/basiralab/DeltaGNN.",['cs.LG'],http://arxiv.org/abs/2501.06002v1
Boosting Graph Neural Network Training by Focusing on Non-Robust Samples from the Training Set,"Graph Neural Networks (GNNs) are a highly effective neural network
architecture for processing graph-structured data. Unlike traditional neural
networks that rely solely on the features of the data as input, GNNs leverage
both the graph structure, which represents the relationships between data
points, and the feature matrix of the data to optimize their feature
representation. This unique capability enables GNNs to achieve superior
performance across various tasks. However, it also makes GNNs more susceptible
to noise from both the graph structure and data features, which can
significantly increase the training difficulty and degrade their performance.
To address this issue, this paper proposes a novel method for selecting
noise-sensitive training samples from the original training set to construct a
smaller yet more effective training set for model training. These samples are
then used to enhance the model's ability to handle noise-prone instances
effectively. We have evaluated our approach on three of the most classical GNN
models -- GCN, GAT, and GraphSAGE -- as well as three widely used benchmark
datasets: Cora, Citeseer, and PubMed. Our experiments demonstrate that the
proposed method can substantially boost the overall training of Graph Neural
Networks compared to using randomly constructed training sets.",['cs.LG'],http://arxiv.org/abs/2412.14738v5
A Text-Based Knowledge-Embedded Soft Sensing Modeling Approach for General Industrial Process Tasks Based on Large Language Model,"Data-driven soft sensors (DDSS) have become mainstream methods for predicting
key performance indicators in process industries. However, DDSS development
requires complex and costly customized designs tailored to various tasks during
the modeling process. Moreover, DDSS are constrained to a single structured
data modality, limiting their ability to incorporate additional contextual
knowledge. Furthermore, DDSSs' limited representation learning leads to weak
predictive performance with scarce data. To address these challenges, we
propose a general framework named LLM-TKESS (large language model for
text-based knowledge-embedded soft sensing), harnessing the powerful general
problem-solving capabilities, cross-modal knowledge transfer abilities, and
few-shot capabilities of LLM for enhanced soft sensing modeling. Specifically,
an auxiliary variable series encoder (AVS Encoder) is proposed to unleash LLM's
potential for capturing temporal relationships within series and spatial
semantic relationships among auxiliary variables. Then, we propose a two-stage
fine-tuning alignment strategy: in the first stage, employing
parameter-efficient fine-tuning through autoregressive training adjusts LLM to
rapidly accommodate process variable data, resulting in a soft sensing
foundation model (SSFM). Subsequently, by training adapters, we adapt the SSFM
to various downstream tasks without modifying its architecture. Then, we
propose two text-based knowledge-embedded soft sensors, integrating new natural
language modalities to overcome the limitations of pure structured data models.
Furthermore, benefiting from LLM's pre-existing world knowledge, our model
demonstrates outstanding predictive capabilities in small sample conditions.
Using the thermal deformation of air preheater rotor as a case study, we
validate through extensive experiments that LLM-TKESS exhibits outstanding
performance.","['cs.AI', 'cs.LG']",http://arxiv.org/abs/2501.05075v1
Let's Ask GNN: Empowering Large Language Model for Graph In-Context Learning,"Textual Attributed Graphs (TAGs) are crucial for modeling complex real-world
systems, yet leveraging large language models (LLMs) for TAGs presents unique
challenges due to the gap between sequential text processing and
graph-structured data. We introduce AskGNN, a novel approach that bridges this
gap by leveraging In-Context Learning (ICL) to integrate graph data and
task-specific information into LLMs. AskGNN employs a Graph Neural Network
(GNN)-powered structure-enhanced retriever to select labeled nodes across
graphs, incorporating complex graph structures and their supervision signals.
Our learning-to-retrieve algorithm optimizes the retriever to select example
nodes that maximize LLM performance on graph. Experiments across three tasks
and seven LLMs demonstrate AskGNN's superior effectiveness in graph task
performance, opening new avenues for applying LLMs to graph-structured data
without extensive fine-tuning.",['cs.LG'],http://arxiv.org/abs/2410.07074v2
A Soft Sensor Method with Uncertainty-Awareness and Self-Explanation Based on Large Language Models Enhanced by Domain Knowledge Retrieval,"Data-driven soft sensors are crucial in predicting key performance indicators
in industrial systems. However, current methods predominantly rely on the
supervised learning paradigms of parameter updating, which inherently faces
challenges such as high development costs, poor robustness, training
instability, and lack of interpretability. Recently, large language models
(LLMs) have demonstrated significant potential across various domains, notably
through In-Context Learning (ICL), which enables high-performance task
execution with minimal input-label demonstrations and no prior training. This
paper aims to replace supervised learning with the emerging ICL paradigm for
soft sensor modeling to address existing challenges and explore new avenues for
advancement. To achieve this, we propose a novel framework called the Few-shot
Uncertainty-aware and self-Explaining Soft Sensor (LLM-FUESS), which includes
the Zero-shot Auxiliary Variable Selector (LLM-ZAVS) and the Uncertainty-aware
Few-shot Soft Sensor (LLM-UFSS). The LLM-ZAVS retrieves from the Industrial
Knowledge Vector Storage to enhance LLMs' domain-specific knowledge, enabling
zero-shot auxiliary variable selection. In the LLM-UFSS, we utilize text-based
context demonstrations of structured data to prompt LLMs to execute ICL for
predicting and propose a context sample retrieval augmentation strategy to
improve performance. Additionally, we explored LLMs' AIGC and probabilistic
characteristics to propose self-explanation and uncertainty quantification
methods for constructing a trustworthy soft sensor. Extensive experiments
demonstrate that our method achieved state-of-the-art predictive performance,
strong robustness, and flexibility, effectively mitigates training instability
found in traditional methods. To the best of our knowledge, this is the first
work to establish soft sensor utilizing LLMs.","['cs.LG', 'cs.AI', 'eess.SP']",http://arxiv.org/abs/2501.03295v2
SALT: Sales Autocompletion Linked Business Tables Dataset,"Foundation models, particularly those that incorporate Transformer
architectures, have demonstrated exceptional performance in domains such as
natural language processing and image processing. Adapting these models to
structured data, like tables, however, introduces significant challenges. These
difficulties are even more pronounced when addressing multi-table data linked
via foreign key, which is prevalent in the enterprise realm and crucial for
empowering business use cases. Despite its substantial impact, research
focusing on such linked business tables within enterprise settings remains a
significantly important yet underexplored domain. To address this, we introduce
a curated dataset sourced from an Enterprise Resource Planning (ERP) system,
featuring extensive linked tables. This dataset is specifically designed to
support research endeavors in table representation learning. By providing
access to authentic enterprise data, our goal is to potentially enhance the
effectiveness and applicability of models for real-world business contexts.","['cs.LG', 'cs.AI', 'cs.DB']",http://arxiv.org/abs/2501.03413v1
Enhancing Trustworthiness of Graph Neural Networks with Rank-Based Conformal Training,"Graph Neural Networks (GNNs) has been widely used in a variety of fields
because of their great potential in representing graph-structured data.
However, lacking of rigorous uncertainty estimations limits their application
in high-stakes. Conformal Prediction (CP) can produce statistically guaranteed
uncertainty estimates by using the classifier's probability estimates to obtain
prediction sets, which contains the true class with a user-specified
probability. In this paper, we propose a Rank-based CP during training
framework to GNNs (RCP-GNN) for reliable uncertainty estimates to enhance the
trustworthiness of GNNs in the node classification scenario. By exploiting rank
information of the classifier's outcome, prediction sets with desired coverage
rate can be efficiently constructed. The strategy of CP during training with
differentiable rank-based conformity loss function is further explored to adapt
prediction sets according to network topology information. In this way, the
composition of prediction sets can be guided by the goal of jointly reducing
inefficiency and probability estimation errors. Extensive experiments on
several real-world datasets show that our model achieves any pre-defined target
marginal coverage while significantly reducing the inefficiency compared with
state-of-the-art methods.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2501.02767v1
DiffGraph: Heterogeneous Graph Diffusion Model,"Recent advances in Graph Neural Networks (GNNs) have revolutionized
graph-structured data modeling, yet traditional GNNs struggle with complex
heterogeneous structures prevalent in real-world scenarios. Despite progress in
handling heterogeneous interactions, two fundamental challenges persist: noisy
data significantly compromising embedding quality and learning performance, and
existing methods' inability to capture intricate semantic transitions among
heterogeneous relations, which impacts downstream predictions. To address these
fundamental issues, we present the Heterogeneous Graph Diffusion Model
(DiffGraph), a pioneering framework that introduces an innovative cross-view
denoising strategy. This advanced approach transforms auxiliary heterogeneous
data into target semantic spaces, enabling precise distillation of
task-relevant information. At its core, DiffGraph features a sophisticated
latent heterogeneous graph diffusion mechanism, implementing a novel forward
and backward diffusion process for superior noise management. This methodology
achieves simultaneous heterogeneous graph denoising and cross-type transition,
while significantly simplifying graph generation through its latent-space
diffusion capabilities. Through rigorous experimental validation on both public
and industrial datasets, we demonstrate that DiffGraph consistently surpasses
existing methods in link prediction and node classification tasks, establishing
new benchmarks for robustness and efficiency in heterogeneous graph processing.
The model implementation is publicly available at:
https://github.com/HKUDS/DiffGraph.","['cs.LG', 'cs.AI', 'cs.IR']",http://arxiv.org/abs/2501.02313v1
Residual connections provably mitigate oversmoothing in graph neural networks,"Graph neural networks (GNNs) have achieved remarkable empirical success in
processing and representing graph-structured data across various domains.
However, a significant challenge known as ""oversmoothing"" persists, where
vertex features become nearly indistinguishable in deep GNNs, severely
restricting their expressive power and practical utility. In this work, we
analyze the asymptotic oversmoothing rates of deep GNNs with and without
residual connections by deriving explicit convergence rates for a normalized
vertex similarity measure. Our analytical framework is grounded in the
multiplicative ergodic theorem. Furthermore, we demonstrate that adding
residual connections effectively mitigates or prevents oversmoothing across
several broad families of parameter distributions. The theoretical findings are
strongly supported by numerical experiments.","['cs.LG', 'math.DS', 'math.PR', 'stat.ML']",http://arxiv.org/abs/2501.00762v2
Higher Order Structures For Graph Explanations,"Graph Neural Networks (GNNs) have emerged as powerful tools for learning
representations of graph-structured data, demonstrating remarkable performance
across various tasks. Recognising their importance, there has been extensive
research focused on explaining GNN predictions, aiming to enhance their
interpretability and trustworthiness. However, GNNs and their explainers face a
notable challenge: graphs are primarily designed to model pair-wise
relationships between nodes, which can make it tough to capture higher-order,
multi-node interactions. This characteristic can pose difficulties for existing
explainers in fully representing multi-node relationships. To address this gap,
we present Framework For Higher-Order Representations In Graph Explanations
(FORGE), a framework that enables graph explainers to capture such interactions
by incorporating higher-order structures, resulting in more accurate and
faithful explanations. Extensive evaluation shows that on average real-world
datasets from the GraphXAI benchmark and synthetic datasets across various
graph explainers, FORGE improves average explanation accuracy by 1.9x and
2.25x, respectively. We perform ablation studies to confirm the importance of
higher-order relations in improving explanations, while our scalability
analysis demonstrates FORGE's efficacy on large graphs.","['cs.LG', 'I.2.4']",http://arxiv.org/abs/2406.03253v6
RealDiffFusionNet: Neural Controlled Differential Equation Informed Multi-Head Attention Fusion Networks for Disease Progression Modeling Using Real-World Data,"This paper presents a novel deep learning-based approach named
RealDiffFusionNet incorporating Neural Controlled Differential Equations
(Neural CDE) - time series models that are robust in handling irregularly
sampled data - and multi-head attention to align relevant multimodal context
(image data, time invariant data, etc.) at each time point. Long short-term
memory (LSTM) models were also used as a baseline. Two different datasets were
used: a data from the Open-Source Imaging Consortium (OSIC) containing
structured time series data of demographics and lung function with a baseline
CT scan of the lungs and the second from the Alzheimer's Disease Neuroimaging
Initiative (ADNI) containing a series of MRI scans along with demographics,
physical examinations, and cognitive assessment data. An ablation study was
performed to understand the role of CDEs, multimodal data, attention fusion,
and interpolation strategies on model performance. When the baseline models
were evaluated, the use of multimodal data resulted in an improvement in Neural
CDE performance, with a lower test RMSE. Additionally, the performance of
multimodal Neural CDE was also superior to multimodal LSTM. In the
attention-based architectures, fusion through concatenation and rectilinear
interpolation were found to improve model performance. The performance of the
proposed RealDiffFusionNet was found to be superior (0.2570) to all models. For
the ADNI dataset, between the Neural-CDE and LSTM models trained only on the
structured data, the test RMSE were comparable (0.471 for LSTM vs. 0.4581
Neural-CDE). Furthermore, the addition of image features from patients' MRI
series resulted in an improvement in performance, with a lower test RMSE
(0.4372 with multimodal vs 0.4581 with structured data). RealDiffFusionNet has
shown promise in utilizing CDEs and multimodal data to accurately predict
disease progression.","['cs.LG', 'cs.CV', 'q-bio.QM']",http://arxiv.org/abs/2501.02025v1
Graph Generative Pre-trained Transformer,"Graph generation is a critical task in numerous domains, including molecular
design and social network analysis, due to its ability to model complex
relationships and structured data. While most modern graph generative models
utilize adjacency matrix representations, this work revisits an alternative
approach that represents graphs as sequences of node set and edge set. We
advocate for this approach due to its efficient encoding of graphs and propose
a novel representation. Based on this representation, we introduce the Graph
Generative Pre-trained Transformer (G2PT), an auto-regressive model that learns
graph structures via next-token prediction. To further exploit G2PT's
capabilities as a general-purpose foundation model, we explore fine-tuning
strategies for two downstream applications: goal-oriented generation and graph
property prediction. We conduct extensive experiments across multiple datasets.
Results indicate that G2PT achieves superior generative performance on both
generic graph and molecule datasets. Furthermore, G2PT exhibits strong
adaptability and versatility in downstream tasks from molecular design to
property prediction.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2501.01073v1
Multi-Objective Optimization-Based Anonymization of Structured Data for Machine Learning,"Data is essential for secondary use, but ensuring its privacy while allowing
such use is a critical challenge. Various techniques have been proposed to
address privacy concerns in data sharing and publishing. However, these methods
often degrade data utility, impacting the performance of machine learning (ML)
models. Our research identifies key limitations in existing optimization models
for privacy preservation, particularly in handling categorical variables,
assessing data utility, and evaluating effectiveness across diverse datasets.
We propose a novel multi-objective optimization model that simultaneously
minimizes information loss and maximizes protection against attacks. This model
is empirically validated using diverse datasets and compared with two existing
algorithms. We assess information loss, the number of individuals subject to
linkage or homogeneity attacks, and ML performance after anonymization. The
results indicate that our model achieves lower information loss and more
effectively mitigates the risk of attacks, reducing the number of individuals
susceptible to these attacks compared to alternative algorithms in some cases.
Additionally, our model maintains comparative ML performance relative to the
original data or data anonymized by other methods. Our findings highlight
significant improvements in privacy protection and ML model performance,
offering a comprehensive framework for balancing privacy and utility in data
sharing.","['cs.LG', 'math.OC']",http://arxiv.org/abs/2501.01002v1
diffIRM: A Diffusion-Augmented Invariant Risk Minimization Framework for Spatiotemporal Prediction over Graphs,"Spatiotemporal prediction over graphs (STPG) is challenging, because
real-world data suffers from the Out-of-Distribution (OOD) generalization
problem, where test data follow different distributions from training ones. To
address this issue, Invariant Risk Minimization (IRM) has emerged as a
promising approach for learning invariant representations across different
environments. However, IRM and its variants are originally designed for
Euclidean data like images, and may not generalize well to graph-structure data
such as spatiotemporal graphs due to spatial correlations in graphs. To
overcome the challenge posed by graph-structure data, the existing graph OOD
methods adhere to the principles of invariance existence, or environment
diversity. However, there is little research that combines both principles in
the STPG problem. A combination of the two is crucial for efficiently
distinguishing between invariant features and spurious ones. In this study, we
fill in this research gap and propose a diffusion-augmented invariant risk
minimization (diffIRM) framework that combines these two principles for the
STPG problem. Our diffIRM contains two processes: i) data augmentation and ii)
invariant learning. In the data augmentation process, a causal mask generator
identifies causal features and a graph-based diffusion model acts as an
environment augmentor to generate augmented spatiotemporal graph data. In the
invariant learning process, an invariance penalty is designed using the
augmented data, and then serves as a regularizer for training the
spatiotemporal prediction model. The real-world experiment uses three human
mobility datasets, i.e. SafeGraph, PeMS04, and PeMS08. Our proposed diffIRM
outperforms baselines.",['cs.LG'],http://arxiv.org/abs/2501.00305v1
Attributed Graph Clustering in Collaborative Settings,"Graph clustering is an unsupervised machine learning method that partitions
the nodes in a graph into different groups. Despite achieving significant
progress in exploiting both attributed and structured data information, graph
clustering methods often face practical challenges related to data isolation.
Moreover, the absence of collaborative methods for graph clustering limits
their effectiveness.
  In this paper, we propose a collaborative graph clustering framework for
attributed graphs, supporting attributed graph clustering over vertically
partitioned data with different participants holding distinct features of the
same data. Our method leverages a novel technique that reduces the sample
space, improving the efficiency of the attributed graph clustering method.
Furthermore, we compare our method to its centralized counterpart under a
proximity condition, demonstrating that the successful local results of each
participant contribute to the overall success of the collaboration.
  We fully implement our approach and evaluate its utility and efficiency by
conducting experiments on four public datasets. The results demonstrate that
our method achieves comparable accuracy levels to centralized attributed graph
clustering methods. Our collaborative graph clustering framework provides an
efficient and effective solution for graph clustering challenges related to
data isolation.","['cs.LG', 'cs.SI']",http://arxiv.org/abs/2411.12329v2
Timeseria: an object-oriented time series processing library,"Timeseria is an object-oriented time series processing library implemented in
Python, which aims at making it easier to manipulate time series data and to
build statistical and machine learning models on top of it. Unlike common data
analysis frameworks, it builds up from well defined and reusable logical units
(objects), which can be easily combined together in order to ensure a high
level of consistency. Thanks to this approach, Timeseria can address by design
several non-trivial issues which are often underestimated, such as handling
data losses, non-uniform sampling rates, differences between aggregated data
and punctual observations, time zones, daylight saving times, and more.
Timeseria comes with a comprehensive set of base data structures, data
transformations for resampling and aggregation, common data manipulation
operations, and extensible models for data reconstruction, forecasting and
anomaly detection. It also integrates a fully featured, interactive plotting
engine capable of handling even millions of data points.",['cs.LG'],http://arxiv.org/abs/2410.09567v3
Cluster-guided Contrastive Class-imbalanced Graph Classification,"This paper studies the problem of class-imbalanced graph classification,
which aims at effectively classifying the graph categories in scenarios with
imbalanced class distributions. While graph neural networks (GNNs) have
achieved remarkable success, their modeling ability on imbalanced
graph-structured data remains suboptimal, which typically leads to predictions
biased towards the majority classes. On the other hand, existing
class-imbalanced learning methods in vision may overlook the rich graph
semantic substructures of the majority classes and excessively emphasize
learning from the minority classes. To address these challenges, we propose a
simple yet powerful approach called C$^3$GNN that integrates the idea of
clustering into contrastive learning to enhance class-imbalanced graph
classification. Technically, C$^3$GNN clusters graphs from each majority class
into multiple subclasses, with sizes comparable to the minority class,
mitigating class imbalance. It also employs the Mixup technique to generate
synthetic samples, enriching the semantic diversity of each subclass.
Furthermore, supervised contrastive learning is used to hierarchically learn
effective graph representations, enabling the model to thoroughly explore
semantic substructures in majority classes while avoiding excessive focus on
minority classes. Extensive experiments on real-world graph benchmark datasets
verify the superior performance of our proposed method against competitive
baselines.","['cs.LG', 'cs.AI', 'cs.IR', 'cs.SI']",http://arxiv.org/abs/2412.12984v2
Graph Structure Refinement with Energy-based Contrastive Learning,"Graph Neural Networks (GNNs) have recently gained widespread attention as a
successful tool for analyzing graph-structured data. However, imperfect graph
structure with noisy links lacks enough robustness and may damage graph
representations, therefore limiting the GNNs' performance in practical tasks.
Moreover, existing generative architectures fail to fit discriminative
graph-related tasks. To tackle these issues, we introduce an unsupervised
method based on a joint of generative training and discriminative training to
learn graph structure and representation, aiming to improve the discriminative
performance of generative models. We propose an Energy-based Contrastive
Learning (ECL) guided Graph Structure Refinement (GSR) framework, denoted as
ECL-GSR. To our knowledge, this is the first work to combine energy-based
models with contrastive learning for GSR. Specifically, we leverage ECL to
approximate the joint distribution of sample pairs, which increases the
similarity between representations of positive pairs while reducing the
similarity between negative ones. Refined structure is produced by augmenting
and removing edges according to the similarity metrics among node
representations. Extensive experiments demonstrate that ECL-GSR outperforms the
state-of-the-art on eight benchmark datasets in node classification. ECL-GSR
achieves faster training with fewer samples and memories against the leading
baseline, highlighting its simplicity and efficiency in downstream tasks.",['cs.LG'],http://arxiv.org/abs/2412.17856v2
Learning from Heterogeneity: A Dynamic Learning Framework for Hypergraphs,"Graph neural network (GNN) has gained increasing popularity in recent years
owing to its capability and flexibility in modeling complex graph structure
data. Among all graph learning methods, hypergraph learning is a technique for
exploring the implicit higher-order correlations when training the embedding
space of the graph. In this paper, we propose a hypergraph learning framework
named LFH that is capable of dynamic hyperedge construction and attentive
embedding update utilizing the heterogeneity attributes of the graph.
Specifically, in our framework, the high-quality features are first generated
by the pairwise fusion strategy that utilizes explicit graph structure
information when generating initial node embedding. Afterwards, a hypergraph is
constructed through the dynamic grouping of implicit hyperedges, followed by
the type-specific hypergraph learning process. To evaluate the effectiveness of
our proposed framework, we conduct comprehensive experiments on several popular
datasets with eleven state-of-the-art models on both node classification and
link prediction tasks, which fall into categories of homogeneous pairwise graph
learning, heterogeneous pairwise graph learning, and hypergraph learning. The
experiment results demonstrate a significant performance gain (average 12.5% in
node classification and 13.3% in link prediction) compared with recent
state-of-the-art methods.",['cs.LG'],http://arxiv.org/abs/2307.03411v3
DiffuEraser: A Diffusion Model for Video Inpainting,"Recent video inpainting algorithms integrate flow-based pixel propagation
with transformer-based generation to leverage optical flow for restoring
textures and objects using information from neighboring frames, while
completing masked regions through visual Transformers. However, these
approaches often encounter blurring and temporal inconsistencies when dealing
with large masks, highlighting the need for models with enhanced generative
capabilities. Recently, diffusion models have emerged as a prominent technique
in image and video generation due to their impressive performance. In this
paper, we introduce DiffuEraser, a video inpainting model based on stable
diffusion, designed to fill masked regions with greater details and more
coherent structures. We incorporate prior information to provide initialization
and weak conditioning,which helps mitigate noisy artifacts and suppress
hallucinations. Additionally, to improve temporal consistency during
long-sequence inference, we expand the temporal receptive fields of both the
prior model and DiffuEraser, and further enhance consistency by leveraging the
temporal smoothing property of Video Diffusion Models. Experimental results
demonstrate that our proposed method outperforms state-of-the-art techniques in
both content completeness and temporal consistency while maintaining acceptable
efficiency.",['cs.CV'],http://arxiv.org/abs/2501.10018v1
Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise,"Generative modeling aims to transform random noise into structured outputs.
In this work, we enhance video diffusion models by allowing motion control via
structured latent noise sampling. This is achieved by just a change in data: we
pre-process training videos to yield structured noise. Consequently, our method
is agnostic to diffusion model design, requiring no changes to model
architectures or training pipelines. Specifically, we propose a novel noise
warping algorithm, fast enough to run in real time, that replaces random
temporal Gaussianity with correlated warped noise derived from optical flow
fields, while preserving the spatial Gaussianity. The efficiency of our
algorithm enables us to fine-tune modern video diffusion base models using
warped noise with minimal overhead, and provide a one-stop solution for a wide
range of user-friendly motion control: local object motion control, global
camera movement control, and motion transfer. The harmonization between
temporal coherence and spatial Gaussianity in our warped noise leads to
effective motion control while maintaining per-frame pixel quality. Extensive
experiments and user studies demonstrate the advantages of our method, making
it a robust and scalable approach for controlling motion in video diffusion
models. Video results are available on our webpage:
https://vgenai-netflix-eyeline-research.github.io/Go-with-the-Flow. Source code
and model checkpoints are available on GitHub:
https://github.com/VGenAI-Netflix-Eyeline-Research/Go-with-the-Flow.",['cs.CV'],http://arxiv.org/abs/2501.08331v2
"Aligning First, Then Fusing: A Novel Weakly Supervised Multimodal Violence Detection Method","Weakly supervised violence detection refers to the technique of training
models to identify violent segments in videos using only video-level labels.
Among these approaches, multimodal violence detection, which integrates
modalities such as audio and optical flow, holds great potential. Existing
methods in this domain primarily focus on designing multimodal fusion models to
address modality discrepancies. In contrast, we take a different approach;
leveraging the inherent discrepancies across modalities in violence event
representation to propose a novel multimodal semantic feature alignment method.
This method sparsely maps the semantic features of local, transient, and less
informative modalities ( such as audio and optical flow ) into the more
informative RGB semantic feature space. Through an iterative process, the
method identifies the suitable no-zero feature matching subspace and aligns the
modality-specific event representations based on this subspace, enabling the
full exploitation of information from all modalities during the subsequent
modality fusion stage. Building on this, we design a new weakly supervised
violence detection framework that consists of unimodal multiple-instance
learning for extracting unimodal semantic features, multimodal alignment,
multimodal fusion, and final detection. Experimental results on benchmark
datasets demonstrate the effectiveness of our method, achieving an average
precision (AP) of 86.07% on the XD-Violence dataset. Our code is available at
https://github.com/xjpp2016/MAVD.",['cs.CV'],http://arxiv.org/abs/2501.07496v1
Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion,"Recently, diffusion-based methods have achieved great improvements in the
video inpainting task. However, these methods still face many challenges, such
as maintaining temporal consistency and the time-consuming issue. This paper
proposes an advanced video inpainting framework using optical Flow-guided
Efficient Diffusion, called FloED. Specifically, FloED employs a dual-branch
architecture, where a flow branch first restores corrupted flow and a
multi-scale flow adapter provides motion guidance to the main inpainting
branch. Additionally, a training-free latent interpolation method is proposed
to accelerate the multi-step denoising process using flow warping. Further
introducing a flow attention cache mechanism, FLoED efficiently reduces the
computational cost brought by incorporating optical flow. Comprehensive
experiments in both background restoration and object removal tasks demonstrate
that FloED outperforms state-of-the-art methods from the perspective of both
performance and efficiency.",['cs.CV'],http://arxiv.org/abs/2412.00857v2
Edit as You See: Image-guided Video Editing via Masked Motion Modeling,"Recent advancements in diffusion models have significantly facilitated
text-guided video editing. However, there is a relative scarcity of research on
image-guided video editing, a method that empowers users to edit videos by
merely indicating a target object in the initial frame and providing an RGB
image as reference, without relying on the text prompts. In this paper, we
propose a novel Image-guided Video Editing Diffusion model, termed IVEDiff for
the image-guided video editing. IVEDiff is built on top of image editing
models, and is equipped with learnable motion modules to maintain the temporal
consistency of edited video. Inspired by self-supervised learning concepts, we
introduce a masked motion modeling fine-tuning strategy that empowers the
motion module's capabilities for capturing inter-frame motion dynamics, while
preserving the capabilities for intra-frame semantic correlations modeling of
the base image editing model. Moreover, an optical-flow-guided motion reference
network is proposed to ensure the accurate propagation of information between
edited video frames, alleviating the misleading effects of invalid information.
We also construct a benchmark to facilitate further research. The comprehensive
experiments demonstrate that our method is able to generate temporally smooth
edited videos while robustly dealing with various editing objects with high
quality.",['cs.CV'],http://arxiv.org/abs/2501.04325v1
ProTracker: Probabilistic Integration for Robust and Accurate Point Tracking,"In this paper, we propose ProTracker, a novel framework for robust and
accurate long-term dense tracking of arbitrary points in videos. The key idea
of our method is incorporating probabilistic integration to refine multiple
predictions from both optical flow and semantic features for robust short-term
and long-term tracking. Specifically, we integrate optical flow estimations in
a probabilistic manner, producing smooth and accurate trajectories by
maximizing the likelihood of each prediction. To effectively re-localize
challenging points that disappear and reappear due to occlusion, we further
incorporate long-term feature correspondence into our flow predictions for
continuous trajectory generation. Extensive experiments show that ProTracker
achieves the state-of-the-art performance among unsupervised and
self-supervised approaches, and even outperforms supervised methods on several
benchmarks. Our code and model will be publicly available upon publication.",['cs.CV'],http://arxiv.org/abs/2501.03220v1
AHMSA-Net: Adaptive Hierarchical Multi-Scale Attention Network for Micro-Expression Recognition,"Micro-expression recognition (MER) presents a significant challenge due to
the transient and subtle nature of the motion changes involved. In recent
years, deep learning methods based on attention mechanisms have made some
breakthroughs in MER. However, these methods still suffer from the limitations
of insufficient feature capture and poor dynamic adaptation when coping with
the instantaneous subtle movement changes of micro-expressions. Therefore, in
this paper, we design an Adaptive Hierarchical Multi-Scale Attention Network
(AHMSA-Net) for MER. Specifically, we first utilize the onset and apex frames
of the micro-expression sequence to extract three-dimensional (3D) optical flow
maps, including horizontal optical flow, vertical optical flow, and optical
flow strain. Subsequently, the optical flow feature maps are inputted into
AHMSA-Net, which consists of two parts: an adaptive hierarchical framework and
a multi-scale attention mechanism. Based on the adaptive downsampling
hierarchical attention framework, AHMSA-Net captures the subtle changes of
micro-expressions from different granularities (fine and coarse) by dynamically
adjusting the size of the optical flow feature map at each layer. Based on the
multi-scale attention mechanism, AHMSA-Net learns micro-expression action
information by fusing features from different scales (channel and spatial).
These two modules work together to comprehensively improve the accuracy of MER.
Additionally, rigorous experiments demonstrate that the proposed method
achieves competitive results on major micro-expression databases, with
AHMSA-Net achieving recognition accuracy of up to 78.21% on composite databases
(SMIC, SAMM, CASMEII) and 77.08% on the CASME^{}3 database.",['cs.CV'],http://arxiv.org/abs/2501.02539v1
Leveraging Consistent Spatio-Temporal Correspondence for Robust Visual Odometry,"Recent approaches to VO have significantly improved performance by using deep
networks to predict optical flow between video frames. However, existing
methods still suffer from noisy and inconsistent flow matching, making it
difficult to handle challenging scenarios and long-sequence estimation. To
overcome these challenges, we introduce Spatio-Temporal Visual Odometry (STVO),
a novel deep network architecture that effectively leverages inherent
spatio-temporal cues to enhance the accuracy and consistency of multi-frame
flow matching. With more accurate and consistent flow matching, STVO can
achieve better pose estimation through the bundle adjustment (BA).
Specifically, STVO introduces two innovative components: 1) the Temporal
Propagation Module that utilizes multi-frame information to extract and
propagate temporal cues across adjacent frames, maintaining temporal
consistency; 2) the Spatial Activation Module that utilizes geometric priors
from the depth maps to enhance spatial consistency while filtering out
excessive noise and incorrect matches. Our STVO achieves state-of-the-art
performance on TUM-RGBD, EuRoc MAV, ETH3D and KITTI Odometry benchmarks.
Notably, it improves accuracy by 77.8% on ETH3D benchmark and 38.9% on KITTI
Odometry benchmark over the previous best methods.",['cs.CV'],http://arxiv.org/abs/2412.16923v2
Spatially-guided Temporal Aggregation for Robust Event-RGB Optical Flow Estimation,"Current optical flow methods exploit the stable appearance of frame (or RGB)
data to establish robust correspondences across time. Event cameras, on the
other hand, provide high-temporal-resolution motion cues and excel in
challenging scenarios. These complementary characteristics underscore the
potential of integrating frame and event data for optical flow estimation.
However, most cross-modal approaches fail to fully utilize the complementary
advantages, relying instead on simply stacking information. This study
introduces a novel approach that uses a spatially dense modality to guide the
aggregation of the temporally dense event modality, achieving effective
cross-modal fusion. Specifically, we propose an event-enhanced frame
representation that preserves the rich texture of frames and the basic
structure of events. We use the enhanced representation as the guiding modality
and employ events to capture temporally dense motion information. The robust
motion features derived from the guiding modality direct the aggregation of
motion information from events. To further enhance fusion, we propose a
transformer-based module that complements sparse event motion features with
spatially rich frame information and enhances global information propagation.
Additionally, a mix-fusion encoder is designed to extract comprehensive
spatiotemporal contextual features from both modalities. Extensive experiments
on the MVSEC and DSEC-Flow datasets demonstrate the effectiveness of our
framework. Leveraging the complementary strengths of frames and events, our
method achieves leading performance on the DSEC-Flow dataset. Compared to the
event-only model, frame guidance improves accuracy by 10\%. Furthermore, it
outperforms the state-of-the-art fusion-based method with a 4\% accuracy gain
and a 45\% reduction in inference time.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2501.00838v1
GFlow: Recovering 4D World from Monocular Video,"Recovering 4D world from monocular video is a crucial yet challenging task.
Conventional methods usually rely on the assumptions of multi-view videos,
known camera parameters, or static scenes. In this paper, we relax all these
constraints and tackle a highly ambitious but practical task: With only one
monocular video without camera parameters, we aim to recover the dynamic 3D
world alongside the camera poses. To solve this, we introduce GFlow, a new
framework that utilizes only 2D priors (depth and optical flow) to lift a video
to a 4D scene, as a flow of 3D Gaussians through space and time. GFlow starts
by segmenting the video into still and moving parts, then alternates between
optimizing camera poses and the dynamics of the 3D Gaussian points. This method
ensures consistency among adjacent points and smooth transitions between
frames. Since dynamic scenes always continually introduce new visual content,
we present prior-driven initialization and pixel-wise densification strategy
for Gaussian points to integrate new content. By combining all those
techniques, GFlow transcends the boundaries of 4D recovery from causal videos;
it naturally enables tracking of points and segmentation of moving objects
across frames. Additionally, GFlow estimates the camera poses for each frame,
enabling novel view synthesis by changing camera pose. This capability
facilitates extensive scene-level or object-level editing, highlighting GFlow's
versatility and effectiveness. Visit our project page at:
https://littlepure2333.github.io/GFlow","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2405.18426v2
BiM-VFI: directional Motion Field-Guided Frame Interpolation for Video with Non-uniform Motions,"Existing Video Frame interpolation (VFI) models tend to suffer from
time-to-location ambiguity when trained with video of non-uniform motions, such
as accelerating, decelerating, and changing directions, which often yield
blurred interpolated frames. In this paper, we propose (i) a novel motion
description map, Bidirectional Motion field (BiM), to effectively describe
non-uniform motions; (ii) a BiM-guided Flow Net (BiMFN) with Content-Aware
Upsampling Network (CAUN) for precise optical flow estimation; and (iii)
Knowledge Distillation for VFI-centric Flow supervision (KDVCF) to supervise
the motion estimation of VFI model with VFI-centric teacher flows. The proposed
VFI is called a Bidirectional Motion field-guided VFI (BiM-VFI) model.
Extensive experiments show that our BiM-VFI model significantly surpasses the
recent state-of-the-art VFI methods by 26% and 45% improvements in LPIPS and
STLPIPS respectively, yielding interpolated frames with much fewer blurs at
arbitrary time instances.",['cs.CV'],http://arxiv.org/abs/2412.11365v2
Motion Transfer-Driven intra-class data augmentation for Finger Vein Recognition,"Finger vein recognition (FVR) has emerged as a secure biometric technique
because of the confidentiality of vascular bio-information. Recently, deep
learning-based FVR has gained increased popularity and achieved promising
performance. However, the limited size of public vein datasets has caused
overfitting issues and greatly limits the recognition performance. Although
traditional data augmentation can partially alleviate this data shortage issue,
it cannot capture the real finger posture variations due to the rigid
label-preserving image transformations, bringing limited performance
improvement. To address this issue, we propose a novel motion transfer (MT)
model for finger vein image data augmentation via modeling the actual finger
posture and rotational movements. The proposed model first utilizes a key point
detector to extract the key point and pose map of the source and drive finger
vein images. We then utilize a dense motion module to estimate the motion
optical flow, which is fed to an image generation module for generating the
image with the target pose. Experiments conducted on three public finger vein
databases demonstrate that the proposed motion transfer model can effectively
improve recognition accuracy. Code is available at:
https://github.com/kevinhuangxf/FingerVeinRecognition.",['cs.CV'],http://arxiv.org/abs/2412.20327v1
Enhancing Marine Debris Acoustic Monitoring by Optical Flow-Based Motion Vector Analysis,"With the development of coastal construction, a large amount of
human-generated waste, particularly plastic debris, is continuously entering
the ocean, posing a severe threat to marine ecosystems. The key to effectively
addressing plastic pollution lies in the ability to autonomously monitor such
debris. Currently, marine debris monitoring primarily relies on optical
sensors, but these methods are limited in their applicability to underwater and
seafloor areas due to low-visibility constraints. The acoustic camera, also
known as high-resolution forward-looking sonar (FLS), has demonstrated
considerable potential in the autonomous monitoring of marine debris, as they
are unaffected by water turbidity and dark environments. The appearance of
targets in sonar images changes with variations in the imaging viewpoint, while
challenges such as low signal-to-noise ratio, weak textures, and imaging
distortions in sonar imagery present significant obstacles to debris monitoring
based on prior class labels. This paper proposes an optical flow-based method
for marine debris monitoring, aiming to fully utilize the time series
information captured by the acoustic camera to enhance the performance of
marine debris monitoring without relying on prior category labels of the
targets. The proposed method was validated through experiments conducted in a
circulating water tank, demonstrating its feasibility and robustness. This
approach holds promise for providing novel insights into the spatial and
temporal distribution of debris.",['cs.CV'],http://arxiv.org/abs/2412.20085v1
Zero-shot Hazard Identification in Autonomous Driving: A Case Study on the COOOL Benchmark,"This paper presents our submission to the COOOL competition, a novel
benchmark for detecting and classifying out-of-label hazards in autonomous
driving. Our approach integrates diverse methods across three core tasks: (i)
driver reaction detection, (ii) hazard object identification, and (iii) hazard
captioning. We propose kernel-based change point detection on bounding boxes
and optical flow dynamics for driver reaction detection to analyze motion
patterns. For hazard identification, we combined a naive proximity-based
strategy with object classification using a pre-trained ViT model. At last, for
hazard captioning, we used the MOLMO vision-language model with tailored
prompts to generate precise and context-aware descriptions of rare and
low-resolution hazards. The proposed pipeline outperformed the baseline methods
by a large margin, reducing the relative error by 33%, and scored 2nd on the
final leaderboard consisting of 32 teams.",['cs.CV'],http://arxiv.org/abs/2412.19944v1
Generalized Uncertainty-Based Evidential Fusion with Hybrid Multi-Head Attention for Weak-Supervised Temporal Action Localization,"Weakly supervised temporal action localization (WS-TAL) is a task of
targeting at localizing complete action instances and categorizing them with
video-level labels. Action-background ambiguity, primarily caused by background
noise resulting from aggregation and intra-action variation, is a significant
challenge for existing WS-TAL methods. In this paper, we introduce a hybrid
multi-head attention (HMHA) module and generalized uncertainty-based evidential
fusion (GUEF) module to address the problem. The proposed HMHA effectively
enhances RGB and optical flow features by filtering redundant information and
adjusting their feature distribution to better align with the WS-TAL task.
Additionally, the proposed GUEF adaptively eliminates the interference of
background noise by fusing snippet-level evidences to refine uncertainty
measurement and select superior foreground feature information, which enables
the model to concentrate on integral action instances to achieve better action
localization and classification performance. Experimental results conducted on
the THUMOS14 dataset demonstrate that our method outperforms state-of-the-art
methods. Our code is available in
\url{https://github.com/heyuanpengpku/GUEF/tree/main}.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2412.19418v1
MotiF: Making Text Count in Image Animation with Motion Focal Loss,"Text-Image-to-Video (TI2V) generation aims to generate a video from an image
following a text description, which is also referred to as text-guided image
animation. Most existing methods struggle to generate videos that align well
with the text prompts, particularly when motion is specified. To overcome this
limitation, we introduce MotiF, a simple yet effective approach that directs
the model's learning to the regions with more motion, thereby improving the
text alignment and motion generation. We use optical flow to generate a motion
heatmap and weight the loss according to the intensity of the motion. This
modified objective leads to noticeable improvements and complements existing
methods that utilize motion priors as model inputs. Additionally, due to the
lack of a diverse benchmark for evaluating TI2V generation, we propose TI2V
Bench, a dataset consists of 320 image-text pairs for robust evaluation. We
present a human evaluation protocol that asks the annotators to select an
overall preference between two videos followed by their justifications. Through
a comprehensive evaluation on TI2V Bench, MotiF outperforms nine open-sourced
models, achieving an average preference of 72%. The TI2V Bench is released in
https://wang-sj16.github.io/motif/.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2412.16153v1
"Deep Learning and Hybrid Approaches for Dynamic Scene Analysis, Object Detection and Motion Tracking","This project aims to develop a robust video surveillance system, which can
segment videos into smaller clips based on the detection of activities. It uses
CCTV footage, for example, to record only major events-like the appearance of a
person or a thief-so that storage is optimized and digital searches are easier.
It utilizes the latest techniques in object detection and tracking, including
Convolutional Neural Networks (CNNs) like YOLO, SSD, and Faster R-CNN, as well
as Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks
(LSTMs), to achieve high accuracy in detection and capture temporal
dependencies. The approach incorporates adaptive background modeling through
Gaussian Mixture Models (GMM) and optical flow methods like Lucas-Kanade to
detect motions. Multi-scale and contextual analysis are used to improve
detection across different object sizes and environments. A hybrid motion
segmentation strategy combines statistical and deep learning models to manage
complex movements, while optimizations for real-time processing ensure
efficient computation. Tracking methods, such as Kalman Filters and Siamese
networks, are employed to maintain smooth tracking even in cases of occlusion.
Detection is improved on various-sized objects for multiple scenarios by
multi-scale and contextual analysis. Results demonstrate high precision and
recall in detecting and tracking objects, with significant improvements in
processing times and accuracy due to real-time optimizations and
illumination-invariant features. The impact of this research lies in its
potential to transform video surveillance, reducing storage requirements and
enhancing security through reliable and efficient object detection and
tracking.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2412.05331v2
Training Datasets Generation for Machine Learning: Application to Vision Based Navigation,"Vision Based Navigation consists in utilizing cameras as precision sensors
for GNC after extracting information from images. To enable the adoption of
machine learning for space applications, one of obstacles is the demonstration
that available training datasets are adequate to validate the algorithms. The
objective of the study is to generate datasets of images and metadata suitable
for training machine learning algorithms. Two use cases were selected and a
robust methodology was developed to validate the datasets including the ground
truth. The first use case is in-orbit rendezvous with a man-made object: a
mockup of satellite ENVISAT. The second use case is a Lunar landing scenario.
Datasets were produced from archival datasets (Chang'e 3), from the laboratory
at DLR TRON facility and at Airbus Robotic laboratory, from SurRender software
high fidelity image simulator using Model Capture and from Generative
Adversarial Networks. The use case definition included the selection of
algorithms as benchmark: an AI-based pose estimation algorithm and a dense
optical flow algorithm were selected. Eventually it is demonstrated that
datasets produced with SurRender and selected laboratory facilities are
adequate to train machine learning algorithms.","['cs.CV', 'astro-ph.EP', 'cs.GR', 'cs.LG']",http://arxiv.org/abs/2409.11383v2
Disentangled Motion Modeling for Video Frame Interpolation,"Video Frame Interpolation (VFI) aims to synthesize intermediate frames
between existing frames to enhance visual smoothness and quality. Beyond the
conventional methods based on the reconstruction loss, recent works have
employed generative models for improved perceptual quality. However, they
require complex training and large computational costs for pixel space
modeling. In this paper, we introduce disentangled Motion Modeling (MoMo), a
diffusion-based approach for VFI that enhances visual quality by focusing on
intermediate motion modeling. We propose a disentangled two-stage training
process. In the initial stage, frame synthesis and flow models are trained to
generate accurate frames and flows optimal for synthesis. In the subsequent
stage, we introduce a motion diffusion model, which incorporates our novel
U-Net architecture specifically designed for optical flow, to generate
bi-directional flows between frames. By learning the simpler low-frequency
representation of motions, MoMo achieves superior perceptual quality with
reduced computational demands compared to the generative modeling methods on
the pixel space. MoMo surpasses state-of-the-art methods in perceptual metrics
across various benchmarks, demonstrating its efficacy and efficiency in VFI.",['cs.CV'],http://arxiv.org/abs/2406.17256v2
Dynamic semantic VSLAM with known and unknown objects,"Traditional Visual Simultaneous Localization and Mapping (VSLAM) systems
assume a static environment, which makes them ineffective in highly dynamic
settings. To overcome this, many approaches integrate semantic information from
deep learning models to identify dynamic regions within images. However, these
methods face a significant limitation as a supervised model cannot recognize
objects not included in the training datasets. This paper introduces a novel
feature-based Semantic VSLAM capable of detecting dynamic features in the
presence of both known and unknown objects. By employing an unsupervised
segmentation network, we achieve unlabeled segmentation, and next utilize an
objector detector to identify any of the known classes among those. We then
pair this with the computed high-gradient optical-flow information to next
identify the static versus dynamic segmentations for both known and unknown
object classes. A consistency check module is also introduced for further
refinement and final classification into static versus dynamic features.
Evaluations using public datasets demonstrate that our method offers superior
performance than traditional VSLAM when unknown objects are present in the
images while still matching the performance of the leading semantic VSLAM
techniques when the images contain only the known objects",['cs.CV'],http://arxiv.org/abs/2412.14359v1
SurgSora: Decoupled RGBD-Flow Diffusion Model for Controllable Surgical Video Generation,"Medical video generation has transformative potential for enhancing surgical
understanding and pathology insights through precise and controllable visual
representations. However, current models face limitations in controllability
and authenticity. To bridge this gap, we propose SurgSora, a
motion-controllable surgical video generation framework that uses a single
input frame and user-controllable motion cues. SurgSora consists of three key
modules: the Dual Semantic Injector (DSI), which extracts object-relevant RGB
and depth features from the input frame and integrates them with segmentation
cues to capture detailed spatial features of complex anatomical structures; the
Decoupled Flow Mapper (DFM), which fuses optical flow with semantic-RGB-D
features at multiple scales to enhance temporal understanding and object
spatial dynamics; and the Trajectory Controller (TC), which allows users to
specify motion directions and estimates sparse optical flow, guiding the video
generation process. The fused features are used as conditions for a frozen
Stable Diffusion model to produce realistic, temporally coherent surgical
videos. Extensive evaluations demonstrate that SurgSora outperforms
state-of-the-art methods in controllability and authenticity, showing its
potential to advance surgical video generation for medical education, training,
and research.","['cs.CV', 'cs.AI', 'cs.MM', 'cs.RO']",http://arxiv.org/abs/2412.14018v1
CompactFlowNet: Efficient Real-time Optical Flow Estimation on Mobile Devices,"We present CompactFlowNet, the first real-time mobile neural network for
optical flow prediction, which involves determining the displacement of each
pixel in an initial frame relative to the corresponding pixel in a subsequent
frame. Optical flow serves as a fundamental building block for various
video-related tasks, such as video restoration, motion estimation, video
stabilization, object tracking, action recognition, and video generation. While
current state-of-the-art methods prioritize accuracy, they often overlook
constraints regarding speed and memory usage. Existing light models typically
focus on reducing size but still exhibit high latency, compromise significantly
on quality, or are optimized for high-performance GPUs, resulting in
sub-optimal performance on mobile devices. This study aims to develop a
mobile-optimized optical flow model by proposing a novel mobile
device-compatible architecture, as well as enhancements to the training
pipeline, which optimize the model for reduced weight, low memory utilization,
and increased speed while maintaining minimal error. Our approach demonstrates
superior or comparable performance to the state-of-the-art lightweight models
on the challenging KITTI and Sintel benchmarks. Furthermore, it attains a
significantly accelerated inference speed, thereby yielding real-time
operational efficiency on the iPhone 8, while surpassing real-time performance
levels on more advanced mobile devices.",['cs.CV'],http://arxiv.org/abs/2412.13273v1
GG-SSMs: Graph-Generating State Space Models,"State Space Models (SSMs) are powerful tools for modeling sequential data in
computer vision and time series analysis domains. However, traditional SSMs are
limited by fixed, one-dimensional sequential processing, which restricts their
ability to model non-local interactions in high-dimensional data. While methods
like Mamba and VMamba introduce selective and flexible scanning strategies,
they rely on predetermined paths, which fails to efficiently capture complex
dependencies. We introduce Graph-Generating State Space Models (GG-SSMs), a
novel framework that overcomes these limitations by dynamically constructing
graphs based on feature relationships. Using Chazelle's Minimum Spanning Tree
algorithm, GG-SSMs adapt to the inherent data structure, enabling robust
feature propagation across dynamically generated graphs and efficiently
modeling complex dependencies. We validate GG-SSMs on 11 diverse datasets,
including event-based eye-tracking, ImageNet classification, optical flow
estimation, and six time series datasets. GG-SSMs achieve state-of-the-art
performance across all tasks, surpassing existing methods by significant
margins. Specifically, GG-SSM attains a top-1 accuracy of 84.9% on ImageNet,
outperforming prior SSMs by 1%, reducing the KITTI-15 error rate to 2.77%, and
improving eye-tracking detection rates by up to 0.33% with fewer parameters.
These results demonstrate that dynamic scanning based on feature relationships
significantly improves SSMs' representational power and efficiency, offering a
versatile tool for various applications in computer vision and beyond.",['cs.LG'],http://arxiv.org/abs/2412.12423v1
Spatiotemporal Blind-Spot Network with Calibrated Flow Alignment for Self-Supervised Video Denoising,"Self-supervised video denoising aims to remove noise from videos without
relying on ground truth data, leveraging the video itself to recover clean
frames. Existing methods often rely on simplistic feature stacking or apply
optical flow without thorough analysis. This results in suboptimal utilization
of both inter-frame and intra-frame information, and it also neglects the
potential of optical flow alignment under self-supervised conditions, leading
to biased and insufficient denoising outcomes. To this end, we first explore
the practicality of optical flow in the self-supervised setting and introduce a
SpatioTemporal Blind-spot Network (STBN) for global frame feature utilization.
In the temporal domain, we utilize bidirectional blind-spot feature propagation
through the proposed blind-spot alignment block to ensure accurate temporal
alignment and effectively capture long-range dependencies. In the spatial
domain, we introduce the spatial receptive field expansion module, which
enhances the receptive field and improves global perception capabilities.
Additionally, to reduce the sensitivity of optical flow estimation to noise, we
propose an unsupervised optical flow distillation mechanism that refines
fine-grained inter-frame interactions during optical flow alignment. Our method
demonstrates superior performance across both synthetic and real-world video
denoising datasets. The source code is publicly available at
https://github.com/ZKCCZ/STBN.",['cs.CV'],http://arxiv.org/abs/2412.11820v1
Exploring More from Multiple Gait Modalities for Human Identification,"The gait, as a kind of soft biometric characteristic, can reflect the
distinct walking patterns of individuals at a distance, exhibiting a promising
technique for unrestrained human identification. With largely excluding
gait-unrelated cues hidden in RGB videos, the silhouette and skeleton, though
visually compact, have acted as two of the most prevailing gait modalities for
a long time. Recently, several attempts have been made to introduce more
informative data forms like human parsing and optical flow images to capture
gait characteristics, along with multi-branch architectures. However, due to
the inconsistency within model designs and experiment settings, we argue that a
comprehensive and fair comparative study among these popular gait modalities,
involving the representational capacity and fusion strategy exploration, is
still lacking. From the perspectives of fine vs. coarse-grained shape and whole
vs. pixel-wise motion modeling, this work presents an in-depth investigation of
three popular gait representations, i.e., silhouette, human parsing, and
optical flow, with various fusion evaluations, and experimentally exposes their
similarities and differences. Based on the obtained insights, we further
develop a C$^2$Fusion strategy, consequently building our new framework
MultiGait++. C$^2$Fusion preserves commonalities while highlighting differences
to enrich the learning of gait features. To verify our findings and
conclusions, extensive experiments on Gait3D, GREW, CCPG, and SUSTech1K are
conducted. The code is available at https://github.com/ShiqiYu/OpenGait.",['cs.CV'],http://arxiv.org/abs/2412.11495v1
Learning Normal Flow Directly From Event Neighborhoods,"Event-based motion field estimation is an important task. However, current
optical flow methods face challenges: learning-based approaches, often
frame-based and relying on CNNs, lack cross-domain transferability, while
model-based methods, though more robust, are less accurate. To address the
limitations of optical flow estimation, recent works have focused on normal
flow, which can be more reliably measured in regions with limited texture or
strong edges. However, existing normal flow estimators are predominantly
model-based and suffer from high errors.
  In this paper, we propose a novel supervised point-based method for normal
flow estimation that overcomes the limitations of existing event learning-based
approaches. Using a local point cloud encoder, our method directly estimates
per-event normal flow from raw events, offering multiple unique advantages: 1)
It produces temporally and spatially sharp predictions. 2) It supports more
diverse data augmentation, such as random rotation, to improve robustness
across various domains. 3) It naturally supports uncertainty quantification via
ensemble inference, which benefits downstream tasks. 4) It enables training and
inference on undistorted data in normalized camera coordinates, improving
transferability across cameras. Extensive experiments demonstrate our method
achieves better and more consistent performance than state-of-the-art methods
when transferred across different datasets. Leveraging this transferability, we
train our model on the union of datasets and release it for public use.
Finally, we introduce an egomotion solver based on a maximum-margin problem
that uses normal flow and IMU to achieve strong performance in challenging
scenarios.",['cs.CV'],http://arxiv.org/abs/2412.11284v1
Video Diffusion Models are Strong Video Inpainter,"Propagation-based video inpainting using optical flow at the pixel or feature
level has recently garnered significant attention. However, it has limitations
such as the inaccuracy of optical flow prediction and the propagation of noise
over time. These issues result in non-uniform noise and time consistency
problems throughout the video, which are particularly pronounced when the
removed area is large and involves substantial movement. To address these
issues, we propose a novel First Frame Filling Video Diffusion Inpainting model
(FFF-VDI). We design FFF-VDI inspired by the capabilities of pre-trained
image-to-video diffusion models that can transform the first frame image into a
highly natural video. To apply this to the video inpainting task, we propagate
the noise latent information of future frames to fill the masked areas of the
first frame's noise latent code. Next, we fine-tune the pre-trained
image-to-video diffusion model to generate the inpainted video. The proposed
model addresses the limitations of existing methods that rely on optical flow
quality, producing much more natural and temporally consistent videos. This
proposed approach is the first to effectively integrate image-to-video
diffusion models into video inpainting tasks. Through various comparative
experiments, we demonstrate that the proposed model can robustly handle diverse
inpainting types with high quality.",['cs.CV'],http://arxiv.org/abs/2408.11402v3
eCARLA-scenes: A synthetically generated dataset for event-based optical flow prediction,"The joint use of event-based vision and Spiking Neural Networks (SNNs) is
expected to have a large impact in robotics in the near future, in tasks such
as, visual odometry and obstacle avoidance. While researchers have used
real-world event datasets for optical flow prediction (mostly captured with
Unmanned Aerial Vehicles (UAVs)), these datasets are limited in diversity,
scalability, and are challenging to collect. Thus, synthetic datasets offer a
scalable alternative by bridging the gap between reality and simulation. In
this work, we address the lack of datasets by introducing eWiz, a comprehensive
library for processing event-based data. It includes tools for data loading,
augmentation, visualization, encoding, and generation of training data, along
with loss functions and performance metrics. We further present a synthetic
event-based datasets and data generation pipelines for optical flow prediction
tasks. Built on top of eWiz, eCARLA-scenes makes use of the CARLA simulator to
simulate self-driving car scenarios. The ultimate goal of this dataset is the
depiction of diverse environments while laying a foundation for advancing
event-based camera applications in autonomous field vehicle navigation, paving
the way for using SNNs on neuromorphic hardware such as the Intel Loihi.","['cs.CV', 'I.2.5; I.2.6; I.2.9; I.2.10']",http://arxiv.org/abs/2412.09209v1
ResFlow: Fine-tuning Residual Optical Flow for Event-based High Temporal Resolution Motion Estimation,"Event cameras hold significant promise for high-temporal-resolution (HTR)
motion estimation. However, estimating event-based HTR optical flow faces two
key challenges: the absence of HTR ground-truth data and the intrinsic sparsity
of event data. Most existing approaches rely on the flow accumulation paradigms
to indirectly supervise intermediate flows, often resulting in accumulation
errors and optimization difficulties. To address these challenges, we propose a
residual-based paradigm for estimating HTR optical flow with event data. Our
approach separates HTR flow estimation into two stages: global linear motion
estimation and HTR residual flow refinement. The residual paradigm effectively
mitigates the impacts of event sparsity on optimization and is compatible with
any LTR algorithm. Next, to address the challenge posed by the absence of HTR
ground truth, we incorporate novel learning strategies. Specifically, we
initially employ a shared refiner to estimate the residual flows, enabling both
LTR supervision and HTR inference. Subsequently, we introduce regional noise to
simulate the residual patterns of intermediate flows, facilitating the
adaptation from LTR supervision to HTR inference. Additionally, we show that
the noise-based strategy supports in-domain self-supervised training.
Comprehensive experimental results demonstrate that our approach achieves
state-of-the-art accuracy in both LTR and HTR metrics, highlighting its
effectiveness and superiority.",['cs.CV'],http://arxiv.org/abs/2412.09105v1
Mojito: Motion Trajectory and Intensity Control for Video Generation,"Recent advancements in diffusion models have shown great promise in producing
high-quality video content. However, efficiently training diffusion models
capable of integrating directional guidance and controllable motion intensity
remains a challenging and under-explored area. This paper introduces Mojito, a
diffusion model that incorporates both \textbf{Mo}tion tra\textbf{j}ectory and
\textbf{i}ntensi\textbf{t}y contr\textbf{o}l for text to video generation.
Specifically, Mojito features a Directional Motion Control module that
leverages cross-attention to efficiently direct the generated object's motion
without additional training, alongside a Motion Intensity Modulator that uses
optical flow maps generated from videos to guide varying levels of motion
intensity. Extensive experiments demonstrate Mojito's effectiveness in
achieving precise trajectory and intensity control with high computational
efficiency, generating motion patterns that closely match specified directions
and intensities, providing realistic dynamics that align well with natural
motion in real-world scenarios.","['cs.CV', 'cs.CL']",http://arxiv.org/abs/2412.08948v1
Labits: Layered Bidirectional Time Surfaces Representation for Event Camera-based Continuous Dense Trajectory Estimation,"Event cameras provide a compelling alternative to traditional frame-based
sensors, capturing dynamic scenes with high temporal resolution and low
latency. Moving objects trigger events with precise timestamps along their
trajectory, enabling smooth continuous-time estimation. However, few works have
attempted to optimize the information loss during event representation
construction, imposing a ceiling on this task. Fully exploiting event cameras
requires representations that simultaneously preserve fine-grained temporal
information, stable and characteristic 2D visual features, and temporally
consistent information density, an unmet challenge in existing representations.
We introduce Labits: Layered Bidirectional Time Surfaces, a simple yet elegant
representation designed to retain all these features. Additionally, we propose
a dedicated module for extracting active pixel local optical flow (APLOF),
significantly boosting the performance. Our approach achieves an impressive 49%
reduction in trajectory end-point error (TEPE) compared to the previous
state-of-the-art on the MultiFlow dataset. The code will be released upon
acceptance.","['cs.CV', 'cs.AI', 'cs.ET']",http://arxiv.org/abs/2412.08849v1
Static-Dynamic Class-level Perception Consistency in Video Semantic Segmentation,"Video semantic segmentation(VSS) has been widely employed in lots of fields,
such as simultaneous localization and mapping, autonomous driving and
surveillance. Its core challenge is how to leverage temporal information to
achieve better segmentation. Previous efforts have primarily focused on
pixel-level static-dynamic contexts matching, utilizing techniques such as
optical flow and attention mechanisms. Instead, this paper rethinks
static-dynamic contexts at the class level and proposes a novel static-dynamic
class-level perceptual consistency (SD-CPC) framework. In this framework, we
propose multivariate class prototype with contrastive learning and a
static-dynamic semantic alignment module. The former provides class-level
constraints for the model, obtaining personalized inter-class features and
diversified intra-class features. The latter first establishes intra-frame
spatial multi-scale and multi-level correlations to achieve static semantic
alignment. Then, based on cross-frame static perceptual differences, it
performs two-stage cross-frame selective aggregation to achieve dynamic
semantic alignment. Meanwhile, we propose a window-based attention map
calculation method that leverages the sparsity of attention points during
cross-frame aggregation to reduce computation cost. Extensive experiments on
VSPW and Cityscapes datasets show that the proposed approach outperforms
state-of-the-art methods. Our implementation will be open-sourced on GitHub.",['cs.CV'],http://arxiv.org/abs/2412.08034v1
CMRNext: Camera to LiDAR Matching in the Wild for Localization and Extrinsic Calibration,"LiDARs are widely used for mapping and localization in dynamic environments.
However, their high cost limits their widespread adoption. On the other hand,
monocular localization in LiDAR maps using inexpensive cameras is a
cost-effective alternative for large-scale deployment. Nevertheless, most
existing approaches struggle to generalize to new sensor setups and
environments, requiring retraining or fine-tuning. In this paper, we present
CMRNext, a novel approach for camera-LIDAR matching that is independent of
sensor-specific parameters, generalizable, and can be used in the wild for
monocular localization in LiDAR maps and camera-LiDAR extrinsic calibration.
CMRNext exploits recent advances in deep neural networks for matching
cross-modal data and standard geometric techniques for robust pose estimation.
We reformulate the point-pixel matching problem as an optical flow estimation
problem and solve the Perspective-n-Point problem based on the resulting
correspondences to find the relative pose between the camera and the LiDAR
point cloud. We extensively evaluate CMRNext on six different robotic
platforms, including three publicly available datasets and three in-house
robots. Our experimental evaluations demonstrate that CMRNext outperforms
existing approaches on both tasks and effectively generalizes to previously
unseen environments and sensor setups in a zero-shot manner. We make the code
and pre-trained models publicly available at http://cmrnext.cs.uni-freiburg.de .","['cs.CV', 'cs.RO']",http://arxiv.org/abs/2402.00129v4
Breaking The Ice: Video Segmentation for Close-Range Ice-Covered Waters,"Rapid ice recession in the Arctic Ocean, with predictions of ice-free summers
by 2060, opens new maritime routes but requires reliable navigation solutions.
Current approaches rely heavily on subjective expert judgment, underscoring the
need for automated, data-driven solutions. This study leverages machine
learning to assess ice conditions using ship-borne optical data, introducing a
finely annotated dataset of 946 images, and a semi-manual, region-based
annotation technique. The proposed video segmentation model, UPerFlow, advances
the SegFlow architecture by incorporating a six-channel ResNet encoder, two
UPerNet-based segmentation decoders for each image, PWCNet as the optical flow
encoder, and cross-connections that integrate bi-directional flow features
without loss of latent information. The proposed architecture outperforms
baseline image segmentation networks by an average 38% in occluded regions,
demonstrating the robustness of video segmentation in addressing challenging
Arctic conditions.",['cs.CV'],http://arxiv.org/abs/2411.05225v4
EvRepSL: Event-Stream Representation via Self-Supervised Learning for Event-Based Vision,"Event-stream representation is the first step for many computer vision tasks
using event cameras. It converts the asynchronous event-streams into a
formatted structure so that conventional machine learning models can be applied
easily. However, most of the state-of-the-art event-stream representations are
manually designed and the quality of these representations cannot be guaranteed
due to the noisy nature of event-streams. In this paper, we introduce a
data-driven approach aiming at enhancing the quality of event-stream
representations. Our approach commences with the introduction of a new
event-stream representation based on spatial-temporal statistics, denoted as
EvRep. Subsequently, we theoretically derive the intrinsic relationship between
asynchronous event-streams and synchronous video frames. Building upon this
theoretical relationship, we train a representation generator, RepGen, in a
self-supervised learning manner accepting EvRep as input. Finally, the
event-streams are converted to high-quality representations, termed as EvRepSL,
by going through the learned RepGen (without the need of fine-tuning or
retraining). Our methodology is rigorously validated through extensive
evaluations on a variety of mainstream event-based classification and optical
flow datasets (captured with various types of event cameras). The experimental
results highlight not only our approach's superior performance over existing
event-stream representations but also its versatility, being agnostic to
different event cameras and tasks.","['cs.CV', 'cs.AI', 'cs.MM']",http://arxiv.org/abs/2412.07080v1
Generalized Closed-form Formulae for Feature-based Subpixel Alignment in Patch-based Matching,"Cost-based image patch matching is at the core of various techniques in
computer vision, photogrammetry and remote sensing. When the subpixel disparity
between the reference patch in the source and target images is required, either
the cost function or the target image have to be interpolated. While cost-based
interpolation is the easiest to implement, multiple works have shown that image
based interpolation can increase the accuracy of the subpixel matching, but
usually at the cost of expensive search procedures. This, however, is
problematic, especially for very computation intensive applications such as
stereo matching or optical flow computation. In this paper, we show that closed
form formulae for subpixel disparity computation for the case of one
dimensional matching, e.g., in the case of rectified stereo images where the
search space is of one dimension, exists when using the standard NCC, SSD and
SAD cost functions. We then demonstrate how to generalize the proposed formulae
to the case of high dimensional search spaces, which is required for
unrectified stereo matching and optical flow extraction. We also compare our
results with traditional cost volume interpolation formulae as well as with
state-of-the-art cost-based refinement methods, and show that the proposed
formulae bring a small improvement over the state-of-the-art cost-based methods
in the case of one dimensional search spaces, and a significant improvement
when the search space is two dimensional.","['cs.CV', 'I.4.8']",http://arxiv.org/abs/2112.00941v3
ICANet: A Method of Short Video Emotion Recognition Driven by Multimodal Data,"With the fast development of artificial intelligence and short videos,
emotion recognition in short videos has become one of the most important
research topics in human-computer interaction. At present, most emotion
recognition methods still stay in a single modality. However, in daily life,
human beings will usually disguise their real emotions, which leads to the
problem that the accuracy of single modal emotion recognition is relatively
terrible. Moreover, it is not easy to distinguish similar emotions. Therefore,
we propose a new approach denoted as ICANet to achieve multimodal short video
emotion recognition by employing three different modalities of audio, video and
optical flow, making up for the lack of a single modality and then improving
the accuracy of emotion recognition in short videos. ICANet has a better
accuracy of 80.77% on the IEMOCAP benchmark, exceeding the SOTA methods by
15.89%.",['cs.CV'],http://arxiv.org/abs/2208.11346v2
Local Attention Transformers for High-Detail Optical Flow Upsampling,"Most recent works on optical flow use convex upsampling as the last step to
obtain high-resolution flow. In this work, we show and discuss several issues
and limitations of this currently widely adopted convex upsampling approach. We
propose a series of changes, in an attempt to resolve current issues. First, we
propose to decouple the weights for the final convex upsampler, making it
easier to find the correct convex combination. For the same reason, we also
provide extra contextual features to the convex upsampler. Then, we increase
the convex mask size by using an attention-based alternative convex upsampler;
Transformers for Convex Upsampling. This upsampler is based on the observation
that convex upsampling can be reformulated as attention, and we propose to use
local attention masks as a drop-in replacement for convex masks to increase the
mask size. We provide empirical evidence that a larger mask size increases the
likelihood of the existence of the convex combination. Lastly, we propose an
alternative training scheme to remove bilinear interpolation artifacts from the
model output. Our proposed ideas could theoretically be applied to almost every
current state-of-the-art optical flow architecture. On the FlyingChairs +
FlyingThings3D training setting we reduce the Sintel Clean training
end-point-error of RAFT from 1.42 to 1.26, GMA from 1.31 to 1.18, and that of
FlowFormer from 0.94 to 0.90, by solely adapting the convex upsampler.",['cs.CV'],http://arxiv.org/abs/2412.06439v1
MotionStone: Decoupled Motion Intensity Modulation with Diffusion Transformer for Image-to-Video Generation,"The image-to-video (I2V) generation is conditioned on the static image, which
has been enhanced recently by the motion intensity as an additional control
signal. These motion-aware models are appealing to generate diverse motion
patterns, yet there lacks a reliable motion estimator for training such models
on large-scale video set in the wild. Traditional metrics, e.g., SSIM or
optical flow, are hard to generalize to arbitrary videos, while, it is very
tough for human annotators to label the abstract motion intensity neither.
Furthermore, the motion intensity shall reveal both local object motion and
global camera movement, which has not been studied before. This paper addresses
the challenge with a new motion estimator, capable of measuring the decoupled
motion intensities of objects and cameras in video. We leverage the contrastive
learning on randomly paired videos and distinguish the video with greater
motion intensity. Such a paradigm is friendly for annotation and easy to scale
up to achieve stable performance on motion estimation. We then present a new
I2V model, named MotionStone, developed with the decoupled motion estimator.
Experimental results demonstrate the stability of the proposed motion estimator
and the state-of-the-art performance of MotionStone on I2V generation. These
advantages warrant the decoupled motion estimator to serve as a general plug-in
enhancer for both data processing and video generation training.",['cs.CV'],http://arxiv.org/abs/2412.05848v1
DeNVeR: Deformable Neural Vessel Representations for Unsupervised Video Vessel Segmentation,"This paper presents Deformable Neural Vessel Representations (DeNVeR), an
unsupervised approach for vessel segmentation in X-ray angiography videos
without annotated ground truth. DeNVeR utilizes optical flow and layer
separation techniques, enhancing segmentation accuracy and adaptability through
test-time training. Key contributions include a novel layer separation
bootstrapping technique, a parallel vessel motion loss, and the integration of
Eulerian motion fields for modeling complex vessel dynamics. A significant
component of this research is the introduction of the XACV dataset, the first
X-ray angiography coronary video dataset with high-quality, manually labeled
segmentation ground truth. Extensive evaluations on both XACV and CADICA
datasets demonstrate that DeNVeR outperforms current state-of-the-art methods
in vessel segmentation accuracy and generalization capability while maintaining
temporal coherency. See our project page for video results at
https://kirito878.github.io/DeNVeR/.",['cs.CV'],http://arxiv.org/abs/2406.01591v3
Generalized Multi-hop Traffic Pressure for Heterogeneous Traffic Perimeter Control,"Perimeter control (PC) prevents loss of traffic network capacity due to
congestion in urban areas. Homogeneous PC allows all access points to a
protected region to have identical permitted inflow. However, homogeneous PC
performs poorly when the congestion in the protected region is heterogeneous
(e.g., imbalanced demand) since the homogeneous PC does not consider specific
traffic conditions around each perimeter intersection. When the protected
region has spatially heterogeneous congestion, one needs to modulate the
perimeter inflow rate to be higher near low-density regions and vice versa for
high-density regions. A na\""ive approach is to leverage 1-hop traffic pressure
to measure traffic condition around perimeter intersections, but such metric is
too spatially myopic for PC. To address this issue, we formulate multi-hop
downstream pressure grounded on Markov chain theory, which ``looks deeper''
into the protected region beyond perimeter intersections. In addition, we
formulate a two-stage hierarchical control scheme that can leverage this novel
multi-hop pressure to redistribute the total permitted inflow provided by a
pre-trained deep reinforcement learning homogeneous control policy.
Experimental results show that our heterogeneous PC approaches leveraging
multi-hop pressure significantly outperform homogeneous PC in scenarios where
the origin-destination flows are highly imbalanced with high spatial
heterogeneity. Moveover, our approach is shown to be robust against turning
ratio uncertainties by a sensitivity analysis.","['cs.LG', 'cs.SY', 'eess.SY']",http://arxiv.org/abs/2409.00753v2
Towards Large Reasoning Models: A Survey on Scaling LLM Reasoning Capabilities,"Language has long been conceived as an essential tool for human reasoning.
The breakthrough of Large Language Models (LLMs) has sparked significant
research interest in leveraging these models to tackle complex reasoning tasks.
Researchers have moved beyond simple autoregressive token generation by
introducing the concept of ""thought"" -- a sequence of tokens representing
intermediate steps in the reasoning process. This innovative paradigm enables
LLMs' to mimic complex human reasoning processes, such as tree search and
reflective thinking. Recently, an emerging trend of learning to reason has
applied reinforcement learning (RL) to train LLMs to master reasoning
processes. This approach enables the automatic generation of high-quality
reasoning trajectories through trial-and-error search algorithms, significantly
expanding LLMs' reasoning capacity by providing substantially more training
data. Furthermore, recent studies demonstrate that encouraging LLMs to ""think""
with more tokens during test-time inference can further significantly boost
reasoning accuracy. Therefore, the train-time and test-time scaling combined to
show a new research frontier -- a path toward Large Reasoning Model. The
introduction of OpenAI's o1 series marks a significant milestone in this
research direction. In this survey, we present a comprehensive review of recent
progress in LLM reasoning. We begin by introducing the foundational background
of LLMs and then explore the key technical components driving the development
of large reasoning models, with a focus on automated data construction,
learning-to-reason techniques, and test-time scaling. We also analyze popular
open-source projects at building large reasoning models, and conclude with open
challenges and future research directions.","['cs.AI', 'cs.CL']",http://arxiv.org/abs/2501.09686v2
The Animal-AI Environment: A Virtual Laboratory For Comparative Cognition and Artificial Intelligence Research,"The Animal-AI Environment is a unique game-based research platform designed
to facilitate collaboration between the artificial intelligence and comparative
cognition research communities. In this paper, we present the latest version of
the Animal-AI Environment, outlining several major features that make the game
more engaging for humans and more complex for AI systems. These features
include interactive buttons, reward dispensers, and player notifications, as
well as an overhaul of the environment's graphics and processing for
significant improvements in agent training time and quality of the human player
experience. We provide detailed guidance on how to build computational and
behavioural experiments with the Animal-AI Environment. We present results from
a series of agents, including the state-of-the-art deep reinforcement learning
agent Dreamer-v3, on newly designed tests and the Animal-AI Testbed of 900
tasks inspired by research in the field of comparative cognition. The Animal-AI
Environment offers a new approach for modelling cognition in humans and
non-human animals, and for building biologically inspired artificial
intelligence.",['cs.AI'],http://arxiv.org/abs/2312.11414v3
Enhancing UAV Path Planning Efficiency Through Accelerated Learning,"Unmanned Aerial Vehicles (UAVs) are increasingly essential in various fields
such as surveillance, reconnaissance, and telecommunications. This study aims
to develop a learning algorithm for the path planning of UAV wireless
communication relays, which can reduce storage requirements and accelerate Deep
Reinforcement Learning (DRL) convergence. Assuming the system possesses terrain
maps of the area and can estimate user locations using localization algorithms
or direct GPS reporting, it can input these parameters into the learning
algorithms to achieve optimized path planning performance. However, higher
resolution terrain maps are necessary to extract topological information such
as terrain height, object distances, and signal blockages. This requirement
increases memory and storage demands on UAVs while also lengthening convergence
times in DRL algorithms. Similarly, defining the telecommunication coverage map
in UAV wireless communication relays using these terrain maps and user position
estimations demands higher memory and storage utilization for the learning path
planning algorithms. Our approach reduces path planning training time by
applying a dimensionality reduction technique based on Principal Component
Analysis (PCA), sample combination, Prioritized Experience Replay (PER), and
the combination of Mean Squared Error (MSE) and Mean Absolute Error (MAE) loss
calculations in the coverage map estimates, thereby enhancing a Twin Delayed
Deep Deterministic Policy Gradient (TD3) algorithm. The proposed solution
reduces the convergence episodes needed for basic training by approximately
four times compared to the traditional TD3.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2501.10141v1
ForestProtector: An IoT Architecture Integrating Machine Vision and Deep Reinforcement Learning for Efficient Wildfire Monitoring,"Early detection of forest fires is crucial to minimizing the environmental
and socioeconomic damage they cause. Indeed, a fire's duration directly
correlates with the difficulty and cost of extinguishing it. For instance, a
fire burning for 1 minute might require 1 liter of water to extinguish, while a
2-minute fire could demand 100 liters, and a 10-minute fire might necessitate
1,000 liters. On the other hand, existing fire detection systems based on novel
technologies (e.g., remote sensing, PTZ cameras, UAVs) are often expensive and
require human intervention, making continuous monitoring of large areas
impractical. To address this challenge, this work proposes a low-cost forest
fire detection system that utilizes a central gateway device with computer
vision capabilities to monitor a 360{\deg} field of view for smoke at long
distances. A deep reinforcement learning agent enhances surveillance by
dynamically controlling the camera's orientation, leveraging real-time sensor
data (smoke levels, ambient temperature, and humidity) from distributed IoT
devices. This approach enables automated wildfire monitoring across expansive
areas while reducing false positives.","['cs.AI', 'cs.CV']",http://arxiv.org/abs/2501.09926v1
From Explainability to Interpretability: Interpretable Policies in Reinforcement Learning Via Model Explanation,"Deep reinforcement learning (RL) has shown remarkable success in complex
domains, however, the inherent black box nature of deep neural network policies
raises significant challenges in understanding and trusting the decision-making
processes. While existing explainable RL methods provide local insights, they
fail to deliver a global understanding of the model, particularly in
high-stakes applications. To overcome this limitation, we propose a novel
model-agnostic approach that bridges the gap between explainability and
interpretability by leveraging Shapley values to transform complex deep RL
policies into transparent representations. The proposed approach offers two key
contributions: a novel approach employing Shapley values to policy
interpretation beyond local explanations and a general framework applicable to
off-policy and on-policy algorithms. We evaluate our approach with three
existing deep RL algorithms and validate its performance in two classic control
environments. The results demonstrate that our approach not only preserves the
original models' performance but also generates more stable interpretable
policies.","['cs.LG', 'cs.AI', 'cs.SY', 'eess.SY']",http://arxiv.org/abs/2501.09858v1
Multi-hop Upstream Anticipatory Traffic Signal Control with Deep Reinforcement Learning,"Coordination in traffic signal control is crucial for managing congestion in
urban networks. Existing pressure-based control methods focus only on immediate
upstream links, leading to suboptimal green time allocation and increased
network delays. However, effective signal control inherently requires
coordination across a broader spatial scope, as the effect of upstream traffic
should influence signal control decisions at downstream intersections,
impacting a large area in the traffic network. Although agent communication
using neural network-based feature extraction can implicitly enhance spatial
awareness, it significantly increases the learning complexity, adding an
additional layer of difficulty to the challenging task of control in deep
reinforcement learning. To address the issue of learning complexity and myopic
traffic pressure definition, our work introduces a novel concept based on
Markov chain theory, namely \textit{multi-hop upstream pressure}, which
generalizes the conventional pressure to account for traffic conditions beyond
the immediate upstream links. This farsighted and compact metric informs the
deep reinforcement learning agent to preemptively clear the multi-hop upstream
queues, guiding the agent to optimize signal timings with a broader spatial
awareness. Simulations on synthetic and realistic (Toronto) scenarios
demonstrate controllers utilizing multi-hop upstream pressure significantly
reduce overall network delay by prioritizing traffic movements based on a
broader understanding of upstream congestion.","['cs.LG', 'cs.AI', 'cs.SY', 'eess.SY', 'math.PR']",http://arxiv.org/abs/2411.07271v2
Reinforcement learning with non-ergodic reward increments: robustness via ergodicity transformations,"Envisioned application areas for reinforcement learning (RL) include
autonomous driving, precision agriculture, and finance, which all require RL
agents to make decisions in the real world. A significant challenge hindering
the adoption of RL methods in these domains is the non-robustness of
conventional algorithms. In particular, the focus of RL is typically on the
expected value of the return. The expected value is the average over the
statistical ensemble of infinitely many trajectories, which can be
uninformative about the performance of the average individual. For instance,
when we have a heavy-tailed return distribution, the ensemble average can be
dominated by rare extreme events. Consequently, optimizing the expected value
can lead to policies that yield exceptionally high returns with a probability
that approaches zero but almost surely result in catastrophic outcomes in
single long trajectories. In this paper, we develop an algorithm that lets RL
agents optimize the long-term performance of individual trajectories. The
algorithm enables the agents to learn robust policies, which we show in an
instructive example with a heavy-tailed return distribution and standard RL
benchmarks. The key element of the algorithm is a transformation that we learn
from data. This transformation turns the time series of collected returns into
one for whose increments expected value and the average over a long trajectory
coincide. Optimizing these increments results in robust policies.",['cs.LG'],http://arxiv.org/abs/2310.11335v3
VideoWorld: Exploring Knowledge Learning from Unlabeled Videos,"This work explores whether a deep generative model can learn complex
knowledge solely from visual input, in contrast to the prevalent focus on
text-based models like large language models (LLMs). We develop VideoWorld, an
auto-regressive video generation model trained on unlabeled video data, and
test its knowledge acquisition abilities in video-based Go and robotic control
tasks. Our experiments reveal two key findings: (1) video-only training
provides sufficient information for learning knowledge, including rules,
reasoning and planning capabilities, and (2) the representation of visual
change is crucial for knowledge acquisition. To improve both the efficiency and
efficacy of this process, we introduce the Latent Dynamics Model (LDM) as a key
component of VideoWorld. Remarkably, VideoWorld reaches a 5-dan professional
level in the Video-GoBench with just a 300-million-parameter model, without
relying on search algorithms or reward mechanisms typical in reinforcement
learning. In robotic tasks, VideoWorld effectively learns diverse control
operations and generalizes across environments, approaching the performance of
oracle models in CALVIN and RLBench. This study opens new avenues for knowledge
acquisition from visual data, with all code, data, and models open-sourced for
further research.",['cs.CV'],http://arxiv.org/abs/2501.09781v1
Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment,"Recent advances in large language models (LLMs) have demonstrated significant
progress in performing complex tasks. While Reinforcement Learning from Human
Feedback (RLHF) has been effective in aligning LLMs with human preferences, it
is susceptible to spurious correlations in reward modeling. Consequently, it
often introduces biases-such as length bias, sycophancy, conceptual bias, and
discrimination that hinder the model's ability to capture true causal
relationships. To address this, we propose a novel causal reward modeling
approach that integrates causal inference to mitigate these spurious
correlations. Our method enforces counterfactual invariance, ensuring reward
predictions remain consistent when irrelevant variables are altered. Through
experiments on both synthetic and real-world datasets, we show that our
approach mitigates various types of spurious correlations effectively,
resulting in more reliable and fair alignment of LLMs with human preferences.
As a drop-in enhancement to the existing RLHF workflow, our causal reward
modeling provides a practical way to improve the trustworthiness and fairness
of LLM finetuning.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2501.09620v1
Hybrid Approaches for Moral Value Alignment in AI Agents: a Manifesto,"Increasing interest in ensuring the safety of next-generation Artificial
Intelligence (AI) systems calls for novel approaches to embedding morality into
autonomous agents. This goal differs qualitatively from traditional
task-specific AI methodologies. In this paper, we provide a systematization of
existing approaches to the problem of introducing morality in machines -
modelled as a continuum. Our analysis suggests that popular techniques lie at
the extremes of this continuum - either being fully hard-coded into top-down,
explicit rules, or entirely learned in a bottom-up, implicit fashion with no
direct statement of any moral principle (this includes learning from human
feedback, as applied to the training and finetuning of large language models,
or LLMs). Given the relative strengths and weaknesses of each type of
methodology, we argue that more hybrid solutions are needed to create adaptable
and robust, yet controllable and interpretable agentic systems. To that end,
this paper discusses both the ethical foundations (including deontology,
consequentialism and virtue ethics) and implementations of morally aligned AI
systems.
  We present a series of case studies that rely on intrinsic rewards, moral
constraints or textual instructions, applied to either pure-Reinforcement
Learning or LLM-based agents. By analysing these diverse implementations under
one framework, we compare their relative strengths and shortcomings in
developing morally aligned AI systems. We then discuss strategies for
evaluating the effectiveness of moral learning agents. Finally, we present open
research questions and implications for the future of AI safety and ethics
which are emerging from this hybrid framework.","['cs.AI', 'cs.CY', 'cs.LG', 'cs.MA']",http://arxiv.org/abs/2312.01818v3
EVaDE : Event-Based Variational Thompson Sampling for Model-Based Reinforcement Learning,"Posterior Sampling for Reinforcement Learning (PSRL) is a well-known
algorithm that augments model-based reinforcement learning (MBRL) algorithms
with Thompson sampling. PSRL maintains posterior distributions of the
environment transition dynamics and the reward function, which are intractable
for tasks with high-dimensional state and action spaces. Recent works show that
dropout, used in conjunction with neural networks, induces variational
distributions that can approximate these posteriors. In this paper, we propose
Event-based Variational Distributions for Exploration (EVaDE), which are
variational distributions that are useful for MBRL, especially when the
underlying domain is object-based. We leverage the general domain knowledge of
object-based domains to design three types of event-based convolutional layers
to direct exploration. These layers rely on Gaussian dropouts and are inserted
between the layers of the deep neural network model to help facilitate
variational Thompson sampling. We empirically show the effectiveness of
EVaDE-equipped Simulated Policy Learning (EVaDE-SimPLe) on the 100K Atari game
suite.",['cs.LG'],http://arxiv.org/abs/2501.09611v1
Provably Efficient Reinforcement Learning with Multinomial Logit Function Approximation,"We study a new class of MDPs that employs multinomial logit (MNL) function
approximation to ensure valid probability distributions over the state space.
Despite its significant benefits, incorporating the non-linear function raises
substantial challenges in both statistical and computational efficiency. The
best-known result of Hwang and Oh [2023] has achieved an
$\widetilde{\mathcal{O}}(\kappa^{-1}dH^2\sqrt{K})$ regret upper bound, where
$\kappa$ is a problem-dependent quantity, $d$ is the feature dimension, $H$ is
the episode length, and $K$ is the number of episodes. However, we observe that
$\kappa^{-1}$ exhibits polynomial dependence on the number of reachable states,
which can be as large as the state space size in the worst case and thus
undermines the motivation for function approximation. Additionally, their
method requires storing all historical data and the time complexity scales
linearly with the episode count, which is computationally expensive. In this
work, we propose a statistically efficient algorithm that achieves a regret of
$\widetilde{\mathcal{O}}(dH^2\sqrt{K} + \kappa^{-1}d^2H^2)$, eliminating the
dependence on $\kappa^{-1}$ in the dominant term for the first time. We then
address the computational challenges by introducing an enhanced algorithm that
achieves the same regret guarantee but with only constant cost. Finally, we
establish the first lower bound for this problem, justifying the optimality of
our results in $d$ and $K$.",['cs.LG'],http://arxiv.org/abs/2405.17061v3
Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion,"Reinforcement Learning (RL) has been used to finetune Large Language Models
(LLMs) using a reward model trained from preference data, to better align with
human judgment. The recently introduced direct alignment methods, which are
often simpler, more stable, and computationally lighter, can more directly
achieve this. However, these approaches cannot optimize arbitrary rewards, and
the preference-based ones are not the only rewards of interest for LLMs (eg.,
unit tests for code generation or textual entailment for summarization, among
others). RL-finetuning is usually done with a variation of policy gradient,
which calls for on-policy or near-on-policy samples, requiring costly
generations. We introduce Contrastive Policy Gradient, or CoPG, a simple and
mathematically principled new RL algorithm that can estimate the optimal policy
even from off-policy data. It can be seen as an off-policy policy gradient
approach that does not rely on important sampling techniques and highlights the
importance of using (the right) state baseline. We show this approach to
generalize the direct alignment method IPO (identity preference optimization)
and classic policy gradient. We experiment with the proposed CoPG on a toy
bandit problem to illustrate its properties, as well as for finetuning LLMs on
a summarization task, using a learned reward function considered as ground
truth for the purpose of the experiments.",['cs.LG'],http://arxiv.org/abs/2406.19185v2
Fast Searching of Extreme Operating Conditions for Relay Protection Setting Calculation Based on Graph Neural Network and Reinforcement Learning,"Searching for the Extreme Operating Conditions (EOCs) is one of the core
problems of power system relay protection setting calculation. The current
methods based on brute-force search, heuristic algorithms, and mathematical
programming can hardly meet the requirements of today's power systems in terms
of computation speed due to the drastic changes in operating conditions induced
by renewables and power electronics. This paper proposes an EOC fast search
method, named Graph Dueling Double Deep Q Network (Graph D3QN), which combines
graph neural network and deep reinforcement learning to address this challenge.
First, the EOC search problem is modeled as a Markov decision process, where
the information of the underlying power system is extracted using graph neural
networks, so that the EOC of the system can be found via deep reinforcement
learning. Then, a two-stage Guided Learning and Free Exploration (GLFE)
training framework is constructed to accelerate the convergence speed of
reinforcement learning. Finally, the proposed Graph D3QN method is validated
through case studies of searching maximum fault current for relay protection
setting calculation on the IEEE 39-bus and 118-bus systems. The experimental
results demonstrate that Graph D3QN can reduce the computation time by 10 to
1000 times while guaranteeing the accuracy of the selected EOCs.",['cs.LG'],http://arxiv.org/abs/2501.09399v1
Deterministic Uncertainty Propagation for Improved Model-Based Offline Reinforcement Learning,"Current approaches to model-based offline reinforcement learning often
incorporate uncertainty-based reward penalization to address the distributional
shift problem. These approaches, commonly known as pessimistic value iteration,
use Monte Carlo sampling to estimate the Bellman target to perform temporal
difference-based policy evaluation. We find out that the randomness caused by
this sampling step significantly delays convergence. We present a theoretical
result demonstrating the strong dependency of suboptimality on the number of
Monte Carlo samples taken per Bellman target calculation. Our main contribution
is a deterministic approximation to the Bellman target that uses progressive
moment matching, a method developed originally for deterministic variational
inference. The resulting algorithm, which we call Moment Matching Offline
Model-Based Policy Optimization (MOMBO), propagates the uncertainty of the next
state through a nonlinear Q-network in a deterministic fashion by approximating
the distributions of hidden layer activations by a normal distribution. We show
that it is possible to provide tighter guarantees for the suboptimality of
MOMBO than the existing Monte Carlo sampling approaches. We also observe MOMBO
to converge faster than these approaches in a large set of benchmark tasks.",['cs.LG'],http://arxiv.org/abs/2406.04088v3
Balancing Act: Prioritization Strategies for LLM-Designed Restless Bandit Rewards,"LLMs are increasingly used to design reward functions based on human
preferences in Reinforcement Learning (RL). We focus on LLM-designed rewards
for Restless Multi-Armed Bandits, a framework for allocating limited resources
among agents. In applications such as public health, this approach empowers
grassroots health workers to tailor automated allocation decisions to community
needs. In the presence of multiple agents, altering the reward function based
on human preferences can impact subpopulations very differently, leading to
complex tradeoffs and a multi-objective resource allocation problem. We are the
first to present a principled method termed Social Choice Language Model for
dealing with these tradeoffs for LLM-designed rewards for multiagent planners
in general and restless bandits in particular. The novel part of our model is a
transparent and configurable selection component, called an adjudicator,
external to the LLM that controls complex tradeoffs via a user-selected social
welfare function. Our experiments demonstrate that our model reliably selects
more effective, aligned, and balanced reward functions compared to purely
LLM-based approaches.","['cs.LG', 'cs.AI', 'cs.MA']",http://arxiv.org/abs/2408.12112v3
Learning to Assist Humans without Inferring Rewards,"Assistive agents should make humans' lives easier. Classically, such
assistance is studied through the lens of inverse reinforcement learning, where
an assistive agent (e.g., a chatbot, a robot) infers a human's intention and
then selects actions to help the human reach that goal. This approach requires
inferring intentions, which can be difficult in high-dimensional settings. We
build upon prior work that studies assistance through the lens of empowerment:
an assistive agent aims to maximize the influence of the human's actions such
that they exert a greater control over the environmental outcomes and can solve
tasks in fewer steps. We lift the major limitation of prior work in this
area--scalability to high-dimensional settings--with contrastive successor
representations. We formally prove that these representations estimate a
similar notion of empowerment to that studied by prior work and provide a
ready-made mechanism for optimizing it. Empirically, our proposed method
outperforms prior methods on synthetic benchmarks, and scales to Overcooked, a
cooperative game setting. Theoretically, our work connects ideas from
information theory, neuroscience, and reinforcement learning, and charts a path
for representations to play a critical role in solving assistive problems.","['cs.AI', 'cs.CY', 'cs.HC', 'cs.LG']",http://arxiv.org/abs/2411.02623v3
The surprising efficiency of temporal difference learning for rare event prediction,"We quantify the efficiency of temporal difference (TD) learning over the
direct, or Monte Carlo (MC), estimator for policy evaluation in reinforcement
learning, with an emphasis on estimation of quantities related to rare events.
Policy evaluation is complicated in the rare event setting by the long
timescale of the event and by the need for \emph{relative accuracy} in
estimates of very small values. Specifically, we focus on least-squares TD
(LSTD) prediction for finite state Markov chains, and show that LSTD can
achieve relative accuracy far more efficiently than MC. We prove a central
limit theorem for the LSTD estimator and upper bound the \emph{relative
asymptotic variance} by simple quantities characterizing the connectivity of
states relative to the transition probabilities between them. Using this bound,
we show that, even when both the timescale of the rare event and the relative
accuracy of the MC estimator are exponentially large in the number of states,
LSTD maintains a fixed level of relative accuracy with a total number of
observed transitions of the Markov chain that is only \emph{polynomially} large
in the number of states.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2405.17638v3
Statistical Efficiency of Distributional Temporal Difference Learning and Freedman's Inequality in Hilbert Spaces,"Distributional reinforcement learning (DRL) has achieved empirical success in
various domains. One core task in DRL is distributional policy evaluation,
which involves estimating the return distribution $\eta^\pi$ for a given policy
$\pi$. Distributional temporal difference learning has been accordingly
proposed, which extends the classic temporal difference learning (TD) in RL. In
this paper, we focus on the non-asymptotic statistical rates of distributional
TD. To facilitate theoretical analysis, we propose non-parametric
distributional TD (NTD). For a $\gamma$-discounted infinite-horizon tabular
Markov decision process, we show that for NTD with a generative model, we need
$\tilde{O}(\varepsilon^{-2}\mu_{\min}^{-1}(1-\gamma)^{-3})$ interactions with
the environment to achieve an $\varepsilon$-optimal estimator with high
probability, when the estimation error is measured by the $1$-Wasserstein. This
sample complexity bound is minimax optimal up to logarithmic factors. In
addition, we revisit categorical distributional TD (CTD), showing that the same
non-asymptotic convergence bounds hold for CTD in the case of the
$1$-Wasserstein distance. We also extend our analysis to the more general
setting where the data generating process is Markovian. In the Markovian
setting, we propose variance-reduced variants of NTD and CTD, and show that
both can achieve a $\tilde{O}(\varepsilon^{-2}
\mu_{\pi,\min}^{-1}(1-\gamma)^{-3}+t_{mix}\mu_{\pi,\min}^{-1}(1-\gamma)^{-1})$
sample complexity bounds in the case of the $1$-Wasserstein distance, which
matches the state-of-the-art statistical results for classic policy evaluation.
To achieve the sharp statistical rates, we establish a novel Freedman's
inequality in Hilbert spaces. This new Freedman's inequality would be of
independent interest for statistical analysis of various infinite-dimensional
online learning problems.","['stat.ML', 'cs.LG']",http://arxiv.org/abs/2403.05811v4
Clone-Robust AI Alignment,"A key challenge in training Large Language Models (LLMs) is properly aligning
them with human preferences. Reinforcement Learning with Human Feedback (RLHF)
uses pairwise comparisons from human annotators to train reward functions and
has emerged as a popular alignment method. However, input datasets in RLHF are
not necessarily balanced in the types of questions and answers that are
included. Therefore, we want RLHF algorithms to perform well even when the set
of alternatives is not uniformly distributed. Drawing on insights from social
choice theory, we introduce robustness to approximate clones, a desirable
property of RLHF algorithms which requires that adding near-duplicate
alternatives does not significantly change the learned reward function. We
first demonstrate that the standard RLHF algorithm based on regularized maximum
likelihood estimation (MLE) fails to satisfy this property. We then propose the
weighted MLE, a new RLHF algorithm that modifies the standard regularized MLE
by weighting alternatives based on their similarity to other alternatives. This
new algorithm guarantees robustness to approximate clones while preserving
desirable theoretical properties.","['cs.LG', 'cs.AI', 'cs.GT']",http://arxiv.org/abs/2501.09254v1
SuperNeRF-GAN: A Universal 3D-Consistent Super-Resolution Framework for Efficient and Enhanced 3D-Aware Image Synthesis,"Neural volume rendering techniques, such as NeRF, have revolutionized
3D-aware image synthesis by enabling the generation of images of a single scene
or object from various camera poses. However, the high computational cost of
NeRF presents challenges for synthesizing high-resolution (HR) images. Most
existing methods address this issue by leveraging 2D super-resolution, which
compromise 3D-consistency. Other methods propose radiance manifolds or
two-stage generation to achieve 3D-consistent HR synthesis, yet they are
limited to specific synthesis tasks, reducing their universality. To tackle
these challenges, we propose SuperNeRF-GAN, a universal framework for
3D-consistent super-resolution. A key highlight of SuperNeRF-GAN is its
seamless integration with NeRF-based 3D-aware image synthesis methods and it
can simultaneously enhance the resolution of generated images while preserving
3D-consistency and reducing computational cost. Specifically, given a
pre-trained generator capable of producing a NeRF representation such as
tri-plane, we first perform volume rendering to obtain a low-resolution image
with corresponding depth and normal map. Then, we employ a NeRF
Super-Resolution module which learns a network to obtain a high-resolution
NeRF. Next, we propose a novel Depth-Guided Rendering process which contains
three simple yet effective steps, including the construction of a
boundary-correct multi-depth map through depth aggregation, a normal-guided
depth super-resolution and a depth-guided NeRF rendering. Experimental results
demonstrate the superior efficiency, 3D-consistency, and quality of our
approach. Additionally, ablation studies confirm the effectiveness of our
proposed components.",['cs.CV'],http://arxiv.org/abs/2501.06770v2
StructSR: Refuse Spurious Details in Real-World Image Super-Resolution,"Diffusion-based models have shown great promise in real-world image
super-resolution (Real-ISR), but often generate content with structural errors
and spurious texture details due to the empirical priors and illusions of these
models. To address this issue, we introduce StructSR, a simple, effective, and
plug-and-play method that enhances structural fidelity and suppresses spurious
details for diffusion-based Real-ISR. StructSR operates without the need for
additional fine-tuning, external model priors, or high-level semantic
knowledge. At its core is the Structure-Aware Screening (SAS) mechanism, which
identifies the image with the highest structural similarity to the
low-resolution (LR) input in the early inference stage, allowing us to leverage
it as a historical structure knowledge to suppress the generation of spurious
details. By intervening in the diffusion inference process, StructSR seamlessly
integrates with existing diffusion-based Real-ISR models. Our experimental
results demonstrate that StructSR significantly improves the fidelity of
structure and texture, improving the PSNR and SSIM metrics by an average of
5.27% and 9.36% on a synthetic dataset (DIV2K-Val) and 4.13% and 8.64% on two
real-world datasets (RealSR and DRealSR) when integrated with four
state-of-the-art diffusion-based Real-ISR methods.",['cs.CV'],http://arxiv.org/abs/2501.05777v2
Bias for Action: Video Implicit Neural Representations with Bias Modulation,"We propose a new continuous video modeling framework based on implicit neural
representations (INRs) called ActINR. At the core of our approach is the
observation that INRs can be considered as a learnable dictionary, with the
shapes of the basis functions governed by the weights of the INR, and their
locations governed by the biases. Given compact non-linear activation
functions, we hypothesize that an INR's biases are suitable to capture motion
across images, and facilitate compact representations for video sequences.
Using these observations, we design ActINR to share INR weights across frames
of a video sequence, while using unique biases for each frame. We further model
the biases as the output of a separate INR conditioned on time index to promote
smoothness. By training the video INR and this bias INR together, we
demonstrate unique capabilities, including $10\times$ video slow motion,
$4\times$ spatial super resolution along with $2\times$ slow motion, denoising,
and video inpainting. ActINR performs remarkably well across numerous video
processing tasks (often achieving more than 6dB improvement), setting a new
standard for continuous modeling of videos.",['cs.CV'],http://arxiv.org/abs/2501.09277v1
Deep learning for temporal super-resolution 4D Flow MRI,"4D Flow Magnetic Resonance Imaging (4D Flow MRI) is a non-invasive technique
for volumetric, time-resolved blood flow quantification. However, apparent
trade-offs between acquisition time, image noise, and resolution limit clinical
applicability. In particular, in regions of highly transient flow, coarse
temporal resolution can hinder accurate capture of physiologically relevant
flow variations. To overcome these issues, post-processing techniques using
deep learning have shown promising results to enhance resolution post-scan
using so-called super-resolution networks. However, while super-resolution has
been focusing on spatial upsampling, temporal super-resolution remains largely
unexplored. The aim of this study was therefore to implement and evaluate a
residual network for temporal super-resolution 4D Flow MRI. To achieve this, an
existing spatial network (4DFlowNet) was re-designed for temporal upsampling,
adapting input dimensions, and optimizing internal layer structures. Training
and testing were performed using synthetic 4D Flow MRI data originating from
patient-specific in-silico models, as well as using in-vivo datasets. Overall,
excellent performance was achieved with input velocities effectively denoised
and temporally upsampled, with a mean absolute error (MAE) of 1.0 cm/s in an
unseen in-silico setting, outperforming deterministic alternatives (linear
interpolation MAE = 2.3 cm/s, sinc interpolation MAE = 2.6 cm/s). Further, the
network synthesized high-resolution temporal information from unseen
low-resolution in-vivo data, with strong correlation observed at peak flow
frames. As such, our results highlight the potential of utilizing data-driven
neural networks for temporal super-resolution 4D Flow MRI, enabling
high-frame-rate flow quantification without extending acquisition times beyond
clinically acceptable limits.",['cs.LG'],http://arxiv.org/abs/2501.08780v1
"State-of-the-Art Transformer Models for Image Super-Resolution: Techniques, Challenges, and Applications","Image Super-Resolution (SR) aims to recover a high-resolution image from its
low-resolution counterpart, which has been affected by a specific degradation
process. This is achieved by enhancing detail and visual quality. Recent
advancements in transformer-based methods have remolded image super-resolution
by enabling high-quality reconstructions surpassing previous deep-learning
approaches like CNN and GAN-based. This effectively addresses the limitations
of previous methods, such as limited receptive fields, poor global context
capture, and challenges in high-frequency detail recovery. Additionally, the
paper reviews recent trends and advancements in transformer-based SR models,
exploring various innovative techniques and architectures that combine
transformers with traditional networks to balance global and local contexts.
These neoteric methods are critically analyzed, revealing promising yet
unexplored gaps and potential directions for future research. Several
visualizations of models and techniques are included to foster a holistic
understanding of recent trends. This work seeks to offer a structured roadmap
for researchers at the forefront of deep learning, specifically exploring the
impact of transformers on super-resolution techniques.","['cs.CV', 'cs.AI', 'cs.ET', 'cs.LG', 'cs.NE']",http://arxiv.org/abs/2501.07855v1
C2PD: Continuity-Constrained Pixelwise Deformation for Guided Depth Super-Resolution,"Guided depth super-resolution (GDSR) has demonstrated impressive performance
across a wide range of domains, with numerous methods being proposed. However,
existing methods often treat depth maps as images, where shading values are
computed discretely, making them struggle to effectively restore the continuity
inherent in the depth map. In this paper, we propose a novel approach that
maximizes the utilization of spatial characteristics in depth, coupled with
human abstract perception of real-world substance, by transforming the GDSR
issue into deformation of a roughcast with ideal plasticity, which can be
deformed by force like a continuous object. Specifically, we firstly designed a
cross-modal operation, Continuity-constrained Asymmetrical Pixelwise Operation
(CAPO), which can mimic the process of deforming an isovolumetrically flexible
object through external forces. Utilizing CAPO as the fundamental component, we
develop the Pixelwise Cross Gradient Deformation (PCGD), which is capable of
emulating operations on ideal plastic objects (without volume constraint).
Notably, our approach demonstrates state-of-the-art performance across four
widely adopted benchmarks for GDSR, with significant advantages in large-scale
tasks and generalizability.",['cs.CV'],http://arxiv.org/abs/2501.07688v1
Diff-Ensembler: Learning to Ensemble 2D Diffusion Models for Volume-to-Volume Medical Image Translation,"Despite success in volume-to-volume translations in medical images, most
existing models struggle to effectively capture the inherent volumetric
distribution using 3D representations. The current state-of-the-art approach
combines multiple 2D-based networks through weighted averaging, thereby
neglecting the 3D spatial structures. Directly training 3D models in medical
imaging presents significant challenges due to high computational demands and
the need for large-scale datasets. To address these challenges, we introduce
Diff-Ensembler, a novel hybrid 2D-3D model for efficient and effective
volumetric translations by ensembling perpendicularly trained 2D diffusion
models with a 3D network in each diffusion step. Moreover, our model can
naturally be used to ensemble diffusion models conditioned on different
modalities, allowing flexible and accurate fusion of input conditions.
Extensive experiments demonstrate that Diff-Ensembler attains superior accuracy
and volumetric realism in 3D medical image super-resolution and modality
translation. We further demonstrate the strength of our model's volumetric
realism using tumor segmentation as a downstream task.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.07430v1
Multi-Label Scene Classification in Remote Sensing Benefits from Image Super-Resolution,"Satellite imagery is a cornerstone for numerous Remote Sensing (RS)
applications; however, limited spatial resolution frequently hinders the
precision of such systems, especially in multi-label scene classification tasks
as it requires a higher level of detail and feature differentiation. In this
study, we explore the efficacy of image Super-Resolution (SR) as a
pre-processing step to enhance the quality of satellite images and thus improve
downstream classification performance. We investigate four SR models -
SRResNet, HAT, SeeSR, and RealESRGAN - and evaluate their impact on multi-label
scene classification across various CNN architectures, including ResNet-50,
ResNet-101, ResNet-152, and Inception-v4. Our results show that applying SR
significantly improves downstream classification performance across various
metrics, demonstrating its ability to preserve spatial details critical for
multi-label tasks. Overall, this work offers valuable insights into the
selection of SR techniques for multi-label prediction in remote sensing and
presents an easy-to-integrate framework to improve existing RS systems.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.06720v1
Physics-Informed Super-Resolution Diffusion for 6D Phase Space Diagnostics,"Adaptive physics-informed super-resolution diffusion is developed for
non-invasive virtual diagnostics of the 6D phase space density of charged
particle beams. An adaptive variational autoencoder (VAE) embeds initial beam
condition images and scalar measurements to a low-dimensional latent space from
which a 326 pixel 6D tensor representation of the beam's 6D phase space density
is generated. Projecting from a 6D tensor generates physically consistent 2D
projections. Physics-guided super-resolution diffusion transforms
low-resolution images of the 6D density to high resolution 256x256 pixel
images. Un-supervised adaptive latent space tuning enables tracking of
time-varying beams without knowledge of time-varying initial conditions. The
method is demonstrated with experimental data and multi-particle simulations at
the HiRES UED. The general approach is applicable to a wide range of complex
dynamic systems evolving in high-dimensional phase space. The method is shown
to be robust to distribution shift without re-training.","['cs.LG', 'math.DS', 'physics.acc-ph']",http://arxiv.org/abs/2501.04305v2
Enhancing Sample Generation of Diffusion Models using Noise Level Correction,"The denoising process of diffusion models can be interpreted as an
approximate projection of noisy samples onto the data manifold. Moreover, the
noise level in these samples approximates their distance to the underlying
manifold. Building on this insight, we propose a novel method to enhance sample
generation by aligning the estimated noise level with the true distance of
noisy samples to the manifold. Specifically, we introduce a noise level
correction network, leveraging a pre-trained denoising network, to refine noise
level estimates during the denoising process. Additionally, we extend this
approach to various image restoration tasks by integrating task-specific
constraints, including inpainting, deblurring, super-resolution, colorization,
and compressed sensing. Experimental results demonstrate that our method
significantly improves sample quality in both unconstrained and constrained
generation scenarios. Notably, the proposed noise level correction framework is
compatible with existing denoising schedulers (e.g., DDIM), offering additional
performance improvements.","['cs.CV', 'cs.LG', 'eess.IV']",http://arxiv.org/abs/2412.05488v2
STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution,"Image diffusion models have been adapted for real-world video
super-resolution to tackle over-smoothing issues in GAN-based methods. However,
these models struggle to maintain temporal consistency, as they are trained on
static images, limiting their ability to capture temporal dynamics effectively.
Integrating text-to-video (T2V) models into video super-resolution for improved
temporal modeling is straightforward. However, two key challenges remain:
artifacts introduced by complex degradations in real-world scenarios, and
compromised fidelity due to the strong generative capacity of powerful T2V
models (\textit{e.g.}, CogVideoX-5B). To enhance the spatio-temporal quality of
restored videos, we introduce\textbf{~\name}
(\textbf{S}patial-\textbf{T}emporal \textbf{A}ugmentation with T2V models for
\textbf{R}eal-world video super-resolution), a novel approach that leverages
T2V models for real-world video super-resolution, achieving realistic spatial
details and robust temporal consistency. Specifically, we introduce a Local
Information Enhancement Module (LIEM) before the global attention block to
enrich local details and mitigate degradation artifacts. Moreover, we propose a
Dynamic Frequency (DF) Loss to reinforce fidelity, guiding the model to focus
on different frequency components across diffusion steps. Extensive experiments
demonstrate\textbf{~\name}~outperforms state-of-the-art methods on both
synthetic and real-world datasets.",['cs.CV'],http://arxiv.org/abs/2501.02976v1
Conditional Mutual Information Based Diffusion Posterior Sampling for Solving Inverse Problems,"Inverse problems are prevalent across various disciplines in science and
engineering. In the field of computer vision, tasks such as inpainting,
deblurring, and super-resolution are commonly formulated as inverse problems.
Recently, diffusion models (DMs) have emerged as a promising approach for
addressing noisy linear inverse problems, offering effective solutions without
requiring additional task-specific training. Specifically, with the prior
provided by DMs, one can sample from the posterior by finding the likelihood.
Since the likelihood is intractable, it is often approximated in the
literature. However, this approximation compromises the quality of the
generated images. To overcome this limitation and improve the effectiveness of
DMs in solving inverse problems, we propose an information-theoretic approach.
Specifically, we maximize the conditional mutual information
$\mathrm{I}(\boldsymbol{x}_0; \boldsymbol{y} | \boldsymbol{x}_t)$, where
$\boldsymbol{x}_0$ represents the reconstructed signal, $\boldsymbol{y}$ is the
measurement, and $\boldsymbol{x}_t$ is the intermediate signal at stage $t$.
This ensures that the intermediate signals $\boldsymbol{x}_t$ are generated in
a way that the final reconstructed signal $\boldsymbol{x}_0$ retains as much
information as possible about the measurement $\boldsymbol{y}$. We demonstrate
that this method can be seamlessly integrated with recent approaches and, once
incorporated, enhances their performance both qualitatively and quantitatively.","['cs.LG', 'stat.ML']",http://arxiv.org/abs/2501.02880v1
HOGSA: Bimanual Hand-Object Interaction Understanding with 3D Gaussian Splatting Based Data Augmentation,"Understanding of bimanual hand-object interaction plays an important role in
robotics and virtual reality. However, due to significant occlusions between
hands and object as well as the high degree-of-freedom motions, it is
challenging to collect and annotate a high-quality, large-scale dataset, which
prevents further improvement of bimanual hand-object interaction-related
baselines. In this work, we propose a new 3D Gaussian Splatting based data
augmentation framework for bimanual hand-object interaction, which is capable
of augmenting existing dataset to large-scale photorealistic data with various
hand-object pose and viewpoints. First, we use mesh-based 3DGS to model objects
and hands, and to deal with the rendering blur problem due to multi-resolution
input images used, we design a super-resolution module. Second, we extend the
single hand grasping pose optimization module for the bimanual hand object to
generate various poses of bimanual hand-object interaction, which can
significantly expand the pose distribution of the dataset. Third, we conduct an
analysis for the impact of different aspects of the proposed data augmentation
on the understanding of the bimanual hand-object interaction. We perform our
data augmentation on two benchmarks, H2O and Arctic, and verify that our method
can improve the performance of the baselines.",['cs.CV'],http://arxiv.org/abs/2501.02845v1
Advancing Super-Resolution in Neural Radiance Fields via Variational Diffusion Strategies,"We present a novel method for diffusion-guided frameworks for view-consistent
super-resolution (SR) in neural rendering. Our approach leverages existing 2D
SR models in conjunction with advanced techniques such as Variational Score
Distilling (VSD) and a LoRA fine-tuning helper, with spatial training to
significantly boost the quality and consistency of upscaled 2D images compared
to the previous methods in the literature, such as Renoised Score Distillation
(RSD) proposed in DiSR-NeRF (1), or SDS proposed in DreamFusion. The VSD score
facilitates precise fine-tuning of SR models, resulting in high-quality,
view-consistent images. To address the common challenge of inconsistencies
among independent SR 2D images, we integrate Iterative 3D Synchronization
(I3DS) from the DiSR-NeRF framework. Our quantitative benchmarks and
qualitative results on the LLFF dataset demonstrate the superior performance of
our system compared to existing methods such as DiSR-NeRF.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2410.18137v2
Transformer-Driven Inverse Problem Transform for Fast Blind Hyperspectral Image Dehazing,"Hyperspectral dehazing (HyDHZ) has become a crucial signal processing
technology to facilitate the subsequent identification and classification
tasks, as the airborne visible/infrared imaging spectrometer (AVIRIS) data
portal reports a massive portion of haze-corrupted areas in typical
hyperspectral remote sensing images. The idea of inverse problem transform
(IPT) has been proposed in recent remote sensing literature in order to
reformulate a hardly tractable inverse problem (e.g., HyDHZ) into a relatively
simple one. Considering the emerging spectral super-resolution (SSR) technique,
which spectrally upsamples multispectral data to hyperspectral data, we aim to
solve the challenging HyDHZ problem by reformulating it as an SSR problem.
Roughly speaking, the proposed algorithm first automatically selects some
uncorrupted/informative spectral bands, from which SSR is applied to spectrally
upsample the selected bands in the feature space, thereby obtaining a clean
hyperspectral image (HSI). The clean HSI is then further refined by a deep
transformer network to obtain the final dehazed HSI, where a global attention
mechanism is designed to capture nonlocal information. There are very few HyDHZ
works in existing literature, and this article introduces the powerful
spatial-spectral transformer into HyDHZ for the first time. Remarkably, the
proposed transformer-driven IPT-based HyDHZ (T2HyDHZ) is a blind algorithm
without requiring the user to manually select the corrupted region. Extensive
experiments demonstrate the superiority of T2HyDHZ with less color distortion.","['cs.CV', 'eess.IV']",http://arxiv.org/abs/2501.01924v1
Flow Priors for Linear Inverse Problems via Iterative Corrupted Trajectory Matching,"Generative models based on flow matching have attracted significant attention
for their simplicity and superior performance in high-resolution image
synthesis. By leveraging the instantaneous change-of-variables formula, one can
directly compute image likelihoods from a learned flow, making them enticing
candidates as priors for downstream tasks such as inverse problems. In
particular, a natural approach would be to incorporate such image probabilities
in a maximum-a-posteriori (MAP) estimation problem. A major obstacle, however,
lies in the slow computation of the log-likelihood, as it requires
backpropagating through an ODE solver, which can be prohibitively slow for
high-dimensional problems. In this work, we propose an iterative algorithm to
approximate the MAP estimator efficiently to solve a variety of linear inverse
problems. Our algorithm is mathematically justified by the observation that the
MAP objective can be approximated by a sum of $N$ ``local MAP'' objectives,
where $N$ is the number of function evaluations. By leveraging Tweedie's
formula, we show that we can perform gradient steps to sequentially optimize
these objectives. We validate our approach for various linear inverse problems,
such as super-resolution, deblurring, inpainting, and compressed sensing, and
demonstrate that we can outperform other methods based on flow matching. Code
is available at https://github.com/YasminZhang/ICTM.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2405.18816v4
Zero-Shot Image Restoration Using Few-Step Guidance of Consistency Models (and Beyond),"In recent years, it has become popular to tackle image restoration tasks with
a single pretrained diffusion model (DM) and data-fidelity guidance, instead of
training a dedicated deep neural network per task. However, such ""zero-shot""
restoration schemes currently require many Neural Function Evaluations (NFEs)
for performing well, which may be attributed to the many NFEs needed in the
original generative functionality of the DMs. Recently, faster variants of DMs
have been explored for image generation. These include Consistency Models
(CMs), which can generate samples via a couple of NFEs. However, existing works
that use guided CMs for restoration still require tens of NFEs or fine-tuning
of the model per task that leads to performance drop if the assumptions during
the fine-tuning are not accurate. In this paper, we propose a zero-shot
restoration scheme that uses CMs and operates well with as little as 4 NFEs. It
is based on a wise combination of several ingredients: better initialization,
back-projection guidance, and above all a novel noise injection mechanism. We
demonstrate the advantages of our approach for image super-resolution,
deblurring and inpainting. Interestingly, we show that the usefulness of our
noise injection technique goes beyond CMs: it can also mitigate the performance
degradation of existing guided DM methods when reducing their NFE count.",['cs.CV'],http://arxiv.org/abs/2412.20596v1
MaIR: A Locality- and Continuity-Preserving Mamba for Image Restoration,"Recent advancements in Mamba have shown promising results in image
restoration. These methods typically flatten 2D images into multiple distinct
1D sequences along rows and columns, process each sequence independently using
selective scan operation, and recombine them to form the outputs. However, such
a paradigm overlooks two vital aspects: i) the local relationships and spatial
continuity inherent in natural images, and ii) the discrepancies among
sequences unfolded through totally different ways. To overcome the drawbacks,
we explore two problems in Mamba-based restoration methods: i) how to design a
scanning strategy preserving both locality and continuity while facilitating
restoration, and ii) how to aggregate the distinct sequences unfolded in
totally different ways. To address these problems, we propose a novel
Mamba-based Image Restoration model (MaIR), which consists of Nested S-shaped
Scanning strategy (NSS) and Sequence Shuffle Attention block (SSA).
Specifically, NSS preserves locality and continuity of the input images through
the stripe-based scanning region and the S-shaped scanning path, respectively.
SSA aggregates sequences through calculating attention weights within the
corresponding channels of different sequences. Thanks to NSS and SSA, MaIR
surpasses 40 baselines across 14 challenging datasets, achieving
state-of-the-art performance on the tasks of image super-resolution, denoising,
deblurring and dehazing. Our codes will be available after acceptance.",['cs.CV'],http://arxiv.org/abs/2412.20066v1
Enhancing Diffusion Models for Inverse Problems with Covariance-Aware Posterior Sampling,"Inverse problems exist in many disciplines of science and engineering. In
computer vision, for example, tasks such as inpainting, deblurring, and super
resolution can be effectively modeled as inverse problems. Recently, denoising
diffusion probabilistic models (DDPMs) are shown to provide a promising
solution to noisy linear inverse problems without the need for additional task
specific training. Specifically, with the prior provided by DDPMs, one can
sample from the posterior by approximating the likelihood. In the literature,
approximations of the likelihood are often based on the mean of conditional
densities of the reverse process, which can be obtained using Tweedie formula.
To obtain a better approximation to the likelihood, in this paper we first
derive a closed form formula for the covariance of the reverse process. Then,
we propose a method based on finite difference method to approximate this
covariance such that it can be readily obtained from the existing pretrained
DDPMs, thereby not increasing the complexity compared to existing approaches.
Finally, based on the mean and approximated covariance of the reverse process,
we present a new approximation to the likelihood. We refer to this method as
covariance-aware diffusion posterior sampling (CA-DPS). Experimental results
show that CA-DPS significantly improves reconstruction performance without
requiring hyperparameter tuning. The code for the paper is put in the
supplementary materials.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2412.20045v1
General Geospatial Inference with a Population Dynamics Foundation Model,"Supporting the health and well-being of dynamic populations around the world
requires governmental agencies, organizations and researchers to understand and
reason over complex relationships between human behavior and local contexts in
order to identify high-risk groups and strategically allocate limited
resources. Traditional approaches to these classes of problems often entail
developing manually curated, task-specific features and models to represent
human behavior and the natural and built environment, which can be challenging
to adapt to new, or even, related tasks. To address this, we introduce a
Population Dynamics Foundation Model (PDFM) that aims to capture the
relationships between diverse data modalities and is applicable to a broad
range of geospatial tasks. We first construct a geo-indexed dataset for postal
codes and counties across the United States, capturing rich aggregated
information on human behavior from maps, busyness, and aggregated search
trends, and environmental factors such as weather and air quality. We then
model this data and the complex relationships between locations using a graph
neural network, producing embeddings that can be adapted to a wide range of
downstream tasks using relatively simple models. We evaluate the effectiveness
of our approach by benchmarking it on 27 downstream tasks spanning three
distinct domains: health indicators, socioeconomic factors, and environmental
measurements. The approach achieves state-of-the-art performance on all 27
geospatial interpolation tasks, and on 25 out of the 27 extrapolation and
super-resolution tasks. We combined the PDFM with a state-of-the-art
forecasting foundation model, TimesFM, to predict unemployment and poverty,
achieving performance that surpasses fully supervised forecasting. The full set
of embeddings and sample code are publicly available for researchers.","['cs.LG', 'cs.CY']",http://arxiv.org/abs/2411.07207v3
Hybrid Deep Learning Model for epileptic seizure classification by using 1D-CNN with multi-head attention mechanism,"Epilepsy is a prevalent neurological disorder globally, impacting around 50
million people \cite{WHO_epilepsy_50million}. Epileptic seizures result from
sudden abnormal electrical activity in the brain, which can be read as sudden
and significant changes in the EEG signal of the brain. The signal can vary in
severity and frequency, which results in loss of consciousness and muscle
contractions for a short period of time \cite{epilepsyfoundation_myoclonic}.
Individuals with epilepsy often face significant employment challenges due to
safety concerns in certain work environments. Many jobs that involve working at
heights, operating heavy machinery, or in other potentially hazardous settings
may be restricted for people with seizure disorders. This certainly limits job
options and economic opportunities for those living with epilepsy.",['cs.LG'],http://arxiv.org/abs/2501.10342v1
HiMix: Reducing Computational Complexity in Large Vision-Language Models,"Benefiting from recent advancements in large language models and modality
alignment techniques, existing Large Vision-Language Models(LVLMs) have
achieved prominent performance across a wide range of scenarios. However, the
excessive computational complexity limits the widespread use of these models in
practical applications. We argue that one main bottleneck in computational
complexity is caused by the involvement of redundant vision sequences in model
computation. This is inspired by a reassessment of the efficiency of vision and
language information transmission in the language decoder of LVLMs. Then, we
propose a novel hierarchical vision-language interaction mechanism called
Hierarchical Vision injection for Mixture Attention (HiMix). In HiMix, only the
language sequence undergoes full forward propagation, while the vision sequence
interacts with the language at specific stages within each language decoder
layer. It is striking that our approach significantly reduces computational
complexity with minimal performance loss. Specifically, HiMix achieves a 10x
reduction in the computational cost of the language decoder across multiple
LVLM models while maintaining comparable performance. This highlights the
advantages of our method, and we hope our research brings new perspectives to
the field of vision-language understanding. Project Page:
https://xuange923.github.io/HiMix",['cs.CV'],http://arxiv.org/abs/2501.10318v1
An Ontology for Social Determinants of Education (SDoEd) based on Human-AI Collaborative Approach,"The use of computational ontologies is well-established in the field of
Medical Informatics. The topic of Social Determinants of Health (SDoH) has also
received extensive attention. Work at the intersection of ontologies and SDoH
has been published. However, a standardized framework for Social Determinants
of Education (SDoEd) is lacking. In this paper, we are closing the gap by
introducing an SDoEd ontology for creating a precise conceptualization of the
interplay between life circumstances of students and their possible educational
achievements. The ontology was developed utilizing suggestions from
ChatGPT-3.5-010422 and validated using peer-reviewed research articles. The
first version of developed ontology was evaluated by human experts in the field
of education and validated using standard ontology evaluation software. This
version of the SDoEd ontology contains 231 domain concepts, 10 object
properties, and 24 data properties",['cs.AI'],http://arxiv.org/abs/2501.10300v1
Challenges and recommendations for Electronic Health Records data extraction and preparation for dynamic prediction modelling in hospitalized patients -- a practical guide,"Dynamic predictive modeling using electronic health record (EHR) data has
gained significant attention in recent years. The reliability and
trustworthiness of such models depend heavily on the quality of the underlying
data, which is largely determined by the stages preceding the model
development: data extraction from EHR systems and data preparation. We list
over forty challenges encountered during these stages and provide actionable
recommendations for addressing them. These challenges are organized into four
categories: cohort definition, outcome definition, feature engineering, and
data cleaning. This list is designed to serve as a practical guide for data
extraction engineers and researchers, supporting better practices and improving
the quality and real-world applicability of dynamic prediction models in
clinical settings.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2501.10240v1
Boosting drug-disease association prediction for drug repositioning via dual-feature extraction and cross-dual-domain decoding,"The extraction of biomedical data has significant academic and practical
value in contemporary biomedical sciences. In recent years, drug repositioning,
a cost-effective strategy for drug development by discovering new indications
for approved drugs, has gained increasing attention. However, many existing
drug repositioning methods focus on mining information from adjacent nodes in
biomedical networks without considering the potential inter-relationships
between the feature spaces of drugs and diseases. This can lead to inaccurate
encoding, resulting in biased mined drug-disease association information. To
address this limitation, we propose a new model called Dual-Feature Drug
Repurposing Neural Network (DFDRNN). DFDRNN allows the mining of two features
(similarity and association) from the drug-disease biomedical networks to
encode drugs and diseases. A self-attention mechanism is utilized to extract
neighbor feature information. It incorporates two dual-feature extraction
modules: the single-domain dual-feature extraction (SDDFE) module for
extracting features within a single domain (drugs or diseases) and the
cross-domain dual-feature extraction (CDDFE) module for extracting features
across domains. By utilizing these modules, we ensure more appropriate encoding
of drugs and diseases. A cross-dual-domain decoder is also designed to predict
drug-disease associations in both domains. Our proposed DFDRNN model
outperforms six state-of-the-art methods on four benchmark datasets, achieving
an average AUROC of 0.946 and an average AUPR of 0.597. Case studies on two
diseases show that the proposed DFDRNN model can be applied in real-world
scenarios, demonstrating its significant potential in drug repositioning.","['cs.LG', 'q-bio.QM']",http://arxiv.org/abs/2407.11812v4
Generate E-commerce Product Background by Integrating Category Commonality and Personalized Style,"The state-of-the-art methods for e-commerce product background generation
suffer from the inefficiency of designing product-wise prompts when scaling up
the production, as well as the ineffectiveness of describing fine-grained
styles when customizing personalized backgrounds for some specific brands. To
address these obstacles, we integrate the category commonality and personalized
style into diffusion models. Concretely, we propose a Category-Wise Generator
to enable large-scale background generation with only one model for the first
time. A unique identifier in the prompt is assigned to each category, whose
attention is located on the background by a mask-guided cross attention layer
to learn the category-wise style. Furthermore, for products with specific and
fine-grained requirements in layout, elements, etc, a Personality-Wise
Generator is devised to learn such personalized style directly from a reference
image to resolve textual ambiguities, and is trained in a self-supervised
manner for more efficient training data usage. To advance research in this
field, the first large-scale e-commerce product background generation dataset
BG60k is constructed, which covers more than 60k product images from over 2k
categories. Experiments demonstrate that our method could generate high-quality
backgrounds for different categories, and maintain the personalized background
style of reference images. BG60k will be available at
\url{https://github.com/Whileherham/BG60k}.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2312.13309v2
WaveDH: Wavelet Sub-bands Guided ConvNet for Efficient Image Dehazing,"The surge in interest regarding image dehazing has led to notable
advancements in deep learning-based single image dehazing approaches,
exhibiting impressive performance in recent studies. Despite these strides,
many existing methods fall short in meeting the efficiency demands of practical
applications. In this paper, we introduce WaveDH, a novel and compact ConvNet
designed to address this efficiency gap in image dehazing. Our WaveDH leverages
wavelet sub-bands for guided up-and-downsampling and frequency-aware feature
refinement. The key idea lies in utilizing wavelet decomposition to extract
low-and-high frequency components from feature levels, allowing for faster
processing while upholding high-quality reconstruction. The downsampling block
employs a novel squeeze-and-attention scheme to optimize the feature
downsampling process in a structurally compact manner through wavelet domain
learning, preserving discriminative features while discarding noise components.
In our upsampling block, we introduce a dual-upsample and fusion mechanism to
enhance high-frequency component awareness, aiding in the reconstruction of
high-frequency details. Departing from conventional dehazing methods that treat
low-and-high frequency components equally, our feature refinement block
strategically processes features with a frequency-aware approach. By employing
a coarse-to-fine methodology, it not only refines the details at frequency
levels but also significantly optimizes computational costs. The refinement is
performed in a maximum 8x downsampled feature space, striking a favorable
efficiency-vs-accuracy trade-off. Extensive experiments demonstrate that our
method, WaveDH, outperforms many state-of-the-art methods on several image
dehazing benchmarks with significantly reduced computational costs. Our code is
available at https://github.com/AwesomeHwang/WaveDH.","['cs.CV', 'eess.IV']",http://arxiv.org/abs/2404.01604v3
X-Dyna: Expressive Dynamic Human Image Animation,"We introduce X-Dyna, a novel zero-shot, diffusion-based pipeline for
animating a single human image using facial expressions and body movements
derived from a driving video, that generates realistic, context-aware dynamics
for both the subject and the surrounding environment. Building on prior
approaches centered on human pose control, X-Dyna addresses key shortcomings
causing the loss of dynamic details, enhancing the lifelike qualities of human
video animations. At the core of our approach is the Dynamics-Adapter, a
lightweight module that effectively integrates reference appearance context
into the spatial attentions of the diffusion backbone while preserving the
capacity of motion modules in synthesizing fluid and intricate dynamic details.
Beyond body pose control, we connect a local control module with our model to
capture identity-disentangled facial expressions, facilitating accurate
expression transfer for enhanced realism in animated scenes. Together, these
components form a unified framework capable of learning physical human motion
and natural scene dynamics from a diverse blend of human and scene videos.
Comprehensive qualitative and quantitative evaluations demonstrate that X-Dyna
outperforms state-of-the-art methods, creating highly lifelike and expressive
animations. The code is available at https://github.com/bytedance/X-Dyna.",['cs.CV'],http://arxiv.org/abs/2501.10021v1
Textoon: Generating Vivid 2D Cartoon Characters from Text Descriptions,"The 2D cartoon style is a prominent art form in digital character creation,
particularly popular among younger audiences. While advancements in digital
human technology have spurred extensive research into photorealistic digital
humans and 3D characters, interactive 2D cartoon characters have received
comparatively less attention. Unlike 3D counterparts, which require
sophisticated construction and resource-intensive rendering, Live2D, a
widely-used format for 2D cartoon characters, offers a more efficient
alternative, which allows to animate 2D characters in a manner that simulates
3D movement without the necessity of building a complete 3D model. Furthermore,
Live2D employs lightweight HTML5 (H5) rendering, improving both accessibility
and efficiency. In this technical report, we introduce Textoon, an innovative
method for generating diverse 2D cartoon characters in the Live2D format based
on text descriptions. The Textoon leverages cutting-edge language and vision
models to comprehend textual intentions and generate 2D appearance, capable of
creating a wide variety of stunning and interactive 2D characters within one
minute. The project homepage is https://human3daigc.github.io/Textoon_webpage/.",['cs.CV'],http://arxiv.org/abs/2501.10020v1
Myriad: Large Multimodal Model by Applying Vision Experts for Industrial Anomaly Detection,"Due to the training configuration, traditional industrial anomaly detection
(IAD) methods have to train a specific model for each deployment scenario,
which is insufficient to meet the requirements of modern design and
manufacturing. On the contrary, large multimodal models~(LMMs) have shown
eminent generalization ability on various vision tasks, and their perception
and comprehension capabilities imply the potential of applying LMMs on IAD
tasks. However, we observe that even though the LMMs have abundant knowledge
about industrial anomaly detection in the textual domain, the LMMs are unable
to leverage the knowledge due to the modality gap between textual and visual
domains. To stimulate the relevant knowledge in LMMs and adapt the LMMs towards
anomaly detection tasks, we introduce existing IAD methods as vision experts
and present a novel large multimodal model applying vision experts for
industrial anomaly detection~(abbreviated to {Myriad}). Specifically, we
utilize the anomaly map generated by the vision experts as guidance for LMMs,
such that the vision model is guided to pay more attention to anomalous
regions. Then, the visual features are modulated via an adapter to fit the
anomaly detection tasks, which are fed into the language model together with
the vision expert guidance and human instructions to generate the final
outputs. Extensive experiments are applied on MVTec-AD, VisA, and PCB Bank
benchmarks demonstrate that our proposed method not only performs favorably
against state-of-the-art methods, but also inherits the flexibility and
instruction-following ability of LMMs in the field of IAD. Source code and
pre-trained models are publicly available at
\url{https://github.com/tzjtatata/Myriad}.",['cs.CV'],http://arxiv.org/abs/2310.19070v3
Learnable Scaled Gradient Descent for Guaranteed Robust Tensor PCA,"Robust tensor principal component analysis (RTPCA) aims to separate the
low-rank and sparse components from multi-dimensional data, making it an
essential technique in the signal processing and computer vision fields.
Recently emerging tensor singular value decomposition (t-SVD) has gained
considerable attention for its ability to better capture the low-rank structure
of tensors compared to traditional matrix SVD. However, existing methods often
rely on the computationally expensive tensor nuclear norm (TNN), which limits
their scalability for real-world tensors. To address this issue, we explore an
efficient scaled gradient descent (SGD) approach within the t-SVD framework for
the first time, and propose the RTPCA-SGD method. Theoretically, we rigorously
establish the recovery guarantees of RTPCA-SGD under mild assumptions,
demonstrating that with appropriate parameter selection, it achieves linear
convergence to the true low-rank tensor at a constant rate, independent of the
condition number. To enhance its practical applicability, we further propose a
learnable self-supervised deep unfolding model, which enables effective
parameter learning. Numerical experiments on both synthetic and real-world
datasets demonstrate the superior performance of the proposed methods while
maintaining competitive computational efficiency, especially consuming less
time than RTPCA-TNN.",['cs.CV'],http://arxiv.org/abs/2501.04565v2
EraseBench: Understanding The Ripple Effects of Concept Erasure Techniques,"Concept erasure techniques have recently gained significant attention for
their potential to remove unwanted concepts from text-to-image models. While
these methods often demonstrate success in controlled scenarios, their
robustness in real-world applications and readiness for deployment remain
uncertain. In this work, we identify a critical gap in evaluating sanitized
models, particularly in terms of their performance across various concept
dimensions. We systematically investigate the failure modes of current concept
erasure techniques, with a focus on visually similar, binomial, and
semantically related concepts. We propose that these interconnected
relationships give rise to a phenomenon of concept entanglement resulting in
ripple effects and degradation in image quality. To facilitate more
comprehensive evaluation, we introduce EraseBENCH, a multi-dimensional
benchmark designed to assess concept erasure methods with greater depth. Our
dataset includes over 100 diverse concepts and more than 1,000 tailored
prompts, paired with a comprehensive suite of metrics that together offer a
holistic view of erasure efficacy. Our findings reveal that even
state-of-the-art techniques struggle with maintaining quality post-erasure,
indicating that these approaches are not yet ready for real-world deployment.
This highlights the gap in reliability of the concept erasure techniques.",['cs.CV'],http://arxiv.org/abs/2501.09833v1
Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key,"Hallucination remains a major challenge for Large Vision-Language Models
(LVLMs). Direct Preference Optimization (DPO) has gained increasing attention
as a simple solution to hallucination issues. It directly learns from
constructed preference pairs that reflect the severity of hallucinations in
responses to the same prompt and image. Nonetheless, different data
construction methods in existing works bring notable performance variations. We
identify a crucial factor here: outcomes are largely contingent on whether the
constructed data aligns on-policy w.r.t the initial (reference) policy of DPO.
Theoretical analysis suggests that learning from off-policy data is impeded by
the presence of KL-divergence between the updated policy and the reference
policy. From the perspective of dataset distribution, we systematically
summarize the inherent flaws in existing algorithms that employ DPO to address
hallucination issues. To alleviate the problems, we propose On-Policy Alignment
(OPA)-DPO framework, which uniquely leverages expert feedback to correct
hallucinated responses and aligns both the original and expert-revised
responses in an on-policy manner. Notably, with only 4.8k data, OPA-DPO
achieves an additional reduction in the hallucination rate of LLaVA-1.5-7B:
13.26% on the AMBER benchmark and 5.39% on the Object-Hal benchmark, compared
to the previous SOTA algorithm trained with 16k samples.",['cs.CV'],http://arxiv.org/abs/2501.09695v1
Reward-Guided Controlled Generation for Inference-Time Alignment in Diffusion Models: Tutorial and Review,"This tutorial provides an in-depth guide on inference-time guidance and
alignment methods for optimizing downstream reward functions in diffusion
models. While diffusion models are renowned for their generative modeling
capabilities, practical applications in fields such as biology often require
sample generation that maximizes specific metrics (e.g., stability, affinity in
proteins, closeness to target structures). In these scenarios, diffusion models
can be adapted not only to generate realistic samples but also to explicitly
maximize desired measures at inference time without fine-tuning. This tutorial
explores the foundational aspects of such inference-time algorithms. We review
these methods from a unified perspective, demonstrating that current techniques
-- such as Sequential Monte Carlo (SMC)-based guidance, value-based sampling,
and classifier guidance -- aim to approximate soft optimal denoising processes
(a.k.a. policies in RL) that combine pre-trained denoising processes with value
functions serving as look-ahead functions that predict from intermediate states
to terminal rewards. Within this framework, we present several novel algorithms
not yet covered in the literature. Furthermore, we discuss (1) fine-tuning
methods combined with inference-time techniques, (2) inference-time algorithms
based on search algorithms such as Monte Carlo tree search, which have received
limited attention in current research, and (3) connections between
inference-time algorithms in language models and diffusion models. The code of
this tutorial on protein design is available at
https://github.com/masa-ue/AlignInversePro","['cs.AI', 'cs.LG', 'q-bio.QM', 'stat.ML']",http://arxiv.org/abs/2501.09685v1
Neural networks for insurance pricing with frequency and severity data: a benchmark study from data preprocessing to technical tariff,"Insurers usually turn to generalized linear models for modeling claim
frequency and severity data. Due to their success in other fields, machine
learning techniques are gaining popularity within the actuarial toolbox. Our
paper contributes to the literature on frequency-severity insurance pricing
with machine learning via deep learning structures. We present a benchmark
study on four insurance data sets with frequency and severity targets in the
presence of multiple types of input features. We compare in detail the
performance of: a generalized linear model on binned input data, a
gradient-boosted tree model, a feed-forward neural network (FFNN), and the
combined actuarial neural network (CANN). The CANNs combine a baseline
prediction established with a GLM and GBM, respectively, with a neural network
correction. We explain the data preprocessing steps with specific focus on the
multiple types of input features typically present in tabular insurance data
sets, such as postal codes, numeric and categorical covariates. Autoencoders
are used to embed the categorical variables into the neural network, and we
explore their potential advantages in a frequency-severity setting. Model
performance is evaluated not only on out-of-sample deviance but also using
statistical and calibration performance criteria and managerial tools to get
more nuanced insights. Finally, we construct global surrogate models for the
neural nets' frequency and severity models. These surrogates enable the
translation of the essential insights captured by the FFNNs or CANNs to GLMs.
As such, a technical tariff table results that can easily be deployed in
practice.","['cs.LG', 'q-fin.RM']",http://arxiv.org/abs/2310.12671v4
Enhancing Crash Frequency Modeling Based on Augmented Multi-Type Data by Hybrid VAE-Diffusion-Based Generative Neural Networks,"Crash frequency modelling analyzes the impact of factors like traffic volume,
road geometry, and environmental conditions on crash occurrences. Inaccurate
predictions can distort our understanding of these factors, leading to
misguided policies and wasted resources, which jeopardize traffic safety. A key
challenge in crash frequency modelling is the prevalence of excessive zero
observations, caused by underreporting, the low probability of crashes, and
high data collection costs. These zero observations often reduce model accuracy
and introduce bias, complicating safety decision making. While existing
approaches, such as statistical methods, data aggregation, and resampling,
attempt to address this issue, they either rely on restrictive assumptions or
result in significant information loss, distorting crash data. To overcome
these limitations, we propose a hybrid VAE-Diffusion neural network, designed
to reduce zero observations and handle the complexities of multi-type tabular
crash data (count, ordinal, nominal, and real-valued variables). We assess the
synthetic data quality generated by this model through metrics like similarity,
accuracy, diversity, and structural consistency, and compare its predictive
performance against traditional statistical models. Our findings demonstrate
that the hybrid VAE-Diffusion model outperforms baseline models across all
metrics, offering a more effective approach to augmenting crash data and
improving the accuracy of crash frequency predictions. This study highlights
the potential of synthetic data to enhance traffic safety by improving crash
frequency modelling and informing better policy decisions.","['cs.AI', 'cs.DB']",http://arxiv.org/abs/2501.10017v1
Collaborative Gym: A Framework for Enabling and Evaluating Human-Agent Collaboration,"Recent advancements in language models (LMs) have sparked growing interest in
developing LM agents. While fully autonomous agents could excel in many
scenarios, numerous use cases inherently require them to collaborate with
humans due to humans' latent preferences, domain expertise, or need for
control. To facilitate the study of human-agent collaboration, we present
Collaborative Gym (Co-Gym), a general framework enabling asynchronous,
tripartite interaction among agents, humans, and task environments. We
instantiate Co-Gym with three representative tasks in both simulated and
real-world conditions, and propose an evaluation framework that assesses both
the collaboration outcomes and processes. Our findings reveal that
collaborative agents consistently outperform their fully autonomous
counterparts in task performance within those delivered cases, achieving win
rates of 86% in Travel Planning, 74% in Tabular Analysis, and 66% in Related
Work when evaluated by real users. However, our study also highlights
significant challenges in developing collaborative agents, requiring
advancements in core aspects of intelligence -- communication capabilities,
situational awareness, and balancing autonomy and human control.","['cs.AI', 'cs.CL', 'cs.HC']",http://arxiv.org/abs/2412.15701v2
EVAL: EigenVector-based Average-reward Learning,"In reinforcement learning, two objective functions have been developed
extensively in the literature: discounted and averaged rewards. The
generalization to an entropy-regularized setting has led to improved robustness
and exploration for both of these objectives. Recently, the entropy-regularized
average-reward problem was addressed using tools from large deviation theory in
the tabular setting. This method has the advantage of linearity, providing
access to both the optimal policy and average reward-rate through properties of
a single matrix. In this paper, we extend that framework to more general
settings by developing approaches based on function approximation by neural
networks. This formulation reveals new theoretical insights into the
relationship between different objectives used in RL. Additionally, we combine
our algorithm with a posterior policy iteration scheme, showing how our
approach can also solve the average-reward RL problem without
entropy-regularization. Using classic control benchmarks, we experimentally
find that our method compares favorably with other algorithms in terms of
stability and rate of convergence.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2501.09770v1
Better by Default: Strong Pre-Tuned MLPs and Boosted Trees on Tabular Data,"For classification and regression on tabular data, the dominance of
gradient-boosted decision trees (GBDTs) has recently been challenged by often
much slower deep learning methods with extensive hyperparameter tuning. We
address this discrepancy by introducing (a) RealMLP, an improved multilayer
perceptron (MLP), and (b) strong meta-tuned default parameters for GBDTs and
RealMLP. We tune RealMLP and the default parameters on a meta-train benchmark
with 118 datasets and compare them to hyperparameter-optimized versions on a
disjoint meta-test benchmark with 90 datasets, as well as the GBDT-friendly
benchmark by Grinsztajn et al. (2022). Our benchmark results on medium-to-large
tabular datasets (1K--500K samples) show that RealMLP offers a favorable
time-accuracy tradeoff compared to other neural baselines and is competitive
with GBDTs in terms of benchmark scores. Moreover, a combination of RealMLP and
GBDTs with improved default parameters can achieve excellent results without
hyperparameter tuning. Finally, we demonstrate that some of RealMLP's
improvements can also considerably improve the performance of TabR with default
parameters.",['cs.LG'],http://arxiv.org/abs/2407.04491v3
Graph Counterfactual Explainable AI via Latent Space Traversal,"Explaining the predictions of a deep neural network is a nontrivial task, yet
high-quality explanations for predictions are often a prerequisite for
practitioners to trust these models. Counterfactual explanations aim to explain
predictions by finding the ''nearest'' in-distribution alternative input whose
prediction changes in a pre-specified way. However, it remains an open question
how to define this nearest alternative input, whose solution depends on both
the domain (e.g. images, graphs, tabular data, etc.) and the specific
application considered. For graphs, this problem is complicated i) by their
discrete nature, as opposed to the continuous nature of state-of-the-art graph
classifiers; and ii) by the node permutation group acting on the graphs. We
propose a method to generate counterfactual explanations for any differentiable
black-box graph classifier, utilizing a case-specific permutation equivariant
graph variational autoencoder. We generate counterfactual explanations in a
continuous fashion by traversing the latent space of the autoencoder across the
classification boundary of the classifier, allowing for seamless integration of
discrete graph structure and continuous graph attributes. We empirically
validate the approach on three graph datasets, showing that our model is
consistently high-performing and more robust than the baselines.","['cs.LG', 'cs.AI', 'stat.ML']",http://arxiv.org/abs/2501.08850v1
A Closer Look at Deep Learning Methods on Tabular Datasets,"Tabular data is prevalent across diverse domains in machine learning. While
classical methods like tree-based models have long been effective, Deep Neural
Network (DNN)-based methods have recently demonstrated promising performance.
However, the diverse characteristics of methods and the inherent heterogeneity
of tabular datasets make understanding and interpreting tabular methods both
challenging and prone to unstable observations. In this paper, we conduct
in-depth evaluations and comprehensive analyses of tabular methods, with a
particular focus on DNN-based models, using a benchmark of over 300 tabular
datasets spanning a wide range of task types, sizes, and domains. First, we
perform an extensive comparison of 32 state-of-the-art deep and tree-based
methods, evaluating their average performance across multiple criteria.
Although method ranks vary across datasets, we empirically find that
top-performing methods tend to concentrate within a small subset of tabular
models, regardless of the criteria used. Next, we investigate whether the
training dynamics of deep tabular models can be predicted based on dataset
properties. This approach not only offers insights into the behavior of deep
tabular methods but also identifies a core set of ""meta-features"" that reflect
dataset heterogeneity. The other subset includes datasets where method ranks
are consistent with the overall benchmark, acting as a reliable probe for
further tabular analysis.",['cs.LG'],http://arxiv.org/abs/2407.00956v3
Constructing Confidence Intervals for 'the' Generalization Error -- a Comprehensive Benchmark Study,"When assessing the quality of prediction models in machine learning,
confidence intervals (CIs) for the generalization error, which measures
predictive performance, are a crucial tool. Luckily, there exist many methods
for computing such CIs and new promising approaches are continuously being
proposed. Typically, these methods combine various resampling procedures, most
popular among them cross-validation and bootstrapping, with different variance
estimation techniques. Unfortunately, however, there is currently no consensus
on when any of these combinations may be most reliably employed and how they
generally compare. In this work, we conduct a large-scale study comparing CIs
for the generalization error, the first one of such size, where we empirically
evaluate 13 different CI methods on a total of 19 tabular regression and
classification problems, using seven different inducers and a total of eight
loss functions. We give an overview of the methodological foundations and
inherent challenges of constructing CIs for the generalization error and
provide a concise review of all 13 methods in a unified framework. Finally, the
CI methods are evaluated in terms of their relative coverage frequency, width,
and runtime. Based on these findings, we can identify a subset of methods that
we would recommend. We also publish the datasets as a benchmarking suite on
OpenML and our code on GitHub to serve as a basis for further studies.","['stat.ML', 'cs.LG']",http://arxiv.org/abs/2409.18836v2
Unsupervised Feature Construction for Anomaly Detection in Time Series -- An Evaluation,"To detect anomalies with precision and without prior knowledge in time
series, is it better to build a detector from the initial temporal
representation, or to compute a new (tabular) representation using an existing
automatic variable construction library? In this article, we address this
question by conducting an in-depth experimental study for two popular detectors
(Isolation Forest and Local Outlier Factor). The obtained results, for 5
different datasets, show that the new representation, computed using the
tsfresh library, allows Isolation Forest to significantly improve its
performance.",['cs.LG'],http://arxiv.org/abs/2501.07999v1
Reward Compatibility: A Framework for Inverse RL,"We provide an original theoretical study of Inverse Reinforcement Learning
(IRL) through the lens of reward compatibility, a novel framework to quantify
the compatibility of a reward with the given expert's demonstrations.
Intuitively, a reward is more compatible with the demonstrations the closer the
performance of the expert's policy computed with that reward is to the optimal
performance for that reward. This generalizes the notion of feasible reward
set, the most common framework in the theoretical IRL literature, for which a
reward is either compatible or not compatible. The grayscale introduced by the
reward compatibility is the key to extend the realm of provably efficient IRL
far beyond what is attainable with the feasible reward set: from tabular to
large-scale MDPs. We analyze the IRL problem across various settings, including
optimal and suboptimal expert's demonstrations and both online and offline data
collection. For all of these dimensions, we provide a tractable algorithm and
corresponding sample complexity analysis, as well as various insights on reward
compatibility and how the framework can pave the way to yet more general
problem settings.",['cs.LG'],http://arxiv.org/abs/2501.07996v1
EPIC: Effective Prompting for Imbalanced-Class Data Synthesis in Tabular Data Classification via Large Language Models,"Large language models (LLMs) have demonstrated remarkable in-context learning
capabilities across diverse applications. In this work, we explore the
effectiveness of LLMs for generating realistic synthetic tabular data,
identifying key prompt design elements to optimize performance. We introduce
EPIC, a novel approach that leverages balanced, grouped data samples and
consistent formatting with unique variable mapping to guide LLMs in generating
accurate synthetic data across all classes, even for imbalanced datasets.
Evaluations on real-world datasets show that EPIC achieves state-of-the-art
machine learning classification performance, significantly improving generation
efficiency. These findings highlight the effectiveness of EPIC for synthetic
tabular data generation, particularly in addressing class imbalance. Our source
code for our work is available at:
https://seharanul17.github.io/project-synthetic-tabular-llm/","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2404.12404v4
SafePowerGraph-LLM: Novel Power Grid Graph Embedding and Optimization with Large Language Models,"Efficiently solving Optimal Power Flow (OPF) problems in power systems is
crucial for operational planning and grid management. There is a growing need
for scalable algorithms capable of handling the increasing variability,
constraints, and uncertainties in modern power networks while providing
accurate and fast solutions. To address this, machine learning techniques,
particularly Graph Neural Networks (GNNs) have emerged as promising approaches.
This letter introduces SafePowerGraph-LLM, the first framework explicitly
designed for solving OPF problems using Large Language Models (LLM)s. The
proposed approach combines graph and tabular representations of power grids to
effectively query LLMs, capturing the complex relationships and constraints in
power systems. A new implementation of in-context learning and fine-tuning
protocols for LLMs is introduced, tailored specifically for the OPF problem.
SafePowerGraph-LLM demonstrates reliable performances using off-the-shelf LLM.
Our study reveals the impact of LLM architecture, size, and fine-tuning and
demonstrates our framework's ability to handle realistic grid components and
constraints.",['cs.AI'],http://arxiv.org/abs/2501.07639v1
Code and Pixels: Multi-Modal Contrastive Pre-training for Enhanced Tabular Data Analysis,"Learning from tabular data is of paramount importance, as it complements the
conventional analysis of image and video data by providing a rich source of
structured information that is often critical for comprehensive understanding
and decision-making processes. We present Multi-task Contrastive Masked Tabular
Modeling (MT-CMTM), a novel method aiming to enhance tabular models by
leveraging the correlation between tabular data and corresponding images.
MT-CMTM employs a dual strategy combining contrastive learning with masked
tabular modeling, optimizing the synergy between these data modalities.
  Central to our approach is a 1D Convolutional Neural Network with residual
connections and an attention mechanism (1D-ResNet-CBAM), designed to
efficiently process tabular data without relying on images. This enables
MT-CMTM to handle purely tabular data for downstream tasks, eliminating the
need for potentially costly image acquisition and processing.
  We evaluated MT-CMTM on the DVM car dataset, which is uniquely suited for
this particular scenario, and the newly developed HIPMP dataset, which connects
membrane fabrication parameters with image data. Our MT-CMTM model outperforms
the proposed tabular 1D-ResNet-CBAM, which is trained from scratch, achieving a
relative 1.48% improvement in relative MSE on HIPMP and a 2.38% increase in
absolute accuracy on DVM. These results demonstrate MT-CMTM's robustness and
its potential to advance the field of multi-modal learning.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2501.07304v1
Tab-Shapley: Identifying Top-k Tabular Data Quality Insights,"We present an unsupervised method for aggregating anomalies in tabular
datasets by identifying the top-k tabular data quality insights. Each insight
consists of a set of anomalous attributes and the corresponding subsets of
records that serve as evidence to the user. The process of identifying these
insight blocks is challenging due to (i) the absence of labeled anomalies, (ii)
the exponential size of the subset search space, and (iii) the complex
dependencies among attributes, which obscure the true sources of anomalies.
Simple frequency-based methods fail to capture these dependencies, leading to
inaccurate results. To address this, we introduce Tab-Shapley, a cooperative
game theory based framework that uses Shapley values to quantify the
contribution of each attribute to the data's anomalous nature. While
calculating Shapley values typically requires exponential time, we show that
our game admits a closed-form solution, making the computation efficient. We
validate the effectiveness of our approach through empirical analysis on
real-world tabular datasets with ground-truth anomaly labels.","['cs.LG', 'stat.ML']",http://arxiv.org/abs/2501.06685v1
Feature Group Tabular Transformer: A Novel Approach to Traffic Crash Modeling and Causality Analysis,"Reliable and interpretable traffic crash modeling is essential for
understanding causality and improving road safety. This study introduces a
novel approach to predicting collision types by utilizing a comprehensive
dataset fused from multiple sources, including weather data, crash reports,
high-resolution traffic information, pavement geometry, and facility
characteristics. Central to our approach is the development of a Feature Group
Tabular Transformer (FGTT) model, which organizes disparate data into
meaningful feature groups, represented as tokens. These group-based tokens
serve as rich semantic components, enabling effective identification of
collision patterns and interpretation of causal mechanisms. The FGTT model is
benchmarked against widely used tree ensemble models, including Random Forest,
XGBoost, and CatBoost, demonstrating superior predictive performance.
Furthermore, model interpretation reveals key influential factors, providing
fresh insights into the underlying causality of distinct crash types.","['cs.LG', 'cs.AI', 'stat.AP']",http://arxiv.org/abs/2412.06825v2
TabuLa: Harnessing Language Models for Tabular Data Synthesis,"Tabular data synthesis is crucial for addressing privacy and security
concerns in industries reliant on tabular data. While recent advancements adopt
large language models (LLMs) for realistic tabular data generation, their long
training times and limited reusability hinder practical applications. In this
paper, we propose Tabula, a tabular data synthesizer that leverages the
structure of LLM. Unlike state-of-the-art (SOTA) LLM-based tabular data
synthesizers that rely on pre-trained LLMs, Tabula discards the pre-trained
weights originally designed for natural language tasks, focusing instead on a
tailored approach for tabular data. In addition, Tabula introduces a token
sequence compression strategy that significantly reduces training time while
maintaining data quality, alongside a novel token padding method that improves
sequence alignment across training batches. Experiments on six datasets show
that Tabula achieves superior synthetic data utility compared to current SOTA
methods. Additionally, the results demonstrate that Tabula model trained on
tabular datasets serves effectively as a foundational model for synthesizing
new tabular datasets. Furthermore, the proposed padding method outperforms the
conventional left and right padding strategies. Finally, the results highlight
that Tabula averagely reduces training time per epoch by 46.2% compared to
state-of-the-art LLM approaches while achieving higher data utility. Our code
is available at https://github.com/zhao-zilong/Tabula",['cs.LG'],http://arxiv.org/abs/2310.12746v2
The Tabular Foundation Model TabPFN Outperforms Specialized Time Series Forecasting Models Based on Simple Features,"Foundation models have become popular in forecasting due to their ability to
make accurate predictions, even with minimal fine-tuning on specific datasets.
In this paper, we demonstrate how the newly released regression variant of
TabPFN, a general tabular foundation model, can be applied to time series
forecasting. We propose a straightforward approach, TabPFN-TS, which pairs
TabPFN with simple feature engineering to achieve strong forecasting
performance. Despite its simplicity and with only 11M parameters, TabPFN-TS
outperforms Chronos-Mini, a model of similar size, and matches or even slightly
outperforms Chronos-Large, which has 65-fold more parameters. A key strength of
our method lies in its reliance solely on artificial data during pre-training,
avoiding the need for large training datasets and eliminating the risk of
benchmark contamination.",['cs.LG'],http://arxiv.org/abs/2501.02945v2
RieszBoost: Gradient Boosting for Riesz Regression,"Answering causal questions often involves estimating linear functionals of
conditional expectations, such as the average treatment effect or the effect of
a longitudinal modified treatment policy. By the Riesz representation theorem,
these functionals can be expressed as the expected product of the conditional
expectation of the outcome and the Riesz representer, a key component in doubly
robust estimation methods. Traditionally, the Riesz representer is estimated
indirectly by deriving its explicit analytical form, estimating its components,
and substituting these estimates into the known form (e.g., the inverse
propensity score). However, deriving or estimating the analytical form can be
challenging, and substitution methods are often sensitive to practical
positivity violations, leading to higher variance and wider confidence
intervals. In this paper, we propose a novel gradient boosting algorithm to
directly estimate the Riesz representer without requiring its explicit
analytical form. This method is particularly suited for tabular data, offering
a flexible, nonparametric, and computationally efficient alternative to
existing methods for Riesz regression. Through simulation studies, we
demonstrate that our algorithm performs on par with or better than indirect
estimation techniques across a range of functionals, providing a user-friendly
and robust solution for estimating causal quantities.","['stat.ML', 'cs.LG', 'stat.ME']",http://arxiv.org/abs/2501.04871v1
"Medical artificial intelligence toolbox (MAIT): an explainable machine learning framework for binary classification, survival modelling, and regression analyses","While machine learning offers diverse techniques suitable for exploring
various medical research questions, a cohesive synergistic framework can
facilitate the integration and understanding of new approaches within unified
model development and interpretation. We therefore introduce the Medical
Artificial Intelligence Toolbox (MAIT), an explainable, open-source Python
pipeline for developing and evaluating binary classification, regression, and
survival models on tabular datasets. MAIT addresses key challenges (e.g., high
dimensionality, class imbalance, mixed variable types, and missingness) while
promoting transparency in reporting (TRIPOD+AI compliant). Offering automated
configurations for beginners and customizable source code for experts, MAIT
streamlines two primary use cases: Discovery (feature importance via unified
scoring, e.g., SHapley Additive exPlanations - SHAP) and Prediction (model
development and deployment with optimized solutions). Moreover, MAIT proposes
new techniques including fine-tuning of probability threshold in binary
classification, translation of cumulative hazard curves to binary
classification, enhanced visualizations for model interpretation for mixed data
types, and handling censoring through semi-supervised learning, to adapt to a
wide set of data constraints and study designs. We provide detailed tutorials
on GitHub, using four open-access data sets, to demonstrate how MAIT can be
used to improve implementation and interpretation of ML models in medical
research.",['cs.LG'],http://arxiv.org/abs/2501.04547v1
In Search of Trees: Decision-Tree Policy Synthesis for Black-Box Systems via Search,"Decision trees, owing to their interpretability, are attractive as control
policies for (dynamical) systems. Unfortunately, constructing, or synthesising,
such policies is a challenging task. Previous approaches do so by imitating a
neural-network policy, approximating a tabular policy obtained via formal
synthesis, employing reinforcement learning, or modelling the problem as a
mixed-integer linear program. However, these works may require access to a
hard-to-obtain accurate policy or a formal model of the environment (within
reach of formal synthesis), and may not provide guarantees on the quality or
size of the final tree policy. In contrast, we present an approach to
synthesise optimal decision-tree policies given a deterministic black-box
environment and specification, a discretisation of the tree predicates, and an
initial set of states, where optimality is defined with respect to the number
of steps to achieve the goal. Our approach is a specialised search algorithm
which systematically explores the (exponentially large) space of decision trees
under the given discretisation. The key component is a novel trace-based
pruning mechanism that significantly reduces the search space. Our approach
represents a conceptually novel way of synthesising small decision-tree
policies with optimality guarantees even for black-box environments with
black-box specifications.","['cs.AI', 'cs.LG']",http://arxiv.org/abs/2409.03260v2
Data Augmentation for Deep Learning Regression Tasks by Machine Learning Models,"Deep learning (DL) models have gained prominence in domains such as computer
vision and natural language processing but remain underutilized for regression
tasks involving tabular data. In these cases, traditional machine learning (ML)
models often outperform DL models. In this study, we propose and evaluate
various data augmentation (DA) techniques to improve the performance of DL
models for tabular data regression tasks. We compare the performance gain of
Neural Networks by different DA strategies ranging from a naive method of
duplicating existing observations and adding noise to a more sophisticated DA
strategy that preserves the underlying statistical relationship in the data.
Our analysis demonstrates that the advanced DA method significantly improves DL
model performance across multiple datasets and regression tasks, resulting in
an average performance increase of over 10\% compared to baseline models
without augmentation. The efficacy of these DA strategies was rigorously
validated across 30 distinct datasets, with multiple iterations and evaluations
using three different automated deep learning (AutoDL) frameworks: AutoKeras,
H2O, and AutoGluon. This study demonstrates that by leveraging advanced DA
techniques, DL models can realize their full potential in regression tasks,
thereby contributing to broader adoption and enhanced performance in practical
applications.",['cs.LG'],http://arxiv.org/abs/2501.03654v1
TabTreeFormer: Tabular Data Generation Using Hybrid Tree-Transformer,"Transformers have achieved remarkable success in tabular data generation.
However, they lack domain-specific inductive biases which are critical to
preserving the intrinsic characteristics of tabular data. Meanwhile, they
suffer from poor scalability and efficiency due to quadratic computational
complexity. In this paper, we propose TabTreeFormer, a hybrid transformer
architecture that incorporates a tree-based model that retains tabular-specific
inductive biases of non-smooth and potentially low-correlated patterns caused
by discreteness and non-rotational invariance, and hence enhances the fidelity
and utility of synthetic data. In addition, we devise a dual-quantization
tokenizer to capture the multimodal continuous distribution and further
facilitate the learning of numerical value distribution. Moreover, our proposed
tokenizer reduces the vocabulary size and sequence length due to the limited
complexity (e.g., dimension-wise semantic meaning) of tabular data, rendering a
significant model size shrink without sacrificing the capability of the
transformer model. We evaluate TabTreeFormer on 10 datasets against multiple
generative models on various metrics; our experimental results show that
TabTreeFormer achieves superior fidelity, utility, privacy, and efficiency. Our
best model yields a 40% utility improvement with 1/16 of the baseline model
size.",['cs.LG'],http://arxiv.org/abs/2501.01216v3
"Deep Learning within Tabular Data: Foundations, Challenges, Advances and Future Directions","Tabular data remains one of the most prevalent data types across a wide range
of real-world applications, yet effective representation learning for this
domain poses unique challenges due to its irregular patterns, heterogeneous
feature distributions, and complex inter-column dependencies. This survey
provides a comprehensive review of state-of-the-art techniques in tabular data
representation learning, structured around three foundational design elements:
training data, neural architectures, and learning objectives. Unlike prior
surveys that focus primarily on either architecture design or learning
strategies, we adopt a holistic perspective that emphasizes the universality
and robustness of representation learning methods across diverse downstream
tasks. We examine recent advances in data augmentation and generation,
specialized neural network architectures tailored to tabular data, and
innovative learning objectives that enhance representation quality.
Additionally, we highlight the growing influence of self-supervised learning
and the adaptation of transformer-based foundation models for tabular data. Our
review is based on a systematic literature search using rigorous inclusion
criteria, encompassing 127 papers published since 2020 in top-tier conferences
and journals. Through detailed analysis and comparison, we identify emerging
trends, critical gaps, and promising directions for future research, aiming to
guide the development of more generalizable and effective tabular data
representation methods.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2501.03540v1
Training Gradient Boosted Decision Trees on Tabular Data Containing Label Noise for Classification Tasks,"Label noise, which refers to the mislabeling of instances in a dataset, can
significantly impair classifier performance, increase model complexity, and
affect feature selection. While most research has concentrated on deep neural
networks for image and text data, this study explores the impact of label noise
on gradient-boosted decision trees (GBDTs), the leading algorithm for tabular
data. This research fills a gap by examining the robustness of GBDTs to label
noise, focusing on adapting two noise detection methods from deep learning for
use with GBDTs and introducing a new detection method called Gradients.
Additionally, we extend a method initially designed for GBDTs to incorporate
relabeling. By using diverse datasets such as Covertype and Breast Cancer, we
systematically introduce varying levels of label noise and evaluate the
effectiveness of early stopping and noise detection methods in maintaining
model performance. Our noise detection methods achieve state-of-the-art
results, with a noise detection accuracy above 99% on the Adult dataset across
all noise levels. This work enhances the understanding of label noise in GBDTs
and provides a foundation for future research in noise detection and correction
methods.",['cs.LG'],http://arxiv.org/abs/2409.08647v2
On LLM-Enhanced Mixed-Type Data Imputation with High-Order Message Passing,"Missing data imputation, which aims to impute the missing values in the raw
datasets to achieve the completeness of datasets, is crucial for modern
data-driven models like large language models (LLMs) and has attracted
increasing interest over the past decades. Despite its importance, existing
solutions for missing data imputation either 1) only support numerical and
categorical data or 2) show an unsatisfactory performance due to their design
prioritizing text data and the lack of key properties for tabular data
imputation. In this paper, we propose UnIMP, a Unified IMPutation framework
that leverages LLM and high-order message passing to enhance the imputation of
mixed-type data including numerical, categorical, and text data. Specifically,
we first introduce a cell-oriented hypergraph to model the table. We then
propose BiHMP, an efficient Bidirectional High-order Message-Passing network to
aggregate global-local information and high-order relationships on the
constructed hypergraph while capturing the inter-column heterogeneity and
intra-column homogeneity. To effectively and efficiently align the capacity of
the LLM with the information aggregated by BiHMP, we introduce Xfusion, which,
together with BiHMP, acts as adapters for the LLM. We follow a pre-training and
fine-tuning pipeline to train UnIMP, integrating two optimizations: chunking
technique, which divides tables into smaller chunks to enhance efficiency; and
progressive masking technique, which gradually adapts the model to learn more
complex data patterns. Both theoretical proofs and empirical experiments on 10
real world datasets highlight the superiority of UnIMP over existing
techniques.","['cs.LG', 'cs.SI']",http://arxiv.org/abs/2501.02191v1
Segmenting Action-Value Functions Over Time-Scales in SARSA via TD($Δ$),"In numerous episodic reinforcement learning (RL) settings, SARSA-based
methodologies are employed to enhance policies aimed at maximizing returns over
long horizons. Conventional SARSA algorithms, however, have difficulties in
balancing bias and variation due to the reliance on a singular, fixed discount
factor. This study expands the temporal difference decomposition approach,
TD($\Delta$), to the SARSA algorithm, which we designate as SARSA($\Delta$).
SARSA, a widely utilised on-policy RL method, enhances action-value functions
via temporal difference updates. TD($\Delta$) facilitates learning over several
time-scales by breaking the action-value function into components associated
with distinct discount factors. This decomposition improves learning efficiency
and stability, particularly in problems necessitating long-horizon
optimization. We illustrate that our methodology mitigates bias in SARSA's
updates while facilitating accelerated convergence in both deterministic and
stochastic environments. Experimental findings across many benchmark tasks
indicate that the proposed SARSA($\Delta$) surpasses conventional TD learning
methods in both tabular and deep RL environments.","['cs.LG', 'F.2.2, I.2.7']",http://arxiv.org/abs/2411.14783v2
On Expressivity of Height in Neural Networks,"In this work, beyond width and depth, we augment a neural network with a new
dimension called height by intra-linking neurons in the same layer to create an
intra-layer hierarchy, which gives rise to the notion of height. We call a
neural network characterized by width, depth, and height a 3D network. To put a
3D network in perspective, we theoretically and empirically investigate the
expressivity of height. We show via bound estimation and explicit construction
that given the same number of neurons and parameters, a 3D ReLU network of
width $W$, depth $K$, and height $H$ has greater expressive power than a 2D
network of width $H\times W$ and depth $K$, \textit{i.e.},
$\mathcal{O}((2^H-1)W)^K)$ vs $\mathcal{O}((HW)^K)$, in terms of generating
more pieces in a piecewise linear function. Next, through approximation rate
analysis, we show that by introducing intra-layer links into networks, a ReLU
network of width $\mathcal{O}(W)$ and depth $\mathcal{O}(K)$ can approximate
polynomials in $[0,1]^d$ with error $\mathcal{O}\left(2^{-2WK}\right)$, which
improves $\mathcal{O}\left(W^{-K}\right)$ and $\mathcal{O}\left(2^{-K}\right)$
for fixed width networks. Lastly, numerical experiments on 5 synthetic
datasets, 15 tabular datasets, and 3 image benchmarks verify that 3D networks
can deliver competitive regression and classification performance.",['cs.LG'],http://arxiv.org/abs/2305.07037v2
Table as Thought: Exploring Structured Thoughts in LLM Reasoning,"Large language models' reasoning abilities benefit from methods that organize
their thought processes, such as chain-of-thought prompting, which employs a
sequential structure to guide the reasoning process step-by-step. However,
existing approaches focus primarily on organizing the sequence of thoughts,
leaving structure in individual thought steps underexplored. To address this
gap, we propose Table as Thought, a framework inspired by cognitive
neuroscience theories on human thought. Table as Thought organizes reasoning
within a tabular schema, where rows represent sequential thought steps and
columns capture critical constraints and contextual information to enhance
reasoning. The reasoning process iteratively populates the table until
self-verification ensures completeness and correctness. Our experiments show
that Table as Thought excels in planning tasks and demonstrates a strong
potential for enhancing LLM performance in mathematical reasoning compared to
unstructured thought baselines. This work provides a novel exploration of
refining thought representation within LLMs, paving the way for advancements in
reasoning and AI cognition.","['cs.AI', 'cs.CL']",http://arxiv.org/abs/2501.02152v1
On the Statistical Complexity for Offline and Low-Adaptive Reinforcement Learning with Structures,"This article reviews the recent advances on the statistical foundation of
reinforcement learning (RL) in the offline and low-adaptive settings. We will
start by arguing why offline RL is the appropriate model for almost any
real-life ML problems, even if they have nothing to do with the recent AI
breakthroughs that use RL. Then we will zoom into two fundamental problems of
offline RL: offline policy evaluation (OPE) and offline policy learning (OPL).
It may be surprising to people that tight bounds for these problems were not
known even for tabular and linear cases until recently. We delineate the
differences between worst-case minimax bounds and instance-dependent bounds. We
also cover key algorithmic ideas and proof techniques behind near-optimal
instance-dependent methods in OPE and OPL. Finally, we discuss the limitations
of offline RL and review a burgeoning problem of \emph{low-adaptive
exploration} which addresses these limitations by providing a sweet middle
ground between offline and online RL.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2501.02089v1
Creating Artificial Students that Never Existed: Leveraging Large Language Models and CTGANs for Synthetic Data Generation,"In this study, we explore the growing potential of AI and deep learning
technologies, particularly Generative Adversarial Networks (GANs) and Large
Language Models (LLMs), for generating synthetic tabular data. Access to
quality students data is critical for advancing learning analytics, but privacy
concerns and stricter data protection regulations worldwide limit their
availability and usage. Synthetic data offers a promising alternative. We
investigate whether synthetic data can be leveraged to create artificial
students for serving learning analytics models. Using the popular GAN model
CTGAN and three LLMs- GPT2, DistilGPT2, and DialoGPT, we generate synthetic
tabular student data. Our results demonstrate the strong potential of these
methods to produce high-quality synthetic datasets that resemble real students
data. To validate our findings, we apply a comprehensive set of utility
evaluation metrics to assess the statistical and predictive performance of the
synthetic data and compare the different generator models used, specially the
performance of LLMs. Our study aims to provide the learning analytics community
with valuable insights into the use of synthetic data, laying the groundwork
for expanding the field methodological toolbox with new innovative approaches
for learning analytics data generation.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2501.01793v1
Toward Robust Early Detection of Alzheimer's Disease via an Integrated Multimodal Learning Approach,"Alzheimer's Disease (AD) is a complex neurodegenerative disorder marked by
memory loss, executive dysfunction, and personality changes. Early diagnosis is
challenging due to subtle symptoms and varied presentations, often leading to
misdiagnosis with traditional unimodal diagnostic methods due to their limited
scope. This study introduces an advanced multimodal classification model that
integrates clinical, cognitive, neuroimaging, and EEG data to enhance
diagnostic accuracy. The model incorporates a feature tagger with a tabular
data coding architecture and utilizes the TimesBlock module to capture
intricate temporal patterns in Electroencephalograms (EEG) data. By employing
Cross-modal Attention Aggregation module, the model effectively fuses Magnetic
Resonance Imaging (MRI) spatial information with EEG temporal data,
significantly improving the distinction between AD, Mild Cognitive Impairment,
and Normal Cognition. Simultaneously, we have constructed the first AD
classification dataset that includes three modalities: EEG, MRI, and tabular
data. Our innovative approach aims to facilitate early diagnosis and
intervention, potentially slowing the progression of AD. The source code and
our private ADMC dataset are available at https://github.com/JustlfC03/MSTNet.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2408.16343v2
Sequential Large Language Model-Based Hyper-parameter Optimization,"This study introduces SLLMBO, an innovative framework leveraging large
language models (LLMs) for hyperparameter optimization (HPO), incorporating
dynamic search space adaptability, enhanced parameter space exploitation, and a
novel LLM-tree-structured parzen estimator (LLM-TPE) sampler. By addressing
limitations in recent fully LLM-based methods and traditional bayesian
optimization (BO), SLLMBO achieves more robust optimization. This comprehensive
benchmarking evaluates multiple LLMs, including GPT-3.5-Turbo, GPT-4o,
Claude-Sonnet-3.5, and Gemini-1.5-Flash, extending prior work and establishing
SLLMBO as the first framework to benchmark a diverse set of LLMs for HPO. By
integrating LLMs' established strengths in parameter initialization with the
exploitation abilities demonstrated in this study, alongside TPE's exploration
capabilities, the LLM-TPE sampler achieves a balanced exploration-exploitation
trade-off, reduces API costs, and mitigates premature early stoppings for more
effective parameter searches. Across 14 tabular tasks in classification and
regression, the LLM-TPE sampler outperformed fully LLM-based methods and
achieved superior results over BO methods in 9 tasks. Testing early stopping in
budget-constrained scenarios demonstrated competitive performance, indicating
that LLM-based methods generally benefit from extended iterations for optimal
results. This work lays the foundation for future research exploring
open-source LLMs, reproducibility of LLM results in HPO, and benchmarking
SLLMBO on complex datasets, such as image classification, segmentation, and
machine translation.","['cs.LG', 'cs.AI', 'cs.CL']",http://arxiv.org/abs/2410.20302v3
TreeLUT: An Efficient Alternative to Deep Neural Networks for Inference Acceleration Using Gradient Boosted Decision Trees,"Accelerating machine learning inference has been an active research area in
recent years. In this context, field-programmable gate arrays (FPGAs) have
demonstrated compelling performance by providing massive parallelism in deep
neural networks (DNNs). Neural networks (NNs) are computationally intensive
during inference, as they require massive amounts of multiplication and
addition, which makes their implementations costly. Numerous studies have
recently addressed this challenge to some extent using a combination of
sparsity induction, quantization, and transformation of neurons or sub-networks
into lookup tables (LUTs) on FPGAs. Gradient boosted decision trees (GBDTs) are
a high-accuracy alternative to DNNs in a wide range of regression and
classification tasks, particularly for tabular datasets. The basic building
block of GBDTs is a decision tree, which resembles the structure of binary
decision diagrams. FPGA design flows are heavily optimized to implement such a
structure efficiently. In addition to decision trees, GBDTs perform simple
operations during inference, including comparison and addition. We present
TreeLUT as an open-source tool for implementing GBDTs using an efficient
quantization scheme, hardware architecture, and pipelining strategy. It
primarily utilizes LUTs with no BRAMs or DSPs on FPGAs, resulting in high
efficiency. We show the effectiveness of TreeLUT using multiple classification
datasets, commonly used to evaluate ultra-low area and latency architectures.
Using these benchmarks, we compare our implementation results with existing DNN
and GBDT methods, such as DWN, PolyLUT-Add, NeuraLUT, LogicNets, FINN, hls4ml,
and others. Our results show that TreeLUT significantly improves hardware
utilization, latency, and throughput at competitive accuracy compared to
previous works.","['cs.LG', 'cs.AR']",http://arxiv.org/abs/2501.01511v1
Multi-Modal Video Feature Extraction for Popularity Prediction,"This work aims to predict the popularity of short videos using the videos
themselves and their related features. Popularity is measured by four key
engagement metrics: view count, like count, comment count, and share count.
This study employs video classification models with different architectures and
training methods as backbone networks to extract video modality features.
Meanwhile, the cleaned video captions are incorporated into a carefully
designed prompt framework, along with the video, as input for video-to-text
generation models, which generate detailed text-based video content
understanding. These texts are then encoded into vectors using a pre-trained
BERT model. Based on the six sets of vectors mentioned above, a neural network
is trained for each of the four prediction metrics. Moreover, the study
conducts data mining and feature engineering based on the video and tabular
data, constructing practical features such as the total frequency of hashtag
appearances, the total frequency of mention appearances, video duration, frame
count, frame rate, and total time online. Multiple machine learning models are
trained, and the most stable model, XGBoost, is selected. Finally, the
predictions from the neural network and XGBoost models are averaged to obtain
the final result.","['cs.CV', 'cs.AI', 'cs.LG']",http://arxiv.org/abs/2501.01422v1
Graph Analysis Using a GPU-based Parallel Algorithm: Quantum Clustering,"The article introduces a new method for applying Quantum Clustering to graph
structures. Quantum Clustering (QC) is a novel density-based unsupervised
learning method that determines cluster centers by constructing a potential
function. In this method, we use the Graph Gradient Descent algorithm to find
the centers of clusters. GPU parallelization is utilized for computing
potential values. We also conducted experiments on five widely used datasets
and evaluated using four indicators. The results show superior performance of
the method. Finally, we discuss the influence of $\sigma$ on the experimental
results.","['cs.LG', 'I.2.0 General']",http://arxiv.org/abs/2305.14641v3
Multiple-Input Variational Auto-Encoder for Anomaly Detection in Heterogeneous Data,"Anomaly detection (AD) plays a pivotal role in AI applications, e.g., in
classification, and intrusion/threat detection in cybersecurity. However, most
existing methods face challenges of heterogeneity amongst feature subsets posed
by non-independent and identically distributed (non-IID) data. We propose a
novel neural network model called Multiple-Input Auto-Encoder for AD (MIAEAD)
to address this. MIAEAD assigns an anomaly score to each feature subset of a
data sample to indicate its likelihood of being an anomaly. This is done by
using the reconstruction error of its sub-encoder as the anomaly score. All
sub-encoders are then simultaneously trained using unsupervised learning to
determine the anomaly scores of feature subsets. The final AUC of MIAEAD is
calculated for each sub-dataset, and the maximum AUC obtained among the
sub-datasets is selected. To leverage the modelling of the distribution of
normal data to identify anomalies of the generative models, we develop a novel
neural network architecture/model called Multiple-Input Variational
Auto-Encoder (MIVAE). MIVAE can process feature subsets through its
sub-encoders before learning distribution of normal data in the latent space.
This allows MIVAE to identify anomalies that deviate from the learned
distribution. We theoretically prove that the difference in the average anomaly
score between normal samples and anomalies obtained by the proposed MIVAE is
greater than that of the Variational Auto-Encoder (VAEAD), resulting in a
higher AUC for MIVAE. Extensive experiments on eight real-world anomaly
datasets demonstrate the superior performance of MIAEAD and MIVAE over
conventional methods and the state-of-the-art unsupervised models, by up to 6%
in terms of AUC score. Alternatively, MIAEAD and MIVAE have a high AUC when
applied to feature subsets with low heterogeneity based on the coefficient of
variation (CV) score.","['cs.AI', 'cs.LG', 'stat.ML']",http://arxiv.org/abs/2501.08149v1
Autoencoded UMAP-Enhanced Clustering for Unsupervised Learning,"We propose a novel approach to unsupervised learning by constructing a
non-linear embedding of the data into a low-dimensional space followed by any
conventional clustering algorithm. The embedding promotes clusterability of the
data and is comprised of two mappings: the encoder of an autoencoder neural
network and the output of UMAP algorithm. The autoencoder is trained with a
composite loss function that incorporates both a conventional data
reconstruction as a regularization component and a clustering-promoting
component built using the spectral graph theory. The two embeddings and the
subsequent clustering are integrated into a three-stage unsupervised learning
framework, referred to as Autoencoded UMAP-Enhanced Clustering (AUEC). When
applied to MNIST data, AUEC significantly outperforms the state-of-the-art
techniques in terms of clustering accuracy.","['cs.LG', '68Q32, 68T07, 68T09, 68T10']",http://arxiv.org/abs/2501.07729v1
PROTECT: Protein circadian time prediction using unsupervised learning,"Circadian rhythms regulate the physiology and behavior of humans and animals.
Despite advancements in understanding these rhythms and predicting circadian
phases at the transcriptional level, predicting circadian phases from proteomic
data remains elusive. This challenge is largely due to the scarcity of time
labels in proteomic datasets, which are often characterized by small sample
sizes, high dimensionality, and significant noise. Furthermore, existing
methods for predicting circadian phases from transcriptomic data typically rely
on prior knowledge of known rhythmic genes, making them unsuitable for
proteomic datasets. To address this gap, we developed a novel computational
method using unsupervised deep learning techniques to predict circadian sample
phases from proteomic data without requiring time labels or prior knowledge of
proteins or genes. Our model involves a two-stage training process optimized
for robust circadian phase prediction: an initial greedy one-layer-at-a-time
pre-training which generates informative initial parameters followed by
fine-tuning. During fine-tuning, a specialized loss function guides the model
to align protein expression levels with circadian patterns, enabling it to
accurately capture the underlying rhythmic structure within the data. We tested
our method on both time-labeled and unlabeled proteomic data. For labeled data,
we compared our predictions to the known time labels, achieving high accuracy,
while for unlabeled human datasets, including postmortem brain regions and
urine samples, we explored circadian disruptions. Notably, our analysis
identified disruptions in rhythmic proteins between Alzheimer's disease and
control subjects across these samples.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2501.07405v1
Learning Spectral Methods by Transformers,"Transformers demonstrate significant advantages as the building block of
modern LLMs. In this work, we study the capacities of Transformers in
performing unsupervised learning. We show that multi-layered Transformers,
given a sufficiently large set of pre-training instances, are able to learn the
algorithms themselves and perform statistical estimation tasks given new
instances. This learning paradigm is distinct from the in-context learning
setup and is similar to the learning procedure of human brains where skills are
learned through past experience. Theoretically, we prove that pre-trained
Transformers can learn the spectral methods and use the classification of
bi-class Gaussian mixture model as an example. Our proof is constructive using
algorithmic design techniques. Our results are built upon the similarities of
multi-layered Transformer architecture with the iterative recovery algorithms
used in practice. Empirically, we verify the strong capacity of the
multi-layered (pre-trained) Transformer on unsupervised learning through the
lens of both the PCA and the Clustering tasks performed on the synthetic and
real-world datasets.","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",http://arxiv.org/abs/2501.01312v3
Black-box optimization and quantum annealing for filtering out mislabeled training instances,"This study proposes an approach for removing mislabeled instances from
contaminated training datasets by combining surrogate model-based black-box
optimization (BBO) with postprocessing and quantum annealing. Mislabeled
training instances, a common issue in real-world datasets, often degrade model
generalization, necessitating robust and efficient noise-removal strategies.
The proposed method evaluates filtered training subsets based on validation
loss, iteratively refines loss estimates through surrogate model-based BBO with
postprocessing, and leverages quantum annealing to efficiently sample diverse
training subsets with low validation error. Experiments on a noisy majority bit
task demonstrate the method's ability to prioritize the removal of high-risk
mislabeled instances. Integrating D-Wave's clique sampler running on a physical
quantum annealer achieves faster optimization and higher-quality training
subsets compared to OpenJij's simulated quantum annealing sampler or Neal's
simulated annealing sampler, offering a scalable framework for enhancing
dataset quality. This work highlights the effectiveness of the proposed method
for supervised learning tasks, with future directions including its application
to unsupervised learning, real-world datasets, and large-scale implementations.","['cs.LG', 'cond-mat.stat-mech', 'quant-ph']",http://arxiv.org/abs/2501.06916v1
"Fixed Points of Deep Neural Networks: Emergence, Stability, and Applications","We present numerical and analytical results on the formation and stability of
a family of fixed points of deep neural networks (DNNs). Such fixed points
appear in a class of DNNs when dimensions of input and output vectors are the
same. We demonstrate examples of applications of such networks in supervised,
semi-supervised and unsupervised learning such as encoding/decoding of images,
restoration of damaged images among others.
  We present several numerical and analytical results. First, we show that for
untrained DNN's with weights and biases initialized by normally distributed
random variables the only one fixed point exists. This result holds for DNN
with any depth (number of layers) $L$, any layer width $N$, and sigmoid-type
activation functions. Second, it has been shown that for a DNN whose parameters
(weights and biases) are initialized by ``light-tailed'' distribution of
weights (e.g. normal distribution), after training the distribution of these
parameters become ``heavy-tailed''. This motivates our study of DNNs with
``heavy-tailed'' initialization. For such DNNs we show numerically %existence
and stability that training leads to emergence of $Q(N,L)$ fixed points, where
$Q(N,L)$ is a positive integer which depends on the number of layers $L$ and
layer width $N$. We further observe numerically that for fixed $N = N_0$ the
function $Q(N_0, L)$ is non-monotone, that is it initially grows as $L$
increases and then decreases to 1.
  This non-monotone behavior of $Q(N_0, L)$ is also obtained by analytical
derivation of equation for Empirical Spectral Distribution (ESD) of
input-output Jacobian followed by numerical solution of this equation.","['cs.LG', 'cs.AI', 'cs.NA', 'math.NA']",http://arxiv.org/abs/2501.04182v1
Multicollinearity Resolution Based on Machine Learning: A Case Study of Carbon Emissions,"This study presents a general analytical framework using DBSCAN clustering
and penalized regression models to address multifactor problems with structural
complexity and multicollinearity issues, such as carbon emission issue. The
framework leverages DBSCAN for unsupervised learning to objectively cluster
features. Meanwhile, penalized regression considers model complexity control
and high dimensional feature selection to identify dominant influencing
factors. Applying this framework to analyze energy consumption data for 46
industries from 2000 to 2019 identified 16 categories in the sample of China.
We quantitatively assessed emission characteristics and drivers for each. The
results demonstrate the framework's analytical approach can identify primary
emission sources by category, providing quantitative references for
decision-making. Overall, this framework can evaluate complex regional issues
like carbon emissions to support policymaking. This research preliminarily
validated its application value in identifying opportunities for emission
reduction worldwide.",['cs.LG'],http://arxiv.org/abs/2309.01115v3
Bridging the Gap: A Decade Review of Time-Series Clustering Methods,"Time series, as one of the most fundamental representations of sequential
data, has been extensively studied across diverse disciplines, including
computer science, biology, geology, astronomy, and environmental sciences. The
advent of advanced sensing, storage, and networking technologies has resulted
in high-dimensional time-series data, however, posing significant challenges
for analyzing latent structures over extended temporal scales. Time-series
clustering, an established unsupervised learning strategy that groups similar
time series together, helps unveil hidden patterns in these complex datasets.
In this survey, we trace the evolution of time-series clustering methods from
classical approaches to recent advances in neural networks. While previous
surveys have focused on specific methodological categories, we bridge the gap
between traditional clustering methods and emerging deep learning-based
algorithms, presenting a comprehensive, unified taxonomy for this research
area. This survey highlights key developments and provides insights to guide
future research in time-series clustering.","['cs.LG', 'cs.AI', 'cs.DB']",http://arxiv.org/abs/2412.20582v1
Extended Cross-Modality United Learning for Unsupervised Visible-Infrared Person Re-identification,"Unsupervised learning visible-infrared person re-identification (USL-VI-ReID)
aims to learn modality-invariant features from unlabeled cross-modality
datasets and reduce the inter-modality gap. However, the existing methods lack
cross-modality clustering or excessively pursue cluster-level association,
which makes it difficult to perform reliable modality-invariant features
learning. To deal with this issue, we propose a Extended Cross-Modality United
Learning (ECUL) framework, incorporating Extended Modality-Camera Clustering
(EMCC) and Two-Step Memory Updating Strategy (TSMem) modules. Specifically, we
design ECUL to naturally integrates intra-modality clustering, inter-modality
clustering and inter-modality instance selection, establishing compact and
accurate cross-modality associations while reducing the introduction of noisy
labels. Moreover, EMCC captures and filters the neighborhood relationships by
extending the encoding vector, which further promotes the learning of
modality-invariant and camera-invariant knowledge in terms of clustering
algorithm. Finally, TSMem provides accurate and generalized proxy points for
contrastive learning by updating the memory in stages. Extensive experiments
results on SYSU-MM01 and RegDB datasets demonstrate that the proposed ECUL
shows promising performance and even outperforms certain supervised methods.","['cs.CV', 'cs.LG', 'eess.IV']",http://arxiv.org/abs/2412.19134v1
Generative Models with ELBOs Converging to Entropy Sums,"The evidence lower bound (ELBO) is one of the most central objectives for
probabilistic unsupervised learning. For the ELBOs of several generative models
and model classes, we here prove convergence to entropy sums. As one result, we
provide a list of generative models for which entropy convergence has been
shown, so far, along with the corresponding expressions for entropy sums. Our
considerations include very prominent generative models such as probabilistic
PCA, sigmoid belief nets or Gaussian mixture models. However, we treat more
models and entire model classes such as general mixtures of exponential family
distributions. Our main contributions are the proofs for the individual models.
For each given model we show that the conditions stated in Theorem 1 or Theorem
2 of [arXiv:2209.03077] are fulfilled such that by virtue of the theorems the
given model's ELBO is equal to an entropy sum at all stationary points. The
equality of the ELBO at stationary points applies under realistic conditions:
for finite numbers of data points, for model/data mismatches, at any stationary
point including saddle points etc, and it applies for any well behaved family
of variational distributions.","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT', 'math.PR', 'math.ST', 'stat.TH', '65C20, 68T07, 60-08, 62-08, 62F99, 68T05', 'G.3']",http://arxiv.org/abs/2501.09022v1
On the Convergence of the ELBO to Entropy Sums,"The variational lower bound (a.k.a. ELBO or free energy) is the central
objective for many established as well as for many novel algorithms for
unsupervised learning. Such algorithms usually increase the bound until
parameters have converged to values close to a stationary point of the learning
dynamics. Here we show that (for a very large class of generative models) the
variational lower bound is at all stationary points of learning equal to a sum
of entropies. Concretely, for standard generative models with one set of
latents and one set of observed variables, the sum consists of three entropies:
(A) the (average) entropy of the variational distributions, (B) the negative
entropy of the model's prior distribution, and (C) the (expected) negative
entropy of the observable distribution. The obtained result applies under
realistic conditions including: finite numbers of data points, at any
stationary point (including saddle points) and for any family of (well behaved)
variational distributions. The class of generative models for which we show the
equality to entropy sums contains many standard as well as novel generative
models including standard (Gaussian) variational autoencoders. The
prerequisites we use to show equality to entropy sums are relatively mild.
Concretely, the distributions defining a given generative model have to be of
the exponential family, and the model has to satisfy a parameterization
criterion (which is usually fulfilled). Proving equality of the ELBO to entropy
sums at stationary points (under the stated conditions) is the main
contribution of this work.","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT', 'math.PR', 'math.ST', 'stat.TH', '65C20, 68T07, 60-08, 62-08, 62F99, 68T05', 'G.3']",http://arxiv.org/abs/2209.03077v6
Unsupervised learning of spatially varying regularization for diffeomorphic image registration,"Spatially varying regularization accommodates the deformation variations that
may be necessary for different anatomical regions during deformable image
registration. Historically, optimization-based registration models have
harnessed spatially varying regularization to address anatomical subtleties.
However, most modern deep learning-based models tend to gravitate towards
spatially invariant regularization, wherein a homogenous regularization
strength is applied across the entire image, potentially disregarding localized
variations. In this paper, we propose a hierarchical probabilistic model that
integrates a prior distribution on the deformation regularization strength,
enabling the end-to-end learning of a spatially varying deformation regularizer
directly from the data. The proposed method is straightforward to implement and
easily integrates with various registration network architectures.
Additionally, automatic tuning of hyperparameters is achieved through Bayesian
optimization, allowing efficient identification of optimal hyperparameters for
any given registration task. Comprehensive evaluations on publicly available
datasets demonstrate that the proposed method significantly improves
registration performance and enhances the interpretability of deep
learning-based registration, all while maintaining smooth deformations.",['cs.CV'],http://arxiv.org/abs/2412.17982v1
Learning to Generate Gradients for Test-Time Adaptation via Test-Time Training Layers,"Test-time adaptation (TTA) aims to fine-tune a trained model online using
unlabeled testing data to adapt to new environments or out-of-distribution
data, demonstrating broad application potential in real-world scenarios.
However, in this optimization process, unsupervised learning objectives like
entropy minimization frequently encounter noisy learning signals. These signals
produce unreliable gradients, which hinder the model ability to converge to an
optimal solution quickly and introduce significant instability into the
optimization process. In this paper, we seek to resolve these issues from the
perspective of optimizer design. Unlike prior TTA using manually designed
optimizers like SGD, we employ a learning-to-optimize approach to automatically
learn an optimizer, called Meta Gradient Generator (MGG). Specifically, we aim
for MGG to effectively utilize historical gradient information during the
online optimization process to optimize the current model. To this end, in MGG,
we design a lightweight and efficient sequence modeling layer -- gradient
memory layer. It exploits a self-supervised reconstruction loss to compress
historical gradient information into network parameters, thereby enabling
better memorization ability over a long-term adaptation process. We only need a
small number of unlabeled samples to pre-train MGG, and then the trained MGG
can be deployed to process unseen samples. Promising results on ImageNet-C, R,
Sketch, and A indicate that our method surpasses current state-of-the-art
methods with fewer updates, less data, and significantly shorter adaptation
iterations. Compared with a previous SOTA method SAR, we achieve 7.4% accuracy
improvement and 4.2 times faster adaptation speed on ImageNet-C.","['cs.LG', 'cs.CV']",http://arxiv.org/abs/2412.16901v1
Leveraging Color Channel Independence for Improved Unsupervised Object Detection,"Object-centric architectures can learn to extract distinct object
representations from visual scenes, enabling downstream applications on the
object level. Similarly to autoencoder-based image models, object-centric
approaches have been trained on the unsupervised reconstruction loss of images
encoded by RGB color spaces. In our work, we challenge the common assumption
that RGB images are the optimal color space for unsupervised learning in
computer vision. We discuss conceptually and empirically that other color
spaces, such as HSV, bear essential characteristics for object-centric
representation learning, like robustness to lighting conditions. We further
show that models improve when requiring them to predict additional color
channels. Specifically, we propose to transform the predicted targets to the
RGB-S space, which extends RGB with HSV's saturation component and leads to
markedly better reconstruction and disentanglement for five common evaluation
datasets. The use of composite color spaces can be implemented with basically
no computational overhead, is agnostic of the models' architecture, and is
universally applicable across a wide range of visual computing tasks and
training types. The findings of our approach encourage additional
investigations in computer vision tasks beyond object-centric learning.","['cs.CV', 'cs.AI', 'cs.LG', 'I.4.8; I.2.10']",http://arxiv.org/abs/2412.15150v1
Distribution-Consistency-Guided Multi-modal Hashing,"Multi-modal hashing methods have gained popularity due to their fast speed
and low storage requirements. Among them, the supervised methods demonstrate
better performance by utilizing labels as supervisory signals compared with
unsupervised methods. Currently, for almost all supervised multi-modal hashing
methods, there is a hidden assumption that training sets have no noisy labels.
However, labels are often annotated incorrectly due to manual labeling in
real-world scenarios, which will greatly harm the retrieval performance. To
address this issue, we first discover a significant distribution consistency
pattern through experiments, i.e., the 1-0 distribution of the presence or
absence of each category in the label is consistent with the high-low
distribution of similarity scores of the hash codes relative to category
centers. Then, inspired by this pattern, we propose a novel
Distribution-Consistency-Guided Multi-modal Hashing (DCGMH), which aims to
filter and reconstruct noisy labels to enhance retrieval performance.
Specifically, the proposed method first randomly initializes several category
centers, which are used to compute the high-low distribution of similarity
scores; Noisy and clean labels are then separately filtered out via the
discovered distribution consistency pattern to mitigate the impact of noisy
labels; Subsequently, a correction strategy, which is indirectly designed via
the distribution consistency pattern, is applied to the filtered noisy labels,
correcting high-confidence ones while treating low-confidence ones as unlabeled
for unsupervised learning, thereby further enhancing the model's performance.
Extensive experiments on three widely used datasets demonstrate the superiority
of the proposed method compared to state-of-the-art baselines in multi-modal
retrieval tasks. The code is available at
https://github.com/LiuJinyu1229/DCGMH.","['cs.CV', 'cs.AI', 'cs.IR']",http://arxiv.org/abs/2412.11216v2
GBRIP: Granular Ball Representation for Imbalanced Partial Label Learning,"Partial label learning (PLL) is a complicated weakly supervised
multi-classification task compounded by class imbalance. Currently, existing
methods only rely on inter-class pseudo-labeling from inter-class features,
often overlooking the significant impact of the intra-class imbalanced features
combined with the inter-class. To address these limitations, we introduce
Granular Ball Representation for Imbalanced PLL (GBRIP), a novel framework for
imbalanced PLL. GBRIP utilizes coarse-grained granular ball representation and
multi-center loss to construct a granular ball-based nfeature space through
unsupervised learning, effectively capturing the feature distribution within
each class. GBRIP mitigates the impact of confusing features by systematically
refining label disambiguation and estimating imbalance distributions. The novel
multi-center loss function enhances learning by emphasizing the relationships
between samples and their respective centers within the granular balls.
Extensive experiments on standard benchmarks demonstrate that GBRIP outperforms
existing state-of-the-art methods, offering a robust solution to the challenges
of imbalanced PLL.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2412.14561v1
Machine Learning-Based Automated Assessment of Intracorporeal Suturing in Laparoscopic Fundoplication,"Automated assessment of surgical skills using artificial intelligence (AI)
provides trainees with instantaneous feedback. After bimanual tool motions are
captured, derived kinematic metrics are reliable predictors of performance in
laparoscopic tasks. Implementing automated tool tracking requires
time-intensive human annotation. We developed AI-based tool tracking using the
Segment Anything Model (SAM) to eliminate the need for human annotators. Here,
we describe a study evaluating the usefulness of our tool tracking model in
automated assessment during a laparoscopic suturing task in the fundoplication
procedure. An automated tool tracking model was applied to recorded videos of
Nissen fundoplication on porcine bowel. Surgeons were grouped as novices
(PGY1-2) and experts (PGY3-5, attendings). The beginning and end of each
suturing step were segmented, and motions of the left and right tools were
extracted. A low-pass filter with a 24 Hz cut-off frequency removed noise.
Performance was assessed using supervised and unsupervised models, and an
ablation study compared results. Kinematic features--RMS velocity, RMS
acceleration, RMS jerk, total path length, and Bimanual Dexterity--were
extracted and analyzed using Logistic Regression, Random Forest, Support Vector
Classifier, and XGBoost. PCA was performed for feature reduction. For
unsupervised learning, a Denoising Autoencoder (DAE) model with classifiers,
such as a 1-D CNN and traditional models, was trained. Data were extracted for
28 participants (9 novices, 19 experts). Supervised learning with PCA and
Random Forest achieved an accuracy of 0.795 and an F1 score of 0.778. The
unsupervised 1-D CNN achieved superior results with an accuracy of 0.817 and an
F1 score of 0.806, eliminating the need for kinematic feature computation. We
demonstrated an AI model capable of automated performance classification,
independent of human annotation.","['cs.CV', 'cs.AI', 'cs.LG']",http://arxiv.org/abs/2412.16195v1
TransPeakNet: Solvent-Aware 2D NMR Prediction via Multi-Task Pre-Training and Unsupervised Learning,"Nuclear Magnetic Resonance (NMR) spectroscopy is essential for revealing
molecular structure, electronic environment, and dynamics. Accurate NMR shift
prediction allows researchers to validate structures by comparing predicted and
observed shifts. While Machine Learning (ML) has improved one-dimensional (1D)
NMR shift prediction, predicting 2D NMR remains challenging due to limited
annotated data. To address this, we introduce an unsupervised training
framework for predicting cross-peaks in 2D NMR, specifically Heteronuclear
Single Quantum Coherence (HSQC).Our approach pretrains an ML model on an
annotated 1D dataset of 1H and 13C shifts, then finetunes it in an unsupervised
manner using unlabeled HSQC data, which simultaneously generates cross-peak
annotations. Our model also adjusts for solvent effects. Evaluation on 479
expert-annotated HSQC spectra demonstrates our model's superiority over
traditional methods (ChemDraw and Mestrenova), achieving Mean Absolute Errors
(MAEs) of 2.05 ppm and 0.165 ppm for 13C shifts and 1H shifts respectively. Our
algorithmic annotations show a 95.21% concordance with experts' assignments,
underscoring the approach's potential for structural elucidation in fields like
organic chemistry, pharmaceuticals, and natural products.","['cs.LG', 'cs.AI', 'physics.chem-ph']",http://arxiv.org/abs/2403.11353v4
Brain-inspired Chaotic Graph Backpropagation for Large-scale Combinatorial Optimization,"Graph neural networks (GNNs) with unsupervised learning can solve large-scale
combinatorial optimization problems (COPs) with efficient time complexity,
making them versatile for various applications. However, since this method maps
the combinatorial optimization problem to the training process of a graph
neural network, and the current mainstream backpropagation-based training
algorithms are prone to fall into local minima, the optimization performance is
still inferior to the current state-of-the-art (SOTA) COP methods. To address
this issue, inspired by possibly chaotic dynamics of real brain learning, we
introduce a chaotic training algorithm, i.e. chaotic graph backpropagation
(CGBP), which introduces a local loss function in GNN that makes the training
process not only chaotic but also highly efficient. Different from existing
methods, we show that the global ergodicity and pseudo-randomness of such
chaotic dynamics enable CGBP to learn each optimal GNN effectively and
globally, thus solving the COP efficiently. We have applied CGBP to solve
various COPs, such as the maximum independent set, maximum cut, and graph
coloring. Results on several large-scale benchmark datasets showcase that CGBP
can outperform not only existing GNN algorithms but also SOTA methods. In
addition to solving large-scale COPs, CGBP as a universal learning algorithm
for GNNs, i.e. as a plug-in unit, can be easily integrated into any existing
method for improving the performance.","['cs.LG', 'cs.AI', 'cs.NE']",http://arxiv.org/abs/2412.09860v1
The Effect of Similarity Measures on Accurate Stability Estimates for Local Surrogate Models in Text-based Explainable AI,"Recent work has investigated the vulnerability of local surrogate methods to
adversarial perturbations on a machine learning (ML) model's inputs, where the
explanation is manipulated while the meaning and structure of the original
input remains similar under the complex model. Although weaknesses across many
methods have been shown to exist, the reasons behind why remain little
explored. Central to the concept of adversarial attacks on explainable AI (XAI)
is the similarity measure used to calculate how one explanation differs from
another. A poor choice of similarity measure can lead to erroneous conclusions
on the efficacy of an XAI method. Too sensitive a measure results in
exaggerated vulnerability, while too coarse understates its weakness. We
investigate a variety of similarity measures designed for text-based ranked
lists, including Kendall's Tau, Spearman's Footrule, and Rank-biased Overlap to
determine how substantial changes in the type of measure or threshold of
success affect the conclusions generated from common adversarial attack
processes. Certain measures are found to be overly sensitive, resulting in
erroneous estimates of stability.","['cs.LG', 'cs.CR']",http://arxiv.org/abs/2406.15839v2
SEANN: A Domain-Informed Neural Network for Epidemiological Insights,"In epidemiology, traditional statistical methods such as logistic regression,
linear regression, and other parametric models are commonly employed to
investigate associations between predictors and health outcomes. However,
non-parametric machine learning techniques, such as deep neural networks
(DNNs), coupled with explainable AI (XAI) tools, offer new opportunities for
this task. Despite their potential, these methods face challenges due to the
limited availability of high-quality, high-quantity data in this field. To
address these challenges, we introduce SEANN, a novel approach for informed
DNNs that leverages a prevalent form of domain-specific knowledge: Pooled
Effect Sizes (PES). PESs are commonly found in published Meta-Analysis studies,
in different forms, and represent a quantitative form of a scientific
consensus. By direct integration within the learning procedure using a custom
loss, we experimentally demonstrate significant improvements in the
generalizability of predictive performances and the scientific plausibility of
extracted relationships compared to a domain-knowledge agnostic neural network
in a scarce and noisy data setting.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2501.10273v1
Counterfactual Explanations for k-means and Gaussian Clustering,"Counterfactuals have been recognized as an effective approach to explain
classifier decisions. Nevertheless, they have not yet been considered in the
context of clustering. In this work, we propose the use of counterfactuals to
explain clustering solutions. First, we present a general definition for
counterfactuals for model-based clustering that includes plausibility and
feasibility constraints. Then we consider the counterfactual generation problem
for k-means and Gaussian clustering assuming Euclidean distance. Our approach
takes as input the factual, the target cluster, a binary mask indicating
actionable or immutable features and a plausibility factor specifying how far
from the cluster boundary the counterfactual should be placed. In the k-means
clustering case, analytical mathematical formulas are presented for computing
the optimal solution, while in the Gaussian clustering case (assuming full,
diagonal, or spherical covariances) our method requires the numerical solution
of a nonlinear equation with a single parameter only. We demonstrate the
advantages of our approach through illustrative examples and quantitative
experimental comparisons.",['cs.LG'],http://arxiv.org/abs/2501.10234v1
Contributions to the Decision Theoretic Foundations of Machine Learning and Robust Statistics under Weakly Structured Information,"This habilitation thesis is cumulative and, therefore, is collecting and
connecting research that I (together with several co-authors) have conducted
over the last few years. Thus, the absolute core of the work is formed by the
ten publications listed on page 5 under the name Contributions 1 to 10. The
references to the complete versions of these articles are also found in this
list, making them as easily accessible as possible for readers wishing to dive
deep into the different research projects. The chapters following this thesis,
namely Parts A to C and the concluding remarks, serve to place the articles in
a larger scientific context, to (briefly) explain their respective content on a
less formal level, and to highlight some interesting perspectives for future
research in their respective contexts. Naturally, therefore, the following
presentation has neither the level of detail nor the formal rigor that can
(hopefully) be found in the papers. The purpose of the following text is to
provide the reader an easy and high-level access to this interesting and
important research field as a whole, thereby, advertising it to a broader
audience.","['stat.ML', 'cs.LG']",http://arxiv.org/abs/2501.10195v1
XEQ Scale for Evaluating XAI Experience Quality,"Explainable Artificial Intelligence (XAI) aims to improve the transparency of
autonomous decision-making through explanations. Recent literature has
emphasised users' need for holistic ""multi-shot"" explanations and personalised
engagement with XAI systems. We refer to this user-centred interaction as an
XAI Experience. Despite advances in creating XAI experiences, evaluating them
in a user-centred manner has remained challenging. In response, we developed
the XAI Experience Quality (XEQ) Scale. XEQ quantifies the quality of
experiences across four dimensions: learning, utility, fulfilment and
engagement. These contributions extend the state-of-the-art of XAI evaluation,
moving beyond the one-dimensional metrics frequently developed to assess
single-shot explanations. This paper presents the XEQ scale development and
validation process, including content validation with XAI experts, and
discriminant and construct validation through a large-scale pilot study. Our
pilot study results offer strong evidence that establishes the XEQ Scale as a
comprehensive framework for evaluating user-centred XAI experiences.","['cs.AI', 'cs.HC']",http://arxiv.org/abs/2407.10662v4
Infrastructure for AI Agents,"Increasingly many AI systems can plan and execute interactions in open-ended
environments, such as making phone calls or buying online goods. As developers
grow the space of tasks that such AI agents can accomplish, we will need tools
both to unlock their benefits and manage their risks. Current tools are largely
insufficient because they are not designed to shape how agents interact with
existing institutions (e.g., legal and economic systems) or actors (e.g.,
digital service providers, humans, other AI agents). For example, alignment
techniques by nature do not assure counterparties that some human will be held
accountable when a user instructs an agent to perform an illegal action. To
fill this gap, we propose the concept of agent infrastructure: technical
systems and shared protocols external to agents that are designed to mediate
and influence their interactions with and impacts on their environments. Agent
infrastructure comprises both new tools and reconfigurations or extensions of
existing tools. For example, to facilitate accountability, protocols that tie
users to agents could build upon existing systems for user authentication, such
as OpenID. Just as the Internet relies on infrastructure like HTTPS, we argue
that agent infrastructure will be similarly indispensable to ecosystems of
agents. We identify three functions for agent infrastructure: 1) attributing
actions, properties, and other information to specific agents, their users, or
other actors; 2) shaping agents' interactions; and 3) detecting and remedying
harmful actions from agents. We propose infrastructure that could help achieve
each function, explaining use cases, adoption, limitations, and open questions.
Making progress on agent infrastructure can prepare society for the adoption of
more advanced agents.",['cs.AI'],http://arxiv.org/abs/2501.10114v1
Tracking student skills real-time through a continuous-variable dynamic Bayesian network,"The field of Knowledge Tracing is focused on predicting the success rate of a
student for a given skill. Modern methods like Deep Knowledge Tracing provide
accurate estimates given enough data, but being based on neural networks they
struggle to explain how these estimates are formed. More classical methods like
Dynamic Bayesian Networks can do this, but they cannot give data on the
accuracy of their estimates and often struggle to incorporate new observations
in real-time due to their high computational load.
  This paper presents a novel method, Performance Distribution Tracing (PDT),
in which the distribution of the success rate is traced live. It uses a Dynamic
Bayesian Network with continuous random variables as nodes. By tracing the
success rate distribution, there is always data available on the accuracy of
any success rate estimation. In addition, it makes it possible to combine data
from similar/related skills to come up with a more informed estimate of success
rates. This makes it possible to predict exercise success rates, providing both
explainability and an accuracy indication, even when an exercise requires a
combination of different skills to solve. And through the use of the beta
distribution functions as conjugate priors, all distributions are available in
analytical form, allowing efficient online updates upon new observations.
Experiments have shown that the resulting estimates generally feel sufficiently
accurate to end-users such that they accept recommendations based on them.","['stat.ML', 'cs.LG']",http://arxiv.org/abs/2501.10050v1
OPCap:Object-aware Prompting Captioning,"In the field of image captioning, the phenomenon where missing or nonexistent
objects are used to explain an image is referred to as object bias (or
hallucination). To mitigate this issue, we propose a target-aware prompting
strategy. This method first extracts object labels and their spatial
information from the image using an object detector. Then, an attribute
predictor further refines the semantic features of the objects. These refined
features are subsequently integrated and fed into the decoder, enhancing the
model's understanding of the image context. Experimental results on the COCO
and nocaps datasets demonstrate that OPCap effectively mitigates hallucination
and significantly improves the quality of generated captions.",['cs.CV'],http://arxiv.org/abs/2412.00095v2
Explainable artificial intelligence (XAI): from inherent explainability to large language models,"Artificial Intelligence (AI) has continued to achieve tremendous success in
recent times. However, the decision logic of these frameworks is often not
transparent, making it difficult for stakeholders to understand, interpret or
explain their behavior. This limitation hinders trust in machine learning
systems and causes a general reluctance towards their adoption in practical
applications, particularly in mission-critical domains like healthcare and
autonomous driving. Explainable AI (XAI) techniques facilitate the
explainability or interpretability of machine learning models, enabling users
to discern the basis of the decision and possibly avert undesirable behavior.
This comprehensive survey details the advancements of explainable AI methods,
from inherently interpretable models to modern approaches for achieving
interpretability of various black box models, including large language models
(LLMs). Additionally, we review explainable AI techniques that leverage LLM and
vision-language model (VLM) frameworks to automate or improve the
explainability of other machine learning models. The use of LLM and VLM as
interpretability methods particularly enables high-level, semantically
meaningful explanations of model decisions and behavior. Throughout the paper,
we highlight the scientific principles, strengths and weaknesses of
state-of-the-art methods and outline different areas of improvement. Where
appropriate, we also present qualitative and quantitative comparison results of
various methods to show how they compare. Finally, we discuss the key
challenges of XAI and directions for future research.","['cs.LG', 'cs.AI', 'cs.CV']",http://arxiv.org/abs/2501.09967v1
A Mechanistic Explanatory Strategy for XAI,"Despite significant advancements in XAI, scholars note a persistent lack of
solid conceptual foundations and integration with broader scientific discourse
on explanation. In response, emerging XAI research draws on explanatory
strategies from various sciences and philosophy of science literature to fill
these gaps. This paper outlines a mechanistic strategy for explaining the
functional organization of deep learning systems, situating recent advancements
in AI explainability within a broader philosophical context. According to the
mechanistic approach, the explanation of opaque AI systems involves identifying
mechanisms that drive decision-making. For deep neural networks, this means
discerning functionally relevant components -- such as neurons, layers,
circuits, or activation patterns -- and understanding their roles through
decomposition, localization, and recomposition. Proof-of-principle case studies
from image recognition and language modeling align these theoretical approaches
with the latest research from AI labs like OpenAI and Anthropic. This research
suggests that a systematic approach to studying model organization can reveal
elements that simpler (or ''more modest'') explainability techniques might
miss, fostering more thoroughly explainable AI. The paper concludes with a
discussion on the epistemic relevance of the mechanistic approach positioned in
the context of selected philosophical debates on XAI.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2411.01332v2
Normal-NeRF: Ambiguity-Robust Normal Estimation for Highly Reflective Scenes,"Neural Radiance Fields (NeRF) often struggle with reconstructing and
rendering highly reflective scenes. Recent advancements have developed various
reflection-aware appearance models to enhance NeRF's capability to render
specular reflections. However, the robust reconstruction of highly reflective
scenes is still hindered by the inherent shape ambiguity on specular surfaces.
Existing methods typically rely on additional geometry priors to regularize the
shape prediction, but this can lead to oversmoothed geometry in complex scenes.
Observing the critical role of surface normals in parameterizing reflections,
we introduce a transmittance-gradient-based normal estimation technique that
remains robust even under ambiguous shape conditions. Furthermore, we propose a
dual activated densities module that effectively bridges the gap between smooth
surface normals and sharp object boundaries. Combined with a reflection-aware
appearance model, our proposed method achieves robust reconstruction and
high-fidelity rendering of scenes featuring both highly specular reflections
and intricate geometric structures. Extensive experiments demonstrate that our
method outperforms existing state-of-the-art methods on various datasets.",['cs.CV'],http://arxiv.org/abs/2501.09460v1
Scalable and High-Quality Neural Implicit Representation for 3D Reconstruction,"Various SDF-based neural implicit surface reconstruction methods have been
proposed recently, and have demonstrated remarkable modeling capabilities.
However, due to the global nature and limited representation ability of a
single network, existing methods still suffer from many drawbacks, such as
limited accuracy and scale of the reconstruction. In this paper, we propose a
versatile, scalable and high-quality neural implicit representation to address
these issues. We integrate a divide-and-conquer approach into the neural
SDF-based reconstruction. Specifically, we model the object or scene as a
fusion of multiple independent local neural SDFs with overlapping regions. The
construction of our representation involves three key steps: (1) constructing
the distribution and overlap relationship of the local radiance fields based on
object structure or data distribution, (2) relative pose registration for
adjacent local SDFs, and (3) SDF blending. Thanks to the independent
representation of each local region, our approach can not only achieve
high-fidelity surface reconstruction, but also enable scalable scene
reconstruction. Extensive experimental results demonstrate the effectiveness
and practicality of our proposed method.","['cs.CV', 'cs.GR']",http://arxiv.org/abs/2501.08577v1
Evaluating Human Perception of Novel View Synthesis: Subjective Quality Assessment of Gaussian Splatting and NeRF in Dynamic Scenes,"Gaussian Splatting (GS) and Neural Radiance Fields (NeRF) are two
groundbreaking technologies that have revolutionized the field of Novel View
Synthesis (NVS), enabling immersive photorealistic rendering and user
experiences by synthesizing multiple viewpoints from a set of images of sparse
views. The potential applications of NVS, such as high-quality virtual and
augmented reality, detailed 3D modeling, and realistic medical organ imaging,
underscore the importance of quality assessment of NVS methods from the
perspective of human perception. Although some previous studies have explored
subjective quality assessments for NVS technology, they still face several
challenges, especially in NVS methods selection, scenario coverage, and
evaluation methodology. To address these challenges, we conducted two
subjective experiments for the quality assessment of NVS technologies
containing both GS-based and NeRF-based methods, focusing on dynamic and
real-world scenes. This study covers 360{\deg}, front-facing, and
single-viewpoint videos while providing a richer and greater number of real
scenes. Meanwhile, it's the first time to explore the impact of NVS methods in
dynamic scenes with moving objects. The two types of subjective experiments
help to fully comprehend the influences of different viewing paths from a human
perception perspective and pave the way for future development of
full-reference and no-reference quality metrics. In addition, we established a
comprehensive benchmark of various state-of-the-art objective metrics on the
proposed database, highlighting that existing methods still struggle to
accurately capture subjective quality. The results give us some insights into
the limitations of existing NVS methods and may promote the development of new
NVS methods.","['cs.CV', 'eess.IV']",http://arxiv.org/abs/2501.08072v1
NeuroPump: Simultaneous Geometric and Color Rectification for Underwater Images,"Underwater image restoration aims to remove geometric and color distortions
due to water refraction, absorption and scattering. Previous studies focus on
restoring either color or the geometry, but to our best knowledge, not both.
However, in practice it may be cumbersome to address the two rectifications
one-by-one. In this paper, we propose NeuroPump, a self-supervised method to
simultaneously optimize and rectify underwater geometry and color as if water
were pumped out. The key idea is to explicitly model refraction, absorption and
scattering in Neural Radiance Field (NeRF) pipeline, such that it not only
performs simultaneous geometric and color rectification, but also enables to
synthesize novel views and optical effects by controlling the decoupled
parameters. In addition, to address issue of lack of real paired ground truth
images, we propose an underwater 360 benchmark dataset that has real paired
(i.e., with and without water) images. Our method clearly outperforms other
baselines both quantitatively and qualitatively. Our project page is available
at: https://ygswu.github.io/NeuroPump.github.io/.",['cs.CV'],http://arxiv.org/abs/2412.15890v2
UV-Attack: Physical-World Adversarial Attacks for Person Detection via Dynamic-NeRF-based UV Mapping,"In recent research, adversarial attacks on person detectors using patches or
static 3D model-based texture modifications have struggled with low success
rates due to the flexible nature of human movement. Modeling the 3D
deformations caused by various actions has been a major challenge. Fortunately,
advancements in Neural Radiance Fields (NeRF) for dynamic human modeling offer
new possibilities. In this paper, we introduce UV-Attack, a groundbreaking
approach that achieves high success rates even with extensive and unseen human
actions. We address the challenge above by leveraging dynamic-NeRF-based UV
mapping. UV-Attack can generate human images across diverse actions and
viewpoints, and even create novel actions by sampling from the SMPL parameter
space. While dynamic NeRF models are capable of modeling human bodies,
modifying clothing textures is challenging because they are embedded in neural
network parameters. To tackle this, UV-Attack generates UV maps instead of RGB
images and modifies the texture stacks. This approach enables real-time texture
edits and makes the attack more practical. We also propose a novel Expectation
over Pose Transformation loss (EoPT) to improve the evasion success rate on
unseen poses and views. Our experiments show that UV-Attack achieves a 92.75%
attack success rate against the FastRCNN model across varied poses in dynamic
video settings, significantly outperforming the state-of-the-art AdvCamou
attack, which only had a 28.50% ASR. Moreover, we achieve 49.5% ASR on the
latest YOLOv8 detector in black-box settings. This work highlights the
potential of dynamic NeRF-based UV mapping for creating more effective
adversarial attacks on person detectors, addressing key challenges in modeling
human movement and texture modification.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.05783v1
NeRFs are Mirror Detectors: Using Structural Similarity for Multi-View Mirror Scene Reconstruction with 3D Surface Primitives,"While neural radiance fields (NeRF) led to a breakthrough in photorealistic
novel view synthesis, handling mirroring surfaces still denotes a particular
challenge as they introduce severe inconsistencies in the scene representation.
Previous attempts either focus on reconstructing single reflective objects or
rely on strong supervision guidance in terms of additional user-provided
annotations of visible image regions of the mirrors, thereby limiting the
practical usability. In contrast, in this paper, we present NeRF-MD, a method
which shows that NeRFs can be considered as mirror detectors and which is
capable of reconstructing neural radiance fields of scenes containing mirroring
surfaces without the need for prior annotations. To this end, we first compute
an initial estimate of the scene geometry by training a standard NeRF using a
depth reprojection loss. Our key insight lies in the fact that parts of the
scene corresponding to a mirroring surface will still exhibit a significant
photometric inconsistency, whereas the remaining parts are already
reconstructed in a plausible manner. This allows us to detect mirror surfaces
by fitting geometric primitives to such inconsistent regions in this initial
stage of the training. Using this information, we then jointly optimize the
radiance field and mirror geometry in a second training stage to refine their
quality. We demonstrate the capability of our method to allow the faithful
detection of mirrors in the scene as well as the reconstruction of a single
consistent scene representation, and demonstrate its potential in comparison to
baseline and mirror-aware approaches.",['cs.CV'],http://arxiv.org/abs/2501.04074v1
NeuralSVG: An Implicit Representation for Text-to-Vector Generation,"Vector graphics are essential in design, providing artists with a versatile
medium for creating resolution-independent and highly editable visual content.
Recent advancements in vision-language and diffusion models have fueled
interest in text-to-vector graphics generation. However, existing approaches
often suffer from over-parameterized outputs or treat the layered structure - a
core feature of vector graphics - as a secondary goal, diminishing their
practical use. Recognizing the importance of layered SVG representations, we
propose NeuralSVG, an implicit neural representation for generating vector
graphics from text prompts. Inspired by Neural Radiance Fields (NeRFs),
NeuralSVG encodes the entire scene into the weights of a small MLP network,
optimized using Score Distillation Sampling (SDS). To encourage a layered
structure in the generated SVG, we introduce a dropout-based regularization
technique that strengthens the standalone meaning of each shape. We
additionally demonstrate that utilizing a neural representation provides an
added benefit of inference-time control, enabling users to dynamically adapt
the generated SVG based on user-provided inputs, all with a single learned
representation. Through extensive qualitative and quantitative evaluations, we
demonstrate that NeuralSVG outperforms existing methods in generating
structured and flexible SVG.",['cs.CV'],http://arxiv.org/abs/2501.03992v1
CoStruction: Conjoint radiance field optimization for urban scene reconStruction with limited image overlap,"Reconstructing the surrounding surface geometry from recorded driving
sequences poses a significant challenge due to the limited image overlap and
complex topology of urban environments. SoTA neural implicit surface
reconstruction methods often struggle in such setting, either failing due to
small vision overlap or exhibiting suboptimal performance in accurately
reconstructing both the surface and fine structures. To address these
limitations, we introduce CoStruction, a novel hybrid implicit surface
reconstruction method tailored for large driving sequences with limited camera
overlap. CoStruction leverages cross-representation uncertainty estimation to
filter out ambiguous geometry caused by limited observations. Our method
performs joint optimization of both radiance fields in addition to guided
sampling achieving accurate reconstruction of large areas along with fine
structures in complex urban scenarios. Extensive evaluation on major driving
datasets demonstrates the superiority of our approach in reconstructing large
driving sequences with limited image overlap, outperforming concurrent SoTA
methods.","['cs.CV', 'I.4.5, I.2.10']",http://arxiv.org/abs/2501.03932v1
AE-NeRF: Augmenting Event-Based Neural Radiance Fields for Non-ideal Conditions and Larger Scene,"Compared to frame-based methods, computational neuromorphic imaging using
event cameras offers significant advantages, such as minimal motion blur,
enhanced temporal resolution, and high dynamic range. The multi-view
consistency of Neural Radiance Fields combined with the unique benefits of
event cameras, has spurred recent research into reconstructing NeRF from data
captured by moving event cameras. While showing impressive performance,
existing methods rely on ideal conditions with the availability of uniform and
high-quality event sequences and accurate camera poses, and mainly focus on the
object level reconstruction, thus limiting their practical applications. In
this work, we propose AE-NeRF to address the challenges of learning event-based
NeRF from non-ideal conditions, including non-uniform event sequences, noisy
poses, and various scales of scenes. Our method exploits the density of event
streams and jointly learn a pose correction module with an event-based NeRF
(e-NeRF) framework for robust 3D reconstruction from inaccurate camera poses.
To generalize to larger scenes, we propose hierarchical event distillation with
a proposal e-NeRF network and a vanilla e-NeRF network to resample and refine
the reconstruction process. We further propose an event reconstruction loss and
a temporal loss to improve the view consistency of the reconstructed scene. We
established a comprehensive benchmark that includes large-scale scenes to
simulate practical non-ideal conditions, incorporating both synthetic and
challenging real-world event datasets. The experimental results show that our
method achieves a new state-of-the-art in event-based 3D reconstruction.",['cs.CV'],http://arxiv.org/abs/2501.02807v2
TexHOI: Reconstructing Textures of 3D Unknown Objects in Monocular Hand-Object Interaction Scenes,"Reconstructing 3D models of dynamic, real-world objects with high-fidelity
textures from monocular frame sequences has been a challenging problem in
recent years. This difficulty stems from factors such as shadows, indirect
illumination, and inaccurate object-pose estimations due to occluding
hand-object interactions. To address these challenges, we propose a novel
approach that predicts the hand's impact on environmental visibility and
indirect illumination on the object's surface albedo. Our method first learns
the geometry and low-fidelity texture of the object, hand, and background
through composite rendering of radiance fields. Simultaneously, we optimize the
hand and object poses to achieve accurate object-pose estimations. We then
refine physics-based rendering parameters - including roughness, specularity,
albedo, hand visibility, skin color reflections, and environmental illumination
- to produce precise albedo, and accurate hand illumination and shadow regions.
Our approach surpasses state-of-the-art methods in texture reconstruction and,
to the best of our knowledge, is the first to account for hand-object
interactions in object texture reconstruction.",['cs.CV'],http://arxiv.org/abs/2501.03525v1
BeSplat: Gaussian Splatting from a Single Blurry Image and Event Stream,"Novel view synthesis has been greatly enhanced by the development of radiance
field methods. The introduction of 3D Gaussian Splatting (3DGS) has effectively
addressed key challenges, such as long training times and slow rendering
speeds, typically associated with Neural Radiance Fields (NeRF), while
maintaining high-quality reconstructions. In this work (BeSplat), we
demonstrate the recovery of sharp radiance field (Gaussian splats) from a
single motion-blurred image and its corresponding event stream. Our method
jointly learns the scene representation via Gaussian Splatting and recovers the
camera motion through Bezier SE(3) formulation effectively, minimizing
discrepancies between synthesized and real-world measurements of both blurry
image and corresponding event stream. We evaluate our approach on both
synthetic and real datasets, showcasing its ability to render view-consistent,
sharp images from the learned radiance field and the estimated camera
trajectory. To the best of our knowledge, ours is the first work to address
this highly challenging ill-posed problem in a Gaussian Splatting framework
with the effective incorporation of temporal information captured using the
event stream.",['cs.CV'],http://arxiv.org/abs/2412.19370v2
NeRFool: Uncovering the Vulnerability of Generalizable Neural Radiance Fields against Adversarial Perturbations,"Generalizable Neural Radiance Fields (GNeRF) are one of the most promising
real-world solutions for novel view synthesis, thanks to their cross-scene
generalization capability and thus the possibility of instant rendering on new
scenes. While adversarial robustness is essential for real-world applications,
little study has been devoted to understanding its implication on GNeRF. We
hypothesize that because GNeRF is implemented by conditioning on the source
views from new scenes, which are often acquired from the Internet or
third-party providers, there are potential new security concerns regarding its
real-world applications. Meanwhile, existing understanding and solutions for
neural networks' adversarial robustness may not be applicable to GNeRF, due to
its 3D nature and uniquely diverse operations. To this end, we present NeRFool,
which to the best of our knowledge is the first work that sets out to
understand the adversarial robustness of GNeRF. Specifically, NeRFool unveils
the vulnerability patterns and important insights regarding GNeRF's adversarial
robustness. Built upon the above insights gained from NeRFool, we further
develop NeRFool+, which integrates two techniques capable of effectively
attacking GNeRF across a wide range of target views, and provide guidelines for
defending against our proposed attacks. We believe that our NeRFool/NeRFool+
lays the initial foundation for future innovations in developing robust
real-world GNeRF solutions. Our codes are available at:
https://github.com/GATECH-EIC/NeRFool.",['cs.CV'],http://arxiv.org/abs/2306.06359v2
Gen-NeRF: Efficient and Generalizable Neural Radiance Fields via Algorithm-Hardware Co-Design,"Novel view synthesis is an essential functionality for enabling immersive
experiences in various Augmented- and Virtual-Reality (AR/VR) applications, for
which generalizable Neural Radiance Fields (NeRFs) have gained increasing
popularity thanks to their cross-scene generalization capability. Despite their
promise, the real-device deployment of generalizable NeRFs is bottlenecked by
their prohibitive complexity due to the required massive memory accesses to
acquire scene features, causing their ray marching process to be
memory-bounded. To this end, we propose Gen-NeRF, an algorithm-hardware
co-design framework dedicated to generalizable NeRF acceleration, which for the
first time enables real-time generalizable NeRFs. On the algorithm side,
Gen-NeRF integrates a coarse-then-focus sampling strategy, leveraging the fact
that different regions of a 3D scene contribute differently to the rendered
pixel, to enable sparse yet effective sampling. On the hardware side, Gen-NeRF
highlights an accelerator micro-architecture to maximize the data reuse
opportunities among different rays by making use of their epipolar geometric
relationship. Furthermore, our Gen-NeRF accelerator features a customized
dataflow to enhance data locality during point-to-hardware mapping and an
optimized scene feature storage strategy to minimize memory bank conflicts.
Extensive experiments validate the effectiveness of our proposed Gen-NeRF
framework in enabling real-time and generalizable novel view synthesis.","['cs.CV', 'cs.AR']",http://arxiv.org/abs/2304.11842v4
Generic Objects as Pose Probes for Few-shot View Synthesis,"Radiance fields including NeRFs and 3D Gaussians demonstrate great potential
in high-fidelity rendering and scene reconstruction, while they require a
substantial number of posed images as inputs. COLMAP is frequently employed for
preprocessing to estimate poses, while it necessitates a large number of
feature matches to operate effectively, and it struggles with scenes
characterized by sparse features, large baselines between images, or a limited
number of input images. We aim to tackle few-view NeRF reconstruction using
only 3 to 6 unposed scene images. Traditional methods often use calibration
boards but they are not common in images. We propose a novel idea of utilizing
everyday objects, commonly found in both images and real life, as ""pose
probes"". The probe object is automatically segmented by SAM, whose shape is
initialized from a cube. We apply a dual-branch volume rendering optimization
(object NeRF and scene NeRF) to constrain the pose optimization and jointly
refine the geometry. Specifically, object poses of two views are first
estimated by PnP matching in an SDF representation, which serves as initial
poses. PnP matching, requiring only a few features, is suitable for
feature-sparse scenes. Additional views are incrementally incorporated to
refine poses from preceding views. In experiments, PoseProbe achieves
state-of-the-art performance in both pose estimation and novel view synthesis
across multiple datasets. We demonstrate its effectiveness, particularly in
few-view and large-baseline scenes where COLMAP struggles. In ablations, using
different objects in a scene yields comparable performance. Our project page is
available at: \href{https://zhirui-gao.github.io/PoseProbe.github.io/}{this
https URL}",['cs.CV'],http://arxiv.org/abs/2408.16690v3
S-NeRF++: Autonomous Driving Simulation via Neural Reconstruction and Generation,"Autonomous driving simulation system plays a crucial role in enhancing
self-driving data and simulating complex and rare traffic scenarios, ensuring
navigation safety. However, traditional simulation systems, which often heavily
rely on manual modeling and 2D image editing, struggled with scaling to
extensive scenes and generating realistic simulation data. In this study, we
present S-NeRF++, an innovative autonomous driving simulation system based on
neural reconstruction. Trained on widely-used self-driving datasets such as
nuScenes and Waymo, S-NeRF++ can generate a large number of realistic street
scenes and foreground objects with high rendering quality as well as offering
considerable flexibility in manipulation and simulation. Specifically, S-NeRF++
is an enhanced neural radiance field for synthesizing large-scale scenes and
moving vehicles, with improved scene parameterization and camera pose learning.
The system effectively utilizes noisy and sparse LiDAR data to refine training
and address depth outliers, ensuring high-quality reconstruction and novel-view
rendering. It also provides a diverse foreground asset bank by reconstructing
and generating different foreground vehicles to support comprehensive scenario
creation.Moreover, we have developed an advanced foreground-background fusion
pipeline that skillfully integrates illumination and shadow effects, further
enhancing the realism of our simulations. With the high-quality simulated data
provided by our S-NeRF++, we found the perception methods enjoy performance
boosts on several autonomous driving downstream tasks, further demonstrating
our proposed simulator's effectiveness.",['cs.CV'],http://arxiv.org/abs/2402.02112v3
Bayesian NeRF: Quantifying Uncertainty with Volume Density for Neural Implicit Fields,"We present a Bayesian Neural Radiance Field (NeRF), which explicitly
quantifies uncertainty in the volume density by modeling uncertainty in the
occupancy, without the need for additional networks, making it particularly
suited for challenging observations and uncontrolled image environments. NeRF
diverges from traditional geometric methods by providing an enriched scene
representation, rendering color and density in 3D space from various
viewpoints. However, NeRF encounters limitations in addressing uncertainties
solely through geometric structure information, leading to inaccuracies when
interpreting scenes with insufficient real-world observations. While previous
efforts have relied on auxiliary networks, we propose a series of formulation
extensions to NeRF that manage uncertainties in density, both color and
density, and occupancy, all without the need for additional networks. In
experiments, we show that our method significantly enhances performance on RGB
and depth images in the comprehensive dataset. Given that uncertainty modeling
aligns well with the inherently uncertain environments of Simultaneous
Localization and Mapping (SLAM), we applied our approach to SLAM systems and
observed notable improvements in mapping and tracking performance. These
results confirm the effectiveness of our Bayesian NeRF approach in quantifying
uncertainty based on geometric structure, making it a robust solution for
challenging real-world scenarios.",['cs.CV'],http://arxiv.org/abs/2404.06727v2
Bringing Objects to Life: 4D generation from 3D objects,"Recent advancements in generative modeling now enable the creation of 4D
content (moving 3D objects) controlled with text prompts. 4D generation has
large potential in applications like virtual worlds, media, and gaming, but
existing methods provide limited control over the appearance and geometry of
generated content. In this work, we introduce a method for animating
user-provided 3D objects by conditioning on textual prompts to guide 4D
generation, enabling custom animations while maintaining the identity of the
original object. We first convert a 3D mesh into a ``static"" 4D Neural Radiance
Field (NeRF) that preserves the visual attributes of the input object. Then, we
animate the object using an Image-to-Video diffusion model driven by text. To
improve motion realism, we introduce an incremental viewpoint selection
protocol for sampling perspectives to promote lifelike movement and a masked
Score Distillation Sampling (SDS) loss, which leverages attention maps to focus
optimization on relevant regions. We evaluate our model in terms of temporal
coherence, prompt adherence, and visual fidelity and find that our method
outperforms baselines that are based on other approaches, achieving up to
threefold improvements in identity preservation measured using LPIPS scores,
and effectively balancing visual quality with dynamic content.",['cs.CV'],http://arxiv.org/abs/2412.20422v1
Canonical Factors for Hybrid Neural Fields,"Factored feature volumes offer a simple way to build more compact, efficient,
and intepretable neural fields, but also introduce biases that are not
necessarily beneficial for real-world data. In this work, we (1) characterize
the undesirable biases that these architectures have for axis-aligned signals
-- they can lead to radiance field reconstruction differences of as high as 2
PSNR -- and (2) explore how learning a set of canonicalizing transformations
can improve representations by removing these biases. We prove in a
two-dimensional model problem that simultaneously learning these
transformations together with scene appearance succeeds with drastically
improved efficiency. We validate the resulting architectures, which we call
TILTED, using image, signed distance, and radiance field reconstruction tasks,
where we observe improvements across quality, robustness, compactness, and
runtime. Results demonstrate that TILTED can enable capabilities comparable to
baselines that are 2x larger, while highlighting weaknesses of neural field
evaluation procedures.","['cs.CV', 'cs.LG', 'math.OC']",http://arxiv.org/abs/2308.15461v2
Learning Radiance Fields from a Single Snapshot Compressive Image,"In this paper, we explore the potential of Snapshot Compressive Imaging (SCI)
technique for recovering the underlying 3D scene structure from a single
temporal compressed image. SCI is a cost-effective method that enables the
recording of high-dimensional data, such as hyperspectral or temporal
information, into a single image using low-cost 2D imaging sensors. To achieve
this, a series of specially designed 2D masks are usually employed, reducing
storage and transmission requirements and offering potential privacy
protection. Inspired by this, we take one step further to recover the encoded
3D scene information leveraging powerful 3D scene representation capabilities
of neural radiance fields (NeRF). Specifically, we propose SCINeRF, in which we
formulate the physical imaging process of SCI as part of the training of NeRF,
allowing us to exploit its impressive performance in capturing complex scene
structures. In addition, we further integrate the popular 3D Gaussian Splatting
(3DGS) framework and propose SCISplat to improve 3D scene reconstruction
quality and training/rendering speed by explicitly optimizing point clouds into
3D Gaussian representations. To assess the effectiveness of our method, we
conduct extensive evaluations using both synthetic data and real data captured
by our SCI system. Experimental results demonstrate that our proposed approach
surpasses the state-of-the-art methods in terms of image reconstruction and
novel view synthesis. Moreover, our method also exhibits the ability to render
high frame-rate multi-view consistent images in real time by leveraging SCI and
the rendering capabilities of 3DGS. Codes will be available at:
https://github.com/WU- CVGL/SCISplat.",['cs.CV'],http://arxiv.org/abs/2412.19483v1
LiHi-GS: LiDAR-Supervised Gaussian Splatting for Highway Driving Scene Reconstruction,"Photorealistic 3D scene reconstruction plays an important role in autonomous
driving, enabling the generation of novel data from existing datasets to
simulate safety-critical scenarios and expand training data without additional
acquisition costs. Gaussian Splatting (GS) facilitates real-time,
photorealistic rendering with an explicit 3D Gaussian representation of the
scene, providing faster processing and more intuitive scene editing than the
implicit Neural Radiance Fields (NeRFs). While extensive GS research has
yielded promising advancements in autonomous driving applications, they
overlook two critical aspects: First, existing methods mainly focus on
low-speed and feature-rich urban scenes and ignore the fact that highway
scenarios play a significant role in autonomous driving. Second, while LiDARs
are commonplace in autonomous driving platforms, existing methods learn
primarily from images and use LiDAR only for initial estimates or without
precise sensor modeling, thus missing out on leveraging the rich depth
information LiDAR offers and limiting the ability to synthesize LiDAR data. In
this paper, we propose a novel GS method for dynamic scene synthesis and
editing with improved scene reconstruction through LiDAR supervision and
support for LiDAR rendering. Unlike prior works that are tested mostly on urban
datasets, to the best of our knowledge, we are the first to focus on the more
challenging and highly relevant highway scenes for autonomous driving, with
sparse sensor views and monotone backgrounds. Visit our project page at:
https://umautobots.github.io/lihi_gs","['cs.CV', 'cs.RO']",http://arxiv.org/abs/2412.15447v2
Generating Editable Head Avatars with 3D Gaussian GANs,"Generating animatable and editable 3D head avatars is essential for various
applications in computer vision and graphics. Traditional 3D-aware generative
adversarial networks (GANs), often using implicit fields like Neural Radiance
Fields (NeRF), achieve photorealistic and view-consistent 3D head synthesis.
However, these methods face limitations in deformation flexibility and
editability, hindering the creation of lifelike and easily modifiable 3D heads.
We propose a novel approach that enhances the editability and animation control
of 3D head avatars by incorporating 3D Gaussian Splatting (3DGS) as an explicit
3D representation. This method enables easier illumination control and improved
editability. Central to our approach is the Editable Gaussian Head (EG-Head)
model, which combines a 3D Morphable Model (3DMM) with texture maps, allowing
precise expression control and flexible texture editing for accurate animation
while preserving identity. To capture complex non-facial geometries like hair,
we use an auxiliary set of 3DGS and tri-plane features. Extensive experiments
demonstrate that our approach delivers high-quality 3D-aware synthesis with
state-of-the-art controllability. Our code and models are available at
https://github.com/liguohao96/EGG3D.",['cs.CV'],http://arxiv.org/abs/2412.19149v1
Topology-Aware 3D Gaussian Splatting: Leveraging Persistent Homology for Optimized Structural Integrity,"Gaussian Splatting (GS) has emerged as a crucial technique for representing
discrete volumetric radiance fields. It leverages unique parametrization to
mitigate computational demands in scene optimization. This work introduces
Topology-Aware 3D Gaussian Splatting (Topology-GS), which addresses two key
limitations in current approaches: compromised pixel-level structural integrity
due to incomplete initial geometric coverage, and inadequate feature-level
integrity from insufficient topological constraints during optimization. To
overcome these limitations, Topology-GS incorporates a novel interpolation
strategy, Local Persistent Voronoi Interpolation (LPVI), and a topology-focused
regularization term based on persistent barcodes, named PersLoss. LPVI utilizes
persistent homology to guide adaptive interpolation, enhancing point coverage
in low-curvature areas while preserving topological structure. PersLoss aligns
the visual perceptual similarity of rendered images with ground truth by
constraining distances between their topological features. Comprehensive
experiments on three novel-view synthesis benchmarks demonstrate that
Topology-GS outperforms existing methods in terms of PSNR, SSIM, and LPIPS
metrics, while maintaining efficient memory usage. This study pioneers the
integration of topology with 3D-GS, laying the groundwork for future research
in this area.","['cs.CV', 'cs.LG', 'eess.IV', 'math.AT', 'math.GT']",http://arxiv.org/abs/2412.16619v2
Editing Implicit and Explicit Representations of Radiance Fields: A Survey,"Neural Radiance Fields (NeRF) revolutionized novel view synthesis in recent
years by offering a new volumetric representation, which is compact and
provides high-quality image rendering. However, the methods to edit those
radiance fields developed slower than the many improvements to other aspects of
NeRF. With the recent development of alternative radiance field-based
representations inspired by NeRF as well as the worldwide rise in popularity of
text-to-image models, many new opportunities and strategies have emerged to
provide radiance field editing. In this paper, we deliver a comprehensive
survey of the different editing methods present in the literature for NeRF and
other similar radiance field representations. We propose a new taxonomy for
classifying existing works based on their editing methodologies, review
pioneering models, reflect on current and potential new applications of
radiance field editing, and compare state-of-the-art approaches in terms of
editing options and performance.",['cs.CV'],http://arxiv.org/abs/2412.17628v1
LokiTalk: Learning Fine-Grained and Generalizable Correspondences to Enhance NeRF-based Talking Head Synthesis,"Despite significant progress in talking head synthesis since the introduction
of Neural Radiance Fields (NeRF), visual artifacts and high training costs
persist as major obstacles to large-scale commercial adoption. We propose that
identifying and establishing fine-grained and generalizable correspondences
between driving signals and generated results can simultaneously resolve both
problems. Here we present LokiTalk, a novel framework designed to enhance
NeRF-based talking heads with lifelike facial dynamics and improved training
efficiency. To achieve fine-grained correspondences, we introduce
Region-Specific Deformation Fields, which decompose the overall portrait motion
into lip movements, eye blinking, head pose, and torso movements. By
hierarchically modeling the driving signals and their associated regions
through two cascaded deformation fields, we significantly improve dynamic
accuracy and minimize synthetic artifacts. Furthermore, we propose ID-Aware
Knowledge Transfer, a plug-and-play module that learns generalizable dynamic
and static correspondences from multi-identity videos, while simultaneously
extracting ID-specific dynamic and static features to refine the depiction of
individual characters. Comprehensive evaluations demonstrate that LokiTalk
delivers superior high-fidelity results and training efficiency compared to
previous methods. The code will be released upon acceptance.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2411.19525v2
Exploring Dynamic Novel View Synthesis Technologies for Cinematography,"Novel view synthesis (NVS) has shown significant promise for applications in
cinematographic production, particularly through the exploitation of Neural
Radiance Fields (NeRF) and Gaussian Splatting (GS). These methods model real 3D
scenes, enabling the creation of new shots that are challenging to capture in
the real world due to set topology or expensive equipment requirement. This
innovation also offers cinematographic advantages such as smooth camera
movements, virtual re-shoots, slow-motion effects, etc. This paper explores
dynamic NVS with the aim of facilitating the model selection process. We
showcase its potential through a short montage filmed using various NVS models.",['cs.CV'],http://arxiv.org/abs/2412.17532v1
WavePlanes: Compact Hex Planes for Dynamic Novel View Synthesis,"Dynamic Novel View Synthesis (Dynamic NVS) enhances NVS technologies to model
moving 3-D scenes. However, current methods are resource intensive and
challenging to compress. To address this, we present WavePlanes, a fast and
more compact hex plane representation, applicable to both Neural Radiance
Fields and Gaussian Splatting methods. Rather than modeling many feature scales
separately (as done previously), we use the inverse discrete wavelet transform
to reconstruct features at varying scales. This leads to a more compact
representation and allows us to explore wavelet-based compression schemes for
further gains. The proposed compression scheme exploits the sparsity of wavelet
coefficients, by applying hard thresholding to the wavelet planes and storing
nonzero coefficients and their locations on each plane in a Hash Map. Compared
to the state-of-the-art (SotA), WavePlanes is significantly smaller, less
resource demanding and competitive in reconstruction quality. Compared to small
SotA models, WavePlanes outperforms methods in both model size and quality of
novel views.","['cs.CV', 'cs.GR']",http://arxiv.org/abs/2312.02218v4
NeRF-To-Real Tester: Neural Radiance Fields as Test Image Generators for Vision of Autonomous Systems,"Autonomous inspection of infrastructure on land and in water is a quickly
growing market, with applications including surveying constructions, monitoring
plants, and tracking environmental changes in on- and off-shore wind energy
farms. For Autonomous Underwater Vehicles and Unmanned Aerial Vehicles
overfitting of controllers to simulation conditions fundamentally leads to poor
performance in the operation environment. There is a pressing need for more
diverse and realistic test data that accurately represents the challenges faced
by these systems. We address the challenge of generating perception test data
for autonomous systems by leveraging Neural Radiance Fields to generate
realistic and diverse test images, and integrating them into a metamorphic
testing framework for vision components such as vSLAM and object detection. Our
tool, N2R-Tester, allows training models of custom scenes and rendering test
images from perturbed positions. An experimental evaluation of N2R-Tester on
eight different vision components in AUVs and UAVs demonstrates the efficacy
and versatility of the approach.",['cs.CV'],http://arxiv.org/abs/2412.16141v1
GSurf: 3D Reconstruction via Signed Distance Fields with Direct Gaussian Supervision,"Surface reconstruction from multi-view images is a core challenge in 3D
vision. Recent studies have explored signed distance fields (SDF) within Neural
Radiance Fields (NeRF) to achieve high-fidelity surface reconstructions.
However, these approaches often suffer from slow training and rendering speeds
compared to 3D Gaussian splatting (3DGS). Current state-of-the-art techniques
attempt to fuse depth information to extract geometry from 3DGS, but frequently
result in incomplete reconstructions and fragmented surfaces. In this paper, we
introduce GSurf, a novel end-to-end method for learning a signed distance field
directly from Gaussian primitives. The continuous and smooth nature of SDF
addresses common issues in the 3DGS family, such as holes resulting from noisy
or missing depth data. By using Gaussian splatting for rendering, GSurf avoids
the redundant volume rendering typically required in other GS and SDF
integrations. Consequently, GSurf achieves faster training and rendering speeds
while delivering 3D reconstruction quality comparable to neural implicit
surface methods, such as VolSDF and NeuS. Experimental results across various
benchmark datasets demonstrate the effectiveness of our method in producing
high-fidelity 3D reconstructions.",['cs.CV'],http://arxiv.org/abs/2411.15723v3
Generative Multiview Relighting for 3D Reconstruction under Extreme Illumination Variation,"Reconstructing the geometry and appearance of objects from photographs taken
in different environments is difficult as the illumination and therefore the
object appearance vary across captured images. This is particularly challenging
for more specular objects whose appearance strongly depends on the viewing
direction. Some prior approaches model appearance variation across images using
a per-image embedding vector, while others use physically-based rendering to
recover the materials and per-image illumination. Such approaches fail at
faithfully recovering view-dependent appearance given significant variation in
input illumination and tend to produce mostly diffuse results. We present an
approach that reconstructs objects from images taken under different
illuminations by first relighting the images under a single reference
illumination with a multiview relighting diffusion model and then
reconstructing the object's geometry and appearance with a radiance field
architecture that is robust to the small remaining inconsistencies among the
relit images. We validate our proposed approach on both synthetic and real
datasets and demonstrate that it greatly outperforms existing techniques at
reconstructing high-fidelity appearance from images taken under extreme
illumination variation. Moreover, our approach is particularly effective at
recovering view-dependent ""shiny"" appearance which cannot be reconstructed by
prior methods.",['cs.CV'],http://arxiv.org/abs/2412.15211v1
LiDAR-RT: Gaussian-based Ray Tracing for Dynamic LiDAR Re-simulation,"This paper targets the challenge of real-time LiDAR re-simulation in dynamic
driving scenarios. Recent approaches utilize neural radiance fields combined
with the physical modeling of LiDAR sensors to achieve high-fidelity
re-simulation results. Unfortunately, these methods face limitations due to
high computational demands in large-scale scenes and cannot perform real-time
LiDAR rendering. To overcome these constraints, we propose LiDAR-RT, a novel
framework that supports real-time, physically accurate LiDAR re-simulation for
driving scenes. Our primary contribution is the development of an efficient and
effective rendering pipeline, which integrates Gaussian primitives and
hardware-accelerated ray tracing technology. Specifically, we model the
physical properties of LiDAR sensors using Gaussian primitives with learnable
parameters and incorporate scene graphs to handle scene dynamics. Building upon
this scene representation, our framework first constructs a bounding volume
hierarchy (BVH), then casts rays for each pixel and generates novel LiDAR views
through a differentiable rendering algorithm. Importantly, our framework
supports realistic rendering with flexible scene editing operations and various
sensor configurations. Extensive experiments across multiple public benchmarks
demonstrate that our method outperforms state-of-the-art methods in terms of
rendering quality and efficiency. Our project page is at
https://zju3dv.github.io/lidar-rt.","['cs.CV', 'cs.LG', 'cs.RO']",http://arxiv.org/abs/2412.15199v1
Bright-NeRF:Brightening Neural Radiance Field with Color Restoration from Low-light Raw Images,"Neural Radiance Fields (NeRFs) have demonstrated prominent performance in
novel view synthesis. However, their input heavily relies on image acquisition
under normal light conditions, making it challenging to learn accurate scene
representation in low-light environments where images typically exhibit
significant noise and severe color distortion. To address these challenges, we
propose a novel approach, Bright-NeRF, which learns enhanced and high-quality
radiance fields from multi-view low-light raw images in an unsupervised manner.
Our method simultaneously achieves color restoration, denoising, and enhanced
novel view synthesis. Specifically, we leverage a physically-inspired model of
the sensor's response to illumination and introduce a chromatic adaptation loss
to constrain the learning of response, enabling consistent color perception of
objects regardless of lighting conditions. We further utilize the raw data's
properties to expose the scene's intensity automatically. Additionally, we have
collected a multi-view low-light raw image dataset to advance research in this
field. Experimental results demonstrate that our proposed method significantly
outperforms existing 2D and 3D approaches. Our code and dataset will be made
publicly available.","['cs.CV', 'eess.IV']",http://arxiv.org/abs/2412.14547v1
GraphAvatar: Compact Head Avatars with GNN-Generated 3D Gaussians,"Rendering photorealistic head avatars from arbitrary viewpoints is crucial
for various applications like virtual reality. Although previous methods based
on Neural Radiance Fields (NeRF) can achieve impressive results, they lack
fidelity and efficiency. Recent methods using 3D Gaussian Splatting (3DGS) have
improved rendering quality and real-time performance but still require
significant storage overhead. In this paper, we introduce a method called
GraphAvatar that utilizes Graph Neural Networks (GNN) to generate 3D Gaussians
for the head avatar. Specifically, GraphAvatar trains a geometric GNN and an
appearance GNN to generate the attributes of the 3D Gaussians from the tracked
mesh. Therefore, our method can store the GNN models instead of the 3D
Gaussians, significantly reducing the storage overhead to just 10MB. To reduce
the impact of face-tracking errors, we also present a novel graph-guided
optimization module to refine face-tracking parameters during training.
Finally, we introduce a 3D-aware enhancer for post-processing to enhance the
rendering quality. We conduct comprehensive experiments to demonstrate the
advantages of GraphAvatar, surpassing existing methods in visual fidelity and
storage consumption. The ablation study sheds light on the trade-offs between
rendering quality and model size. The code will be released at:
https://github.com/ucwxb/GraphAvatar",['cs.CV'],http://arxiv.org/abs/2412.13983v1
GN-FR:Generalizable Neural Radiance Fields for Flare Removal,"Flare, an optical phenomenon resulting from unwanted scattering and
reflections within a lens system, presents a significant challenge in imaging.
The diverse patterns of flares, such as halos, streaks, color bleeding, and
haze, complicate the flare removal process. Existing traditional and
learning-based methods have exhibited limited efficacy due to their reliance on
single-image approaches, where flare removal is highly ill-posed. We address
this by framing flare removal as a multi-view image problem, taking advantage
of the view-dependent nature of flare artifacts. This approach leverages
information from neighboring views to recover details obscured by flare in
individual images. Our proposed framework, GN-FR (Generalizable Neural Radiance
Fields for Flare Removal), can render flare-free views from a sparse set of
input images affected by lens flare and generalizes across different scenes in
an unsupervised manner. GN-FR incorporates several modules within the
Generalizable NeRF Transformer (GNT) framework: Flare-occupancy Mask Generation
(FMG), View Sampler (VS), and Point Sampler (PS). To overcome the
impracticality of capturing both flare-corrupted and flare-free data, we
introduce a masking loss function that utilizes mask information in an
unsupervised setting. Additionally, we present a 3D multi-view flare dataset,
comprising 17 real flare scenes with 782 images, 80 real flare patterns, and
their corresponding annotated flare-occupancy masks. To our knowledge, this is
the first work to address flare removal within a Neural Radiance Fields (NeRF)
framework.","['cs.CV', 'eess.IV', 'I.4.1; I.4.8']",http://arxiv.org/abs/2412.08200v2
RelationField: Relate Anything in Radiance Fields,"Neural radiance fields are an emerging 3D scene representation and recently
even been extended to learn features for scene understanding by distilling
open-vocabulary features from vision-language models. However, current method
primarily focus on object-centric representations, supporting object
segmentation or detection, while understanding semantic relationships between
objects remains largely unexplored. To address this gap, we propose
RelationField, the first method to extract inter-object relationships directly
from neural radiance fields. RelationField represents relationships between
objects as pairs of rays within a neural radiance field, effectively extending
its formulation to include implicit relationship queries. To teach
RelationField complex, open-vocabulary relationships, relationship knowledge is
distilled from multi-modal LLMs. To evaluate RelationField, we solve
open-vocabulary 3D scene graph generation tasks and relationship-guided
instance segmentation, achieving state-of-the-art performance in both tasks.
See the project website at https://relationfield.github.io.",['cs.CV'],http://arxiv.org/abs/2412.13652v1
Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance Fields,"Novel-view synthesis is an important problem in computer vision with
applications in 3D reconstruction, mixed reality, and robotics. Recent methods
like 3D Gaussian Splatting (3DGS) have become the preferred method for this
task, providing high-quality novel views in real time. However, the training
time of a 3DGS model is slow, often taking 30 minutes for a scene with 200
views. In contrast, our goal is to reduce the optimization time by training for
fewer steps while maintaining high rendering quality. Specifically, we combine
the guidance from both the position error and the appearance error to achieve a
more effective densification. To balance the rate between adding new Gaussians
and fitting old Gaussians, we develop a convergence-aware budget control
mechanism. Moreover, to make the densification process more reliable, we
selectively add new Gaussians from mostly visited regions. With these designs,
we reduce the Gaussian optimization steps to one-third of the previous approach
while achieving a comparable or even better novel view rendering quality. To
further facilitate the rapid fitting of 4K resolution images, we introduce a
dilation-based rendering technique. Our method, Turbo-GS, speeds up
optimization for typical scenes and scales well to high-resolution (4K)
scenarios on standard datasets. Through extensive experiments, we show that our
method is significantly faster in optimization than other methods while
retaining quality. Project page: https://ivl.cs.brown.edu/research/turbo-gs.",['cs.CV'],http://arxiv.org/abs/2412.13547v1
Optimize the Unseen -- Fast NeRF Cleanup with Free Space Prior,"Neural Radiance Fields (NeRF) have advanced photorealistic novel view
synthesis, but their reliance on photometric reconstruction introduces
artifacts, commonly known as ""floaters"". These artifacts degrade novel view
quality, especially in areas unseen by the training cameras. We present a fast,
post-hoc NeRF cleanup method that eliminates such artifacts by enforcing our
Free Space Prior, effectively minimizing floaters without disrupting the NeRF's
representation of observed regions. Unlike existing approaches that rely on
either Maximum Likelihood (ML) estimation to fit the data or a complex, local
data-driven prior, our method adopts a Maximum-a-Posteriori (MAP) approach,
selecting the optimal model parameters under a simple global prior assumption
that unseen regions should remain empty. This enables our method to clean
artifacts in both seen and unseen areas, enhancing novel view quality even in
challenging scene regions. Our method is comparable with existing NeRF cleanup
models while being 2.5x faster in inference time, requires no additional memory
beyond the original NeRF, and achieves cleanup training in less than 30
seconds. Our code will be made publically available.",['cs.CV'],http://arxiv.org/abs/2412.12772v2
AdvIRL: Reinforcement Learning-Based Adversarial Attacks on 3D NeRF Models,"The increasing deployment of AI models in critical applications has exposed
them to significant risks from adversarial attacks. While adversarial
vulnerabilities in 2D vision models have been extensively studied, the threat
landscape for 3D generative models, such as Neural Radiance Fields (NeRF),
remains underexplored. This work introduces \textit{AdvIRL}, a novel framework
for crafting adversarial NeRF models using Instant Neural Graphics Primitives
(Instant-NGP) and Reinforcement Learning. Unlike prior methods, \textit{AdvIRL}
generates adversarial noise that remains robust under diverse 3D
transformations, including rotations and scaling, enabling effective black-box
attacks in real-world scenarios. Our approach is validated across a wide range
of scenes, from small objects (e.g., bananas) to large environments (e.g.,
lighthouses). Notably, targeted attacks achieved high-confidence
misclassifications, such as labeling a banana as a slug and a truck as a
cannon, demonstrating the practical risks posed by adversarial NeRFs. Beyond
attacking, \textit{AdvIRL}-generated adversarial models can serve as
adversarial training data to enhance the robustness of vision systems. The
implementation of \textit{AdvIRL} is publicly available at
\url{https://github.com/Tommy-Nguyen-cpu/AdvIRL/tree/MultiView-Clean}, ensuring
reproducibility and facilitating future research.","['cs.CV', 'cs.AI', 'cs.CY', 'cs.GR', 'eess.IV']",http://arxiv.org/abs/2412.16213v1
CATSplat: Context-Aware Transformer with Spatial Guidance for Generalizable 3D Gaussian Splatting from A Single-View Image,"Recently, generalizable feed-forward methods based on 3D Gaussian Splatting
have gained significant attention for their potential to reconstruct 3D scenes
using finite resources. These approaches create a 3D radiance field,
parameterized by per-pixel 3D Gaussian primitives, from just a few images in a
single forward pass. However, unlike multi-view methods that benefit from
cross-view correspondences, 3D scene reconstruction with a single-view image
remains an underexplored area. In this work, we introduce CATSplat, a novel
generalizable transformer-based framework designed to break through the
inherent constraints in monocular settings. First, we propose leveraging
textual guidance from a visual-language model to complement insufficient
information from a single image. By incorporating scene-specific contextual
details from text embeddings through cross-attention, we pave the way for
context-aware 3D scene reconstruction beyond relying solely on visual cues.
Moreover, we advocate utilizing spatial guidance from 3D point features toward
comprehensive geometric understanding under single-view settings. With 3D
priors, image features can capture rich structural insights for predicting 3D
Gaussians without multi-view techniques. Extensive experiments on large-scale
datasets demonstrate the state-of-the-art performance of CATSplat in
single-view 3D scene reconstruction with high-quality novel view synthesis.",['cs.CV'],http://arxiv.org/abs/2412.12906v1
SBAMDT: Bayesian Additive Decision Trees with Adaptive Soft Semi-multivariate Split Rules,"Bayesian Additive Regression Trees [BART, Chipman et al., 2010] have gained
significant popularity due to their remarkable predictive performance and
ability to quantify uncertainty. However, standard decision tree models rely on
recursive data splits at each decision node, using deterministic decision rules
based on a single univariate feature. This approach limits their ability to
effectively capture complex decision boundaries, particularly in scenarios
involving multiple features, such as spatial domains, or when transitions are
either sharp or smoothly varying. In this paper, we introduce a novel
probabilistic additive decision tree model that employs a soft split rule. This
method enables highly flexible splits that leverage both univariate and
multivariate features, while also respecting the geometric properties of the
feature domain. Notably, the probabilistic split rule adapts dynamically across
decision nodes, allowing the model to account for varying levels of smoothness
in the regression function. We demonstrate the utility of the proposed model
through comparisons with existing tree-based models on synthetic datasets and a
New York City education dataset.","['stat.ML', 'cs.LG', 'math.ST', 'stat.ME', 'stat.TH']",http://arxiv.org/abs/2501.09900v1
"Benchmarking Classical, Deep, and Generative Models for Human Activity Recognition","Human Activity Recognition (HAR) has gained significant importance with the
growing use of sensor-equipped devices and large datasets. This paper evaluates
the performance of three categories of models : classical machine learning,
deep learning architectures, and Restricted Boltzmann Machines (RBMs) using
five key benchmark datasets of HAR (UCI-HAR, OPPORTUNITY, PAMAP2, WISDM, and
Berkeley MHAD). We assess various models, including Decision Trees, Random
Forests, Convolutional Neural Networks (CNN), and Deep Belief Networks (DBNs),
using metrics such as accuracy, precision, recall, and F1-score for a
comprehensive comparison. The results show that CNN models offer superior
performance across all datasets, especially on the Berkeley MHAD. Classical
models like Random Forest do well on smaller datasets but face challenges with
larger, more complex data. RBM-based models also show notable potential,
particularly for feature learning. This paper offers a detailed comparison to
help researchers choose the most suitable model for HAR tasks.","['cs.CV', 'cs.AI', '68T05', 'I.2.1']",http://arxiv.org/abs/2501.08471v1
Decoding Interpretable Logic Rules from Neural Networks,"As deep neural networks continue to excel across various domains, their
black-box nature has raised concerns about transparency and trust. In
particular, interpretability has become increasingly essential for applications
that demand high safety and knowledge rigor, such as drug discovery, autonomous
driving, and genomics. However, progress in understanding even the simplest
deep neural networks - such as fully connected networks - has been limited,
despite their role as foundational elements in state-of-the-art models like
ResNet and Transformer. In this paper, we address this challenge by introducing
NeuroLogic, a novel approach for decoding interpretable logic rules from neural
networks. NeuroLogic leverages neural activation patterns to capture the
model's critical decision-making processes, translating them into logical rules
represented by hidden predicates. Thanks to its flexible design in the
grounding phase, NeuroLogic can be adapted to a wide range of neural networks.
For simple fully connected neural networks, hidden predicates can be grounded
in certain split patterns of original input features to derive
decision-tree-like rules. For large, complex vision neural networks, NeuroLogic
grounds hidden predicates into high-level visual concepts that are
understandable to humans. Our empirical study demonstrates that NeuroLogic can
extract global and interpretable rules from state-of-the-art models such as
ResNet, a task at which existing work struggles. We believe NeuroLogic can help
pave the way for understanding the black-box nature of neural networks.",['cs.LG'],http://arxiv.org/abs/2501.08281v1
Guiding the classification of hepatocellular carcinoma on 3D CT-scans using deep and handcrafted radiological features,"Hepatocellular carcinoma is the most spread primary liver cancer across the
world ($\sim$80\% of the liver tumors). The gold standard for HCC diagnosis is
liver biopsy. However, in the clinical routine, expert radiologists provide a
visual diagnosis by interpreting hepatic CT-scans according to a standardized
protocol, the LI-RADS, which uses five radiological criteria with an associated
decision tree. In this paper, we propose an automatic approach to predict
histology-proven HCC from CT images in order to reduce radiologists'
inter-variability. We first show that standard deep learning methods fail to
accurately predict HCC from CT-scans on a challenging database, and propose a
two-step approach inspired by the LI-RADS system to improve the performance. We
achieve improvements from 6 to 18 points of AUC with respect to deep learning
baselines trained with different architectures. We also provide clinical
validation of our method, achieving results that outperform non-expert
radiologists and are on par with expert ones.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.08097v1
"A New Flexible Train-Test Split Algorithm, an approach for choosing among the Hold-out, K-fold cross-validation, and Hold-out iteration","Artificial Intelligent transformed industries, like engineering, medicine,
finance. Predictive models use supervised learning, a vital Machine learning
subset. Crucial for model evaluation, cross-validation includes
re-substitution, hold-out, and K-fold. This study focuses on improving the
accuracy of ML algorithms across three different datasets. To evaluate
Hold-out, Hold-out with iteration, and K-fold Cross-Validation techniques, we
created a flexible Python program. By modifying parameters like test size,
Random State, and 'k' values, we were able to improve accuracy assessment. The
outcomes demonstrate the Hold-out validation method's persistent superiority,
particularly with a test size of 10%. With iterations and Random State
settings, hold-out with iteration shows little accuracy variance. It suggests
that there are variances according to algorithm, with Decision Tree doing best
for Framingham and Naive Bayes and K Nearest Neighbors for COVID-19. Different
datasets require different optimal K values in K-Fold Cross Validation,
highlighting these considerations. This study challenges the universality of K
values in K-Fold Cross Validation and suggests a 10% test size and 90% training
size for better outcomes. It also emphasizes the contextual impact of dataset
features, sample size, feature count, and selected methodologies. Researchers
can adapt these codes for their dataset to obtain highest accuracy with
specific evaluation.",['cs.LG'],http://arxiv.org/abs/2501.06492v1
Soft regression trees: a model variant and a decomposition training algorithm,"Decision trees are widely used for classification and regression tasks in a
variety of application fields due to their interpretability and good accuracy.
During the past decade, growing attention has been devoted to globally
optimized decision trees with deterministic or soft splitting rules at branch
nodes, which are trained by optimizing the error function over all the tree
parameters. In this work, we propose a new variant of soft multivariate
regression trees (SRTs) where, for every input vector, the prediction is
defined as the linear regression associated to a single leaf node, namely, the
leaf node obtained by routing the input vector from the root along the branches
with higher probability. SRTs exhibit the conditional computational property,
i.e., each prediction depends on a small number of nodes (parameters), and our
nonlinear optimization formulation for training them is amenable to
decomposition. After showing a universal approximation result for SRTs, we
present a decomposition training algorithm including a clustering-based
initialization procedure and a heuristic for reassigning the input vectors
along the tree. Under mild assumptions, we establish asymptotic convergence
guarantees. Experiments on 15 wellknown datasets indicate that our SRTs and
decomposition algorithm yield higher accuracy and robustness compared with
traditional soft regression trees trained using the nonlinear optimization
formulation of Blanquero et al., and a significant reduction in training times
as well as a slightly better average accuracy compared with the mixed-integer
optimization approach of Bertsimas and Dunn. We also report a comparison with
the Random Forest ensemble method.","['cs.LG', 'math.OC']",http://arxiv.org/abs/2501.05942v1
Methodology for Interpretable Reinforcement Learning for Optimizing Mechanical Ventilation,"Mechanical ventilation is a critical life support intervention that delivers
controlled air and oxygen to a patient's lungs, assisting or replacing
spontaneous breathing. While several data-driven approaches have been proposed
to optimize ventilator control strategies, they often lack interpretability and
alignment with domain knowledge, hindering clinical adoption. This paper
presents a methodology for interpretable reinforcement learning (RL) aimed at
improving mechanical ventilation control as part of connected health systems.
Using a causal, nonparametric model-based off-policy evaluation, we assess RL
policies for their ability to enhance patient-specific outcomes-specifically,
increasing blood oxygen levels (SpO2), while avoiding aggressive ventilator
settings that may cause ventilator-induced lung injuries and other
complications. Through numerical experiments on real-world ICU data from the
MIMIC-III database, we demonstrate that our interpretable decision tree policy
achieves performance comparable to state-of-the-art deep RL methods while
outperforming standard behavior cloning approaches. The results highlight the
potential of interpretable, data-driven decision support systems to improve
safety and efficiency in personalized ventilation strategies, paving the way
for seamless integration into connected healthcare environments.","['cs.LG', 'math.OC']",http://arxiv.org/abs/2404.03105v2
Representation Learning of Lab Values via Masked AutoEncoder,"Accurate imputation of missing laboratory values in electronic health records
(EHRs) is critical to enable robust clinical predictions and reduce biases in
AI systems in healthcare. Existing methods, such as variational autoencoders
(VAEs) and decision tree-based approaches such as XGBoost, struggle to model
the complex temporal and contextual dependencies in EHR data, mainly in
underrepresented groups. In this work, we propose Lab-MAE, a novel
transformer-based masked autoencoder framework that leverages self-supervised
learning for the imputation of continuous sequential lab values. Lab-MAE
introduces a structured encoding scheme that jointly models laboratory test
values and their corresponding timestamps, enabling explicit capturing temporal
dependencies. Empirical evaluation on the MIMIC-IV dataset demonstrates that
Lab-MAE significantly outperforms the state-of-the-art baselines such as
XGBoost across multiple metrics, including root mean square error (RMSE),
R-squared (R2), and Wasserstein distance (WD). Notably, Lab-MAE achieves
equitable performance across demographic groups of patients, advancing fairness
in clinical predictions. We further investigate the role of follow-up
laboratory values as potential shortcut features, revealing Lab-MAE's
robustness in scenarios where such data is unavailable. The findings suggest
that our transformer-based architecture, adapted to the characteristics of the
EHR data, offers a foundation model for more accurate and fair clinical
imputation models. In addition, we measure and compare the carbon footprint of
Lab-MAE with the baseline XGBoost model, highlighting its environmental
requirements.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2501.02648v2
Towards understanding the bias in decision trees,"There is a widespread and longstanding belief that machine learning models
are biased towards the majority (or negative) class when learning from
imbalanced data, leading them to neglect or ignore the minority (or positive)
class. In this study, we show that this belief is not necessarily correct for
decision trees, and that their bias can actually be in the opposite direction.
Motivated by a recent simulation study that suggested that decision trees can
be biased towards the minority class, our paper aims to reconcile the conflict
between that study and decades of other works. First, we critically evaluate
past literature on this problem, finding that failing to consider the data
generating process has led to incorrect conclusions about the bias in decision
trees. We then prove that, under specific conditions related to the predictors,
decision trees fit to purity and trained on a dataset with only one positive
case are biased towards the minority class. Finally, we demonstrate that splits
in a decision tree are also biased when there is more than one positive case.
Our findings have implications on the use of popular tree-based models, such as
random forests.","['stat.ML', 'cs.LG']",http://arxiv.org/abs/2501.04903v1
Explainable Reinforcement Learning for Formula One Race Strategy,"In Formula One, teams compete to develop their cars and achieve the highest
possible finishing position in each race. During a race, however, teams are
unable to alter the car, so they must improve their cars' finishing positions
via race strategy, i.e. optimising their selection of which tyre compounds to
put on the car and when to do so. In this work, we introduce a reinforcement
learning model, RSRL (Race Strategy Reinforcement Learning), to control race
strategies in simulations, offering a faster alternative to the industry
standard of hard-coded and Monte Carlo-based race strategies. Controlling cars
with a pace equating to an expected finishing position of P5.5 (where P1
represents first place and P20 is last place), RSRL achieves an average
finishing position of P5.33 on our test race, the 2023 Bahrain Grand Prix,
outperforming the best baseline of P5.63. We then demonstrate, in a
generalisability study, how performance for one track or multiple tracks can be
prioritised via training. Further, we supplement model predictions with feature
importance, decision tree-based surrogate models, and decision tree
counterfactuals towards improving user trust in the model. Finally, we provide
illustrations which exemplify our approach in real-world situations, drawing
parallels between simulations and reality.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2501.04068v1
Explainable Time Series Prediction of Tyre Energy in Formula One Race Strategy,"Formula One (F1) race strategy takes place in a high-pressure and fast-paced
environment where split-second decisions can drastically affect race results.
Two of the core decisions of race strategy are when to make pit stops (i.e.
replace the cars' tyres) and which tyre compounds (hard, medium or soft, in
normal conditions) to select. The optimal pit stop decisions can be determined
by estimating the tyre degradation of these compounds, which in turn can be
computed from the energy applied to each tyre, i.e. the tyre energy. In this
work, we trained deep learning models, using the Mercedes-AMG PETRONAS F1
team's historic race data consisting of telemetry, to forecast tyre energies
during races. Additionally, we fitted XGBoost, a decision tree-based machine
learning algorithm, to the same dataset and compared the results, with both
giving impressive performance. Furthermore, we incorporated two different
explainable AI methods, namely feature importance and counterfactual
explanations, to gain insights into the reasoning behind the forecasts. Our
contributions thus result in an explainable, automated method which could
assist F1 teams in optimising their race strategy.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2501.04067v1
Optimization of Transformer heart disease prediction model based on particle swarm optimization algorithm,"Aiming at the latest particle swarm optimization algorithm, this paper
proposes an improved Transformer model to improve the accuracy of heart disease
prediction and provide a new algorithm idea. We first use three mainstream
machine learning classification algorithms - decision tree, random forest and
XGBoost, and then output the confusion matrix of these three models. The
results showed that the random forest model had the best performance in
predicting the classification of heart disease, with an accuracy of 92.2%.
Then, we apply the Transformer model based on particle swarm optimization (PSO)
algorithm to the same dataset for classification experiment. The results show
that the classification accuracy of the model is as high as 96.5%, 4.3
percentage points higher than that of random forest, which verifies the
effectiveness of PSO in optimizing Transformer model. From the above research,
we can see that particle swarm optimization significantly improves Transformer
performance in heart disease prediction. Improving the ability to predict heart
disease is a global priority with benefits for all humankind. Accurate
prediction can enhance public health, optimize medical resources, and reduce
healthcare costs, leading to healthier populations and more productive
societies worldwide. This advancement paves the way for more efficient health
management and supports the foundation of a healthier, more resilient global
community.",['cs.AI'],http://arxiv.org/abs/2412.02801v3
Can Explainable AI Assess Personalized Health Risks from Indoor Air Pollution?,"Acknowledging the effects of outdoor air pollution, the literature
inadequately addresses indoor air pollution's impacts. Despite daily health
risks, existing research primarily focused on monitoring, lacking accuracy in
pinpointing indoor pollution sources. In our research work, we thoroughly
investigated the influence of indoor activities on pollution levels. A survey
of 143 participants revealed limited awareness of indoor air pollution.
Leveraging 65 days of diverse data encompassing activities like incense stick
usage, indoor smoking, inadequately ventilated cooking, excessive AC usage, and
accidental paper burning, we developed a comprehensive monitoring system. We
identify pollutant sources and effects with high precision through clustering
analysis and interpretability models (LIME and SHAP). Our method integrates
Decision Trees, Random Forest, Naive Bayes, and SVM models, excelling at 99.8%
accuracy with Decision Trees. Continuous 24-hour data allows personalized
assessments for targeted pollution reduction strategies, achieving 91% accuracy
in predicting activities and pollution exposure.",['cs.LG'],http://arxiv.org/abs/2501.06222v1
Practical machine learning is learning on small samples,"Based on limited observations, machine learning discerns a dependence which
is expected to hold in the future. What makes it possible? Statistical learning
theory imagines indefinitely increasing training sample to justify its
approach. In reality, there is no infinite time or even infinite general
population for learning. Here I argue that practical machine learning is based
on an implicit assumption that underlying dependence is relatively ``smooth"" :
likely, there are no abrupt differences in feedback between cases with close
data points. From this point of view learning shall involve selection of the
hypothesis ``smoothly"" approximating the training set. I formalize this as
Practical learning paradigm. The paradigm includes terminology and rules for
description of learners. Popular learners (local smoothing, k-NN, decision
trees, Naive Bayes, SVM for classification and for regression) are shown here
to be implementations of this paradigm.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2501.01836v1
Probabilistic Explanations for Linear Models,"Formal XAI is an emerging field that focuses on providing explanations with
mathematical guarantees for the decisions made by machine learning models. A
significant amount of work in this area is centered on the computation of
""sufficient reasons"". Given a model $M$ and an input instance $\vec{x}$, a
sufficient reason for the decision $M(\vec{x})$ is a subset $S$ of the features
of $\vec{x}$ such that for any instance $\vec{z}$ that has the same values as
$\vec{x}$ for every feature in $S$, it holds that $M(\vec{x}) = M(\vec{z})$.
Intuitively, this means that the features in $S$ are sufficient to fully
justify the classification of $\vec{x}$ by $M$. For sufficient reasons to be
useful in practice, they should be as small as possible, and a natural way to
reduce the size of sufficient reasons is to consider a probabilistic
relaxation; the probability of $M(\vec{x}) = M(\vec{z})$ must be at least some
value $\delta \in (0,1]$, for a random instance $\vec{z}$ that coincides with
$\vec{x}$ on the features in $S$. Computing small $\delta$-sufficient reasons
($\delta$-SRs) is known to be a theoretically hard problem; even over decision
trees--traditionally deemed simple and interpretable models--strong
inapproximability results make the efficient computation of small $\delta$-SRs
unlikely. We propose the notion of $(\delta, \epsilon)$-SR, a simple relaxation
of $\delta$-SRs, and show that this kind of explanation can be computed
efficiently over linear models.","['cs.AI', 'cs.CC']",http://arxiv.org/abs/2501.00154v1
An Integrated Optimization and Deep Learning Pipeline for Predicting Live Birth Success in IVF Using Feature Optimization and Transformer-Based Models,"In vitro fertilization (IVF) is a widely utilized assisted reproductive
technology, yet predicting its success remains challenging due to the
multifaceted interplay of clinical, demographic, and procedural factors. This
study develops a robust artificial intelligence (AI) pipeline aimed at
predicting live birth outcomes in IVF treatments. The pipeline uses anonymized
data from 2010 to 2018, obtained from the Human Fertilization and Embryology
Authority (HFEA). We evaluated the prediction performance of live birth success
as a binary outcome (success/failure) by integrating different feature
selection methods, such as principal component analysis (PCA) and particle
swarm optimization (PSO), with different traditional machine learning-based
classifiers including random forest (RF) and decision tree, as well as deep
learning-based classifiers including custom transformer-based model and a tab
transformer model with an attention mechanism. Our research demonstrated that
the best performance was achieved by combining PSO for feature selection with
the TabTransformer-based deep learning model, yielding an accuracy of 99.50%
and an AUC of 99.96%, highlighting its significant performance to predict live
births. This study establishes a highly accurate AI pipeline for predicting
live birth outcomes in IVF, demonstrating its potential to enhance personalized
fertility treatments.",['cs.AI'],http://arxiv.org/abs/2412.19696v1
DOFEN: Deep Oblivious Forest ENsemble,"Deep Neural Networks (DNNs) have revolutionized artificial intelligence,
achieving impressive results on diverse data types, including images, videos,
and texts. However, DNNs still lag behind Gradient Boosting Decision Trees
(GBDT) on tabular data, a format extensively utilized across various domains.
In this paper, we propose DOFEN, short for \textbf{D}eep \textbf{O}blivious
\textbf{F}orest \textbf{EN}semble, a novel DNN architecture inspired by
oblivious decision trees. DOFEN constructs relaxed oblivious decision trees
(rODTs) by randomly combining conditions for each column and further enhances
performance with a two-level rODT forest ensembling process. By employing this
approach, DOFEN achieves state-of-the-art results among DNNs and further
narrows the gap between DNNs and tree-based models on the well-recognized
benchmark: Tabular Benchmark \citep{grinsztajn2022tree}, which includes 73
total datasets spanning a wide array of domains. The code of DOFEN is available
at: \url{https://github.com/Sinopac-Digital-Technology-Division/DOFEN}.","['cs.LG', 'stat.ML']",http://arxiv.org/abs/2412.16534v2
IMVB7t: A Multi-Modal Model for Food Preferences based on Artificially Produced Traits,"Human behavior and interactions are profoundly influenced by visual stimuli
present in their surroundings. This influence extends to various aspects of
life, notably food consumption and selection. In our study, we employed various
models to extract different attributes from the environmental images.
Specifically, we identify five key attributes and employ an ensemble model
IMVB7 based on five distinct models for some of their detection resulted 0.85
mark. In addition, we conducted surveys to discern patterns in food preferences
in response to visual stimuli. Leveraging the insights gleaned from these
surveys, we formulate recommendations using decision tree for dishes based on
the amalgamation of identified attributes resulted IMVB7t 0.96 mark. This study
serves as a foundational step, paving the way for further exploration of this
interdisciplinary domain.",['cs.CV'],http://arxiv.org/abs/2412.16807v1
Little is Enough: Boosting Privacy by Sharing Only Hard Labels in Federated Semi-Supervised Learning,"In many critical applications, sensitive data is inherently distributed and
cannot be centralized due to privacy concerns. A wide range of federated
learning approaches have been proposed to train models locally at each client
without sharing their sensitive data, typically by exchanging model parameters,
or probabilistic predictions (soft labels) on a public dataset or a combination
of both. However, these methods still disclose private information and restrict
local models to those that can be trained using gradient-based methods. We
propose a federated co-training (FedCT) approach that improves privacy by
sharing only definitive (hard) labels on a public unlabeled dataset. Clients
use a consensus of these shared labels as pseudo-labels for local training.
This federated co-training approach empirically enhances privacy without
compromising model quality. In addition, it allows the use of local models that
are not suitable for parameter aggregation in traditional federated learning,
such as gradient-boosted decision trees, rule ensembles, and random forests.
Furthermore, we observe that FedCT performs effectively in federated
fine-tuning of large language models, where its pseudo-labeling mechanism is
particularly beneficial. Empirical evaluations and theoretical analyses suggest
its applicability across a range of federated learning scenarios.",['cs.LG'],http://arxiv.org/abs/2310.05696v4
Machine Learning Techniques for Pattern Recognition in High-Dimensional Data Mining,"This paper proposes a frequent pattern data mining algorithm based on support
vector machine (SVM), aiming to solve the performance bottleneck of traditional
frequent pattern mining algorithms in high-dimensional and sparse data
environments. By converting the frequent pattern mining task into a
classification problem, the SVM model is introduced to improve the accuracy and
robustness of pattern extraction. In terms of method design, the kernel
function is used to map the data to a high-dimensional feature space, so as to
construct the optimal classification hyperplane, realize the nonlinear
separation of patterns and the accurate mining of frequent items. In the
experiment, two public datasets, Retail and Mushroom, were selected to compare
and analyze the proposed algorithm with traditional FP-Growth, FP-Tree,
decision tree and random forest models. The experimental results show that the
algorithm in this paper is significantly better than the traditional model in
terms of three key indicators: support, confidence and lift, showing strong
pattern recognition ability and rule extraction effect. The study shows that
the SVM model has excellent performance advantages in an environment with high
data sparsity and a large number of transactions, and can effectively cope with
complex pattern mining tasks. At the same time, this paper also points out the
potential direction of future research, including the introduction of deep
learning and ensemble learning frameworks to further improve the scalability
and adaptability of the algorithm. This research not only provides a new idea
for frequent pattern mining, but also provides important technical support for
solving pattern discovery and association rule mining problems in practical
applications.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2412.15593v1
Two-level Solar Irradiance Clustering with Season Identification: A Comparative Analysis,"Solar irradiance clustering can enhance solar power capacity planning and
help improve forecasting models by identifying similar irradiance patterns
influenced by seasonal and weather changes. In this study, we adopt an
efficient two-level clustering approach to automatically identify seasons using
the clear sky irradiance in first level and subsequently to identify daily
cloud level as clear, cloudy and partly cloudy within each season in second
level. In the second level of clustering, three methods are compared, namely,
Daily Irradiance Index (DII or $\beta$), Euclidean Distance (ED), and Dynamic
Time Warping (DTW) distance. The DII is computed as the ratio of time integral
of measured irradiance to time integral of the clear sky irradiance. The
identified clusters were compared quantitatively using established clustering
metrics and qualitatively by comparing the mean irradiance profiles. The
results clearly establish the superiority of the $\beta$-based clustering
approach as the leader, setting a new benchmark for solar irradiance clustering
studies. Moreover, $\beta$-based clustering remains effective even for annual
data unlike the time-series methods which suffer significant performance
degradation. Interestingly, contrary to expectations, ED-based clustering
outperforms the more compute-intensive DTW distance-based clustering. The
method has been rigorously validated using data from two distinct US locations,
demonstrating robust scalability for larger datasets and potential
applicability for other locations.",['cs.LG'],http://arxiv.org/abs/2501.10084v1
Cueless EEG imagined speech for subject identification: dataset and benchmarks,"Electroencephalogram (EEG) signals have emerged as a promising modality for
biometric identification. While previous studies have explored the use of
imagined speech with semantically meaningful words for subject identification,
most have relied on additional visual or auditory cues. In this study, we
introduce a cueless EEG-based imagined speech paradigm, where subjects imagine
the pronunciation of semantically meaningful words without any external cues.
This innovative approach addresses the limitations of prior methods by
requiring subjects to select and imagine words from a predefined list
naturally. The dataset comprises over 4,350 trials from 11 subjects across five
sessions. We assess a variety of classification methods, including traditional
machine learning techniques such as Support Vector Machines (SVM) and XGBoost,
as well as time-series foundation models and deep learning architectures
specifically designed for EEG classification, such as EEG Conformer and Shallow
ConvNet. A session-based hold-out validation strategy was employed to ensure
reliable evaluation and prevent data leakage. Our results demonstrate
outstanding classification accuracy, reaching 97.93%. These findings highlight
the potential of cueless EEG paradigms for secure and reliable subject
identification in real-world applications, such as brain-computer interfaces
(BCIs).","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2501.09700v1
AALF: Almost Always Linear Forecasting,"Recent works for time-series forecasting more and more leverage the high
predictive power of Deep Learning models. With this increase in model
complexity, however, comes a lack in understanding of the underlying model
decision process, which is problematic for high-stakes application scenarios.
At the same time, simple, interpretable forecasting methods such as ARIMA still
perform very well, sometimes on-par, with Deep Learning approaches. We argue
that simple models are good enough most of the time, and that forecasting
performance could be improved by choosing a Deep Learning method only for few,
important predictions, increasing the overall interpretability of the
forecasting process. In this context, we propose a novel online model selection
framework which learns to identify these predictions. An extensive empirical
study on various real-world datasets shows that our selection methodology
performs comparable to state-of-the-art online model selections methods in most
cases while being significantly more interpretable. We find that almost always
choosing a simple autoregressive linear model for forecasting results in
competitive performance, suggesting that the need for opaque black-box models
in time-series forecasting might be smaller than recent works would suggest.",['cs.LG'],http://arxiv.org/abs/2409.10142v2
Hidden Markov Neural Networks,"We define an evolving in-time Bayesian neural network called a Hidden Markov
Neural Network, which addresses the crucial challenge in time-series
forecasting and continual learning: striking a balance between adapting to new
data and appropriately forgetting outdated information. This is achieved by
modelling the weights of a neural network as the hidden states of a Hidden
Markov model, with the observed process defined by the available data. A
filtering algorithm is employed to learn a variational approximation of the
evolving-in-time posterior distribution over the weights. By leveraging a
sequential variant of Bayes by Backprop, enriched with a stronger
regularization technique called variational DropConnect, Hidden Markov Neural
Networks achieve robust regularization and scalable inference. Experiments on
MNIST, dynamic classification tasks, and next-frame forecasting in videos
demonstrate that Hidden Markov Neural Networks provide strong predictive
performance while enabling effective uncertainty quantification.","['stat.ML', 'cs.LG']",http://arxiv.org/abs/2004.06963v3
Free-Knots Kolmogorov-Arnold Network: On the Analysis of Spline Knots and Advancing Stability,"Kolmogorov-Arnold Neural Networks (KANs) have gained significant attention in
the machine learning community. However, their implementation often suffers
from poor training stability and heavy trainable parameter. Furthermore, there
is limited understanding of the behavior of the learned activation functions
derived from B-splines. In this work, we analyze the behavior of KANs through
the lens of spline knots and derive the lower and upper bound for the number of
knots in B-spline-based KANs. To address existing limitations, we propose a
novel Free Knots KAN that enhances the performance of the original KAN while
reducing the number of trainable parameters to match the trainable parameter
scale of standard Multi-Layer Perceptrons (MLPs). Additionally, we introduce
new a training strategy to ensure $C^2$ continuity of the learnable spline,
resulting in smoother activation compared to the original KAN and improve the
training stability by range expansion. The proposed method is comprehensively
evaluated on 8 datasets spanning various domains, including image, text, time
series, multimodal, and function approximation tasks. The promising results
demonstrates the feasibility of KAN-based network and the effectiveness of
proposed method.",['cs.LG'],http://arxiv.org/abs/2501.09283v1
Adaptive Law-Based Transformation (ALT): A Lightweight Feature Representation for Time Series Classification,"Time series classification (TSC) is fundamental in numerous domains,
including finance, healthcare, and environmental monitoring. However,
traditional TSC methods often struggle with the inherent complexity and
variability of time series data. Building on our previous work with the linear
law-based transformation (LLT) - which improved classification accuracy by
transforming the feature space based on key data patterns - we introduce
adaptive law-based transformation (ALT). ALT enhances LLT by incorporating
variable-length shifted time windows, enabling it to capture distinguishing
patterns of various lengths and thereby handle complex time series more
effectively. By mapping features into a linearly separable space, ALT provides
a fast, robust, and transparent solution that achieves state-of-the-art
performance with only a few hyperparameters.","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML', '62H30, 68T10, 62M10', 'I.5; I.2.0; G.3']",http://arxiv.org/abs/2501.09217v1
Deep Self-Supervised Disturbance Mapping with the OPERA Sentinel-1 Radiometric Terrain Corrected SAR Backscatter Product,"Mapping land surface disturbances supports disaster response, resource and
ecosystem management, and climate adaptation efforts. Synthetic aperture radar
(SAR) is an invaluable tool for disturbance mapping, providing consistent
time-series images of the ground regardless of weather or illumination
conditions. Despite SAR's potential for disturbance mapping, processing SAR
data to an analysis-ready format requires expertise and significant compute
resources, particularly for large-scale global analysis. In October 2023,
NASA's Observational Products for End-Users from Remote Sensing Analysis
(OPERA) project released the near-global Radiometric Terrain Corrected SAR
backscatter from Sentinel-1 (RTC-S1) dataset, providing publicly available,
analysis-ready SAR imagery. In this work, we utilize this new dataset to
systematically analyze land surface disturbances. As labeling SAR data is often
prohibitively time-consuming, we train a self-supervised vision transformer -
which requires no labels to train - on OPERA RTC-S1 data to estimate a
per-pixel distribution from the set of baseline imagery and assess disturbances
when there is significant deviation from the modeled distribution. To test our
model's capability and generality, we evaluate three different natural
disasters - which represent high-intensity, abrupt disturbances - from three
different regions of the world. Across events, our approach yields high quality
delineations: F1 scores exceeding 0.6 and Areas Under the Precision-Recall
Curve exceeding 0.65, consistently outperforming existing SAR disturbance
methods. Our findings suggest that a self-supervised vision transformer is
well-suited for global disturbance mapping and can be a valuable tool for
operational, near-global disturbance monitoring, particularly when labeled data
does not exist.","['cs.CV', 'cs.LG', 'eess.IV', 'J.2; I.2.6; I.4.8']",http://arxiv.org/abs/2501.09129v1
Surrogate Modeling for Explainable Predictive Time Series Corrections,"We introduce a local surrogate approach for explainable time-series
forecasting. An initially non-interpretable predictive model to improve the
forecast of a classical time-series 'base model' is used. 'Explainability' of
the correction is provided by fitting the base model again to the data from
which the error prediction is removed (subtracted), yielding a difference in
the model parameters which can be interpreted. We provide illustrative examples
to demonstrate the potential of the method to discover and explain underlying
patterns in the data.","['stat.ML', 'cs.LG']",http://arxiv.org/abs/2412.19897v2
A Discrete-sequence Dataset for Evaluating Online Unsupervised Anomaly Detection Approaches for Multivariate Time Series,"Benchmarking anomaly detection approaches for multivariate time series is
challenging due to the lack of high-quality datasets. Current publicly
available datasets are too small, not diverse and feature trivial anomalies,
which hinders measurable progress in this research area. We propose a solution:
a diverse, extensive, and non-trivial dataset generated via state-of-the-art
simulation tools that reflects realistic behaviour of an automotive powertrain,
including its multivariate, dynamic and variable-state properties. To cater for
both unsupervised and semi-supervised anomaly detection settings, as well as
time series generation and forecasting, we make different versions of the
dataset available, where training and test subsets are offered in contaminated
and clean versions, depending on the task. We also provide baseline results
from a small selection of approaches based on deterministic and variational
autoencoders, as well as a non-parametric approach. As expected, the baseline
experimentation shows that the approaches trained on the semi-supervised
version of the dataset outperform their unsupervised counterparts, highlighting
a need for approaches more robust to contaminated training data.","['cs.LG', 'cs.AI', 'cs.CE', 'cs.SY', 'eess.SY']",http://arxiv.org/abs/2411.13951v3
Kolmogorov-Arnold Networks for Time Series Granger Causality Inference,"We introduce Granger Causality Kolmogorov-Arnold Networks (GCKAN), an
innovative architecture that extends the recently proposed Kolmogorov-Arnold
Networks (KAN) to the domain of causal inference. By extracting base weights
from KAN layers and incorporating the sparsity-inducing penalty along with
ridge regularization, GCKAN infers the Granger causality from time series while
enabling automatic time lag selection. Additionally, we propose an algorithm
leveraging time-reversed Granger causality to enhance inference accuracy. The
algorithm compares prediction and sparse-inducing losses derived from the
original and time-reversed series, automatically selecting the casual
relationship with the higher score or integrating the results to mitigate
spurious connectivities. Comprehensive experiments conducted on Lorenz-96, gene
regulatory networks, fMRI BOLD signals, and VAR datasets demonstrate that the
proposed model achieves competitive performance to state-of-the-art methods in
inferring Granger causality from nonlinear, high-dimensional, and
limited-sample time series.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2501.08958v1
Volterra Accentuated Non-Linear Dynamical Admittance (VANYA) to model Deforestation: An Exemplification from the Amazon Rainforest,"Intelligent automation supports us against cyclones, droughts, and seismic
events with recent technology advancements. Algorithmic learning has advanced
fields like neuroscience, genetics, and human-computer interaction. Time-series
data boosts progress. Challenges persist in adopting these approaches in
traditional fields. Neural networks face comprehension and bias issues. AI's
expansion across scientific areas is due to adaptable descriptors and
combinatorial argumentation. This article focuses on modeling Forest loss using
the VANYA Model, incorporating Prey Predator Dynamics. VANYA predicts forest
cover, demonstrated on Amazon Rainforest data against other forecasters like
Long Short-Term Memory, N-BEATS, RCN.",['cs.LG'],http://arxiv.org/abs/2308.06471v2
Transformer-based Multivariate Time Series Anomaly Localization,"With the growing complexity of Cyber-Physical Systems (CPS) and the
integration of Internet of Things (IoT), the use of sensors for online
monitoring generates large volume of multivariate time series (MTS) data.
Consequently, the need for robust anomaly diagnosis in MTS is paramount to
maintaining system reliability and safety. While significant advancements have
been made in anomaly detection, localization remains a largely underexplored
area, though crucial for intelligent decision-making. This paper introduces a
novel transformer-based model for unsupervised anomaly diagnosis in MTS, with a
focus on improving localization performance, through an in-depth analysis of
the self-attention mechanism's learning behavior under both normal and
anomalous conditions. We formulate the anomaly localization problem as a
three-stage process: time-step, window, and segment-based. This leads to the
development of the Space-Time Anomaly Score (STAS), a new metric inspired by
the connection between transformer latent representations and space-time
statistical models. STAS is designed to capture individual anomaly behaviors
and inter-series dependencies, delivering enhanced localization performance.
Additionally, the Statistical Feature Anomaly Score (SFAS) complements STAS by
analyzing statistical features around anomalies, with their combination helping
to reduce false alarms. Experiments on real world and synthetic datasets
illustrate the model's superiority over state-of-the-art methods in both
detection and localization tasks.",['cs.LG'],http://arxiv.org/abs/2501.08628v1
CT-PatchTST: Channel-Time Patch Time-Series Transformer for Long-Term Renewable Energy Forecasting,"Accurately predicting renewable energy output is crucial for the efficient
integration of solar and wind power into modern energy systems. This study
develops and evaluates an advanced deep learning model, Channel-Time Patch
Time-Series Transformer (CT-PatchTST), to forecast the power output of
photovoltaic and wind energy systems using annual offshore wind power, onshore
wind power, and solar power generation data from Denmark. While the original
Patch Time-Series Transformer(PatchTST) model employs a channel-independent
(CI) approach, it tends to overlook inter-channel relationships during
training, potentially leading to a loss of critical information. To address
this limitation and further leverage the benefits of increased data granularity
brought by CI, we propose CT-PatchTST. This enhanced model improves the
processing of inter-channel information while maintaining the advantages of the
channel-independent approach. The predictive performance of CT-PatchTST is
rigorously analyzed, demonstrating its ability to provide precise and reliable
energy forecasts. This work contributes to improving the predictability of
renewable energy systems, supporting their broader adoption and integration
into energy grids.",['cs.LG'],http://arxiv.org/abs/2501.08620v1
Statistical Properties of Deep Neural Networks with Dependent Data,"This paper establishes statistical properties of deep neural network (DNN)
estimators under dependent data. Two general results for nonparametric sieve
estimators directly applicable to DNN estimators are given. The first
establishes rates for convergence in probability under nonstationary data. The
second provides non-asymptotic probability bounds on $\mathcal{L}^{2}$-errors
under stationary $\beta$-mixing data. I apply these results to DNN estimators
in both regression and classification contexts imposing only a standard
H\""older smoothness assumption. The DNN architectures considered are common in
applications, featuring fully connected feedforward networks with any
continuous piecewise linear activation function, unbounded weights, and a width
and depth that grows with sample size. The framework provided also offers
potential for research into other DNN architectures and time-series
applications.","['stat.ML', 'cs.LG', 'econ.EM', '62G05 (Primary) 68T07, 62M10 (Secondary)', 'G.3']",http://arxiv.org/abs/2410.11113v3
GRAPPA -- A Hybrid Graph Neural Network for Predicting Pure Component Vapor Pressures,"Although the pure component vapor pressure is one of the most important
properties for designing chemical processes, no broadly applicable,
sufficiently accurate, and open-source prediction method has been available. To
overcome this, we have developed GRAPPA - a hybrid graph neural network for
predicting vapor pressures of pure components. GRAPPA enables the prediction of
the vapor pressure curve of basically any organic molecule, requiring only the
molecular structure as input. The new model consists of three parts: A graph
attention network for the message passing step, a pooling function that
captures long-range interactions, and a prediction head that yields the
component-specific parameters of the Antoine equation, from which the vapor
pressure can readily and consistently be calculated for any temperature. We
have trained and evaluated GRAPPA on experimental vapor pressure data of almost
25,000 pure components. We found excellent prediction accuracy for unseen
components, outperforming state-of-the-art group contribution methods and other
machine learning approaches in applicability and accuracy. The trained model
and its code are fully disclosed, and GRAPPA is directly applicable via the
interactive website ml-prop.mv.rptu.de.","['cs.LG', 'cs.CE']",http://arxiv.org/abs/2501.08729v1
Interpreting Equivariant Representations,"Latent representations are used extensively for downstream tasks, such as
visualization, interpolation or feature extraction of deep learning models.
Invariant and equivariant neural networks are powerful and well-established
models for enforcing inductive biases. In this paper, we demonstrate that the
inductive bias imposed on the by an equivariant model must also be taken into
account when using latent representations. We show how not accounting for the
inductive biases leads to decreased performance on downstream tasks, and vice
versa, how accounting for inductive biases can be done effectively by using an
invariant projection of the latent representations. We propose principles for
how to choose such a projection, and show the impact of using these principles
in two common examples: First, we study a permutation equivariant variational
auto-encoder trained for molecule graph generation; here we show that invariant
projections can be designed that incur no loss of information in the resulting
invariant representation. Next, we study a rotation-equivariant representation
used for image classification. Here, we illustrate how random invariant
projections can be used to obtain an invariant representation with a high
degree of retained information. In both cases, the analysis of invariant latent
representations proves superior to their equivariant counterparts. Finally, we
illustrate that the phenomena documented here for equivariant neural networks
have counterparts in standard neural networks where invariance is encouraged
via augmentation. Thus, while these ambiguities may be known by experienced
developers of equivariant models, we make both the knowledge as well as
effective tools to handle the ambiguities available to the broader community.","['cs.LG', 'stat.ML']",http://arxiv.org/abs/2401.12588v2
Score-based 3D molecule generation with neural fields,"We introduce a new representation for 3D molecules based on their continuous
atomic density fields. Using this representation, we propose a new model based
on walk-jump sampling for unconditional 3D molecule generation in the
continuous space using neural fields. Our model, FuncMol, encodes molecular
fields into latent codes using a conditional neural field, samples noisy codes
from a Gaussian-smoothed distribution with Langevin MCMC (walk), denoises these
samples in a single step (jump), and finally decodes them into molecular
fields. FuncMol performs all-atom generation of 3D molecules without
assumptions on the molecular structure and scales well with the size of
molecules, unlike most approaches. Our method achieves competitive results on
drug-like molecules and easily scales to macro-cyclic peptides, with at least
one order of magnitude faster sampling. The code is available at
https://github.com/prescient-design/funcmol.","['cs.LG', 'q-bio.BM']",http://arxiv.org/abs/2501.08508v1
Avoiding subtraction and division of stochastic signals using normalizing flows: NFdeconvolve,"Across the scientific realm, we find ourselves subtracting or dividing
stochastic signals. For instance, consider a stochastic realization, $x$,
generated from the addition or multiplication of two stochastic signals $a$ and
$b$, namely $x=a+b$ or $x = ab$. For the $x=a+b$ example, $a$ can be
fluorescence background and $b$ the signal of interest whose statistics are to
be learned from the measured $x$. Similarly, when writing $x=ab$, $a$ can be
thought of as the illumination intensity and $b$ the density of fluorescent
molecules of interest. Yet dividing or subtracting stochastic signals amplifies
noise, and we ask instead whether, using the statistics of $a$ and the
measurement of $x$ as input, we can recover the statistics of $b$. Here, we
show how normalizing flows can generate an approximation of the probability
distribution over $b$, thereby avoiding subtraction or division altogether.
This method is implemented in our software package, NFdeconvolve, available on
GitHub with a tutorial linked in the main text.","['stat.ML', 'cs.LG', 'math.PR', 'physics.data-an', 'q-bio.QM']",http://arxiv.org/abs/2501.08288v1
GDiffRetro: Retrosynthesis Prediction with Dual Graph Enhanced Molecular Representation and Diffusion Generation,"Retrosynthesis prediction focuses on identifying reactants capable of
synthesizing a target product. Typically, the retrosynthesis prediction
involves two phases: Reaction Center Identification and Reactant Generation.
However, we argue that most existing methods suffer from two limitations in the
two phases: (i) Existing models do not adequately capture the ``face''
information in molecular graphs for the reaction center identification. (ii)
Current approaches for the reactant generation predominantly use sequence
generation in a 2D space, which lacks versatility in generating reasonable
distributions for completed reactive groups and overlooks molecules' inherent
3D properties. To overcome the above limitations, we propose GDiffRetro. For
the reaction center identification, GDiffRetro uniquely integrates the original
graph with its corresponding dual graph to represent molecular structures,
which helps guide the model to focus more on the faces in the graph. For the
reactant generation, GDiffRetro employs a conditional diffusion model in 3D to
further transform the obtained synthon into a complete reactant. Our
experimental findings reveal that GDiffRetro outperforms state-of-the-art
semi-template models across various evaluative metrics.",['cs.AI'],http://arxiv.org/abs/2501.08001v1
FaceXBench: Evaluating Multimodal LLMs on Face Understanding,"Multimodal Large Language Models (MLLMs) demonstrate impressive
problem-solving abilities across a wide range of tasks and domains. However,
their capacity for face understanding has not been systematically studied. To
address this gap, we introduce FaceXBench, a comprehensive benchmark designed
to evaluate MLLMs on complex face understanding tasks. FaceXBench includes
5,000 multimodal multiple-choice questions derived from 25 public datasets and
a newly created dataset, FaceXAPI. These questions cover 14 tasks across 6
broad categories, assessing MLLMs' face understanding abilities in bias and
fairness, face authentication, recognition, analysis, localization and tool
retrieval. Using FaceXBench, we conduct an extensive evaluation of 26
open-source MLLMs alongside 2 proprietary models, revealing the unique
challenges in complex face understanding tasks. We analyze the models across
three evaluation settings: zero-shot, in-context task description, and
chain-of-thought prompting. Our detailed analysis reveals that current MLLMs,
including advanced models like GPT-4o, and GeminiPro 1.5, show significant room
for improvement. We believe FaceXBench will be a crucial resource for
developing MLLMs equipped to perform sophisticated face understanding. Code:
https://github.com/Kartik-3004/facexbench",['cs.CV'],http://arxiv.org/abs/2501.10360v1
MVTamperBench: Evaluating Robustness of Vision-Language Models,"Multimodal Large Language Models (MLLMs) have driven major advances in video
understanding, yet their vulnerability to adversarial tampering and
manipulations remains underexplored. To address this gap, we introduce
MVTamperBench, a benchmark that systematically evaluates MLLM robustness
against five prevalent tampering techniques: rotation, masking, substitution,
repetition, and dropping. Built from 3.4K original videos-expanded to over 17K
tampered clips spanning 19 video tasks.
  MVTamperBench challenges models to detect manipulations in spatial and
temporal coherence. We evaluate 45 recent MLLMs from 15+ model families,
revealing substantial variability in resilience across tampering types and
showing that larger parameter counts do not necessarily guarantee robustness.
MVTamperBench sets a new benchmark for developing tamper-resilient MLLM in
safety-critical applications, including detecting clickbait, preventing harmful
content distribution, and enforcing policies on media platforms. We release all
code and data to foster open research in trustworthy video understanding.
  Code: https://amitbcp.github.io/MVTamperBench/ Data:
https://huggingface.co/datasets/Srikant86/MVTamperBench","['cs.CV', '68T37, 68T05, 68Q32, 68T45, 94A08, 68T40, 68Q85', 'I.2.10; I.2.7; I.5.4; I.4.9; I.4.8; H.5.1']",http://arxiv.org/abs/2412.19794v4
Moonshine: Distilling Game Content Generators into Steerable Generative Models,"Procedural Content Generation via Machine Learning (PCGML) has enhanced game
content creation, yet challenges in controllability and limited training data
persist. This study addresses these issues by distilling a constructive PCG
algorithm into a controllable PCGML model. We first generate a large amount of
content with a constructive algorithm and label it using a Large Language Model
(LLM). We use these synthetic labels to condition two PCGML models for
content-specific generation, a diffusion model and the five-dollar model. This
neural network distillation process ensures that the generation aligns with the
original algorithm while introducing controllability through plain text. We
define this text-conditioned PCGML as a Text-to-game-Map (T2M) task, offering
an alternative to prevalent text-to-image multi-modal tasks. We compare our
distilled models with the baseline constructive algorithm. Our analysis of the
variety, accuracy, and quality of our generation demonstrates the efficacy of
distilling constructive methods into controllable text-conditioned PCGML
models.","['cs.AI', 'I.2.1']",http://arxiv.org/abs/2408.09594v2
Large Language Model is Secretly a Protein Sequence Optimizer,"We consider the protein sequence engineering problem, which aims to find
protein sequences with high fitness levels, starting from a given wild-type
sequence. Directed evolution has been a dominating paradigm in this field which
has an iterative process to generate variants and select via experimental
feedback. We demonstrate large language models (LLMs), despite being trained on
massive texts, are secretly protein sequence optimizers. With a directed
evolutionary method, LLM can perform protein engineering through Pareto and
experiment-budget constrained optimization, demonstrating success on both
synthetic and experimental fitness landscapes.","['cs.LG', 'cs.AI', 'q-bio.QM']",http://arxiv.org/abs/2501.09274v2
Jailbreaking as a Reward Misspecification Problem,"The widespread adoption of large language models (LLMs) has raised concerns
about their safety and reliability, particularly regarding their vulnerability
to adversarial attacks. In this paper, we propose a novel perspective that
attributes this vulnerability to reward misspecification during the alignment
process. This misspecification occurs when the reward function fails to
accurately capture the intended behavior, leading to misaligned model outputs.
We introduce a metric ReGap to quantify the extent of reward misspecification
and demonstrate its effectiveness and robustness in detecting harmful backdoor
prompts. Building upon these insights, we present ReMiss, a system for
automated red teaming that generates adversarial prompts in a
reward-misspecified space. ReMiss achieves state-of-the-art attack success
rates on the AdvBench benchmark against various target aligned LLMs while
preserving the human readability of the generated prompts. Furthermore, these
attacks on open-source models demonstrate high transferability to closed-source
models like GPT-4o and out-of-distribution tasks from HarmBench. Detailed
analysis highlights the unique advantages of the proposed reward
misspecification objective compared to previous methods, offering new insights
for improving LLM safety and robustness.","['cs.LG', 'cs.CL']",http://arxiv.org/abs/2406.14393v4
Generative Artificial Intelligence: Implications for Biomedical and Health Professions Education,"Generative AI has had a profound impact on biomedicine and health, both in
professional work and in education. Based on large language models (LLMs),
generative AI has been found to perform as well as humans in simulated
situations taking medical board exams, answering clinical questions, solving
clinical cases, applying clinical reasoning, and summarizing information.
Generative AI is also being used widely in education, performing well in
academic courses and their assessments. This review summarizes the successes of
LLMs and highlights some of their challenges in the context of education, most
notably aspects that may undermines the acquisition of knowledge and skills for
professional work. It then provides recommendations for best practices
overcoming shortcomings for LLM use in education. Although there are challenges
for use of generative AI in education, all students and faculty, in biomedicine
and health and beyond, must have understanding and be competent in its use.",['cs.AI'],http://arxiv.org/abs/2501.10186v1
Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents,"The recent rapid development of large language models (LLMs) has sparked a
new wave of technological revolution in medical artificial intelligence (AI).
While LLMs are designed to understand and generate text like a human,
autonomous agents that utilize LLMs as their ""brain"" have exhibited
capabilities beyond text processing such as planning, reflection, and using
tools by enabling their ""bodies"" to interact with the environment. We introduce
a simulacrum of hospital called Agent Hospital that simulates the entire
process of treating illness, in which all patients, nurses, and doctors are
LLM-powered autonomous agents. Within the simulacrum, doctor agents are able to
evolve by treating a large number of patient agents without the need to label
training data manually. After treating tens of thousands of patient agents in
the simulacrum (human doctors may take several years in the real world), the
evolved doctor agents outperform state-of-the-art medical agent methods on the
MedQA benchmark comprising US Medical Licensing Examination (USMLE) test
questions. Our methods of simulacrum construction and agent evolution have the
potential in benefiting a broad range of applications beyond medical AI.",['cs.AI'],http://arxiv.org/abs/2405.02957v3
LLM-Based Routing in Mixture of Experts: A Novel Framework for Trading,"Recent advances in deep learning and large language models (LLMs) have
facilitated the deployment of the mixture-of-experts (MoE) mechanism in the
stock investment domain. While these models have demonstrated promising trading
performance, they are often unimodal, neglecting the wealth of information
available in other modalities, such as textual data. Moreover, the traditional
neural network-based router selection mechanism fails to consider contextual
and real-world nuances, resulting in suboptimal expert selection. To address
these limitations, we propose LLMoE, a novel framework that employs LLMs as the
router within the MoE architecture. Specifically, we replace the conventional
neural network-based router with LLMs, leveraging their extensive world
knowledge and reasoning capabilities to select experts based on historical
price data and stock news. This approach provides a more effective and
interpretable selection mechanism. Our experiments on multimodal real-world
stock datasets demonstrate that LLMoE outperforms state-of-the-art MoE models
and other deep neural network approaches. Additionally, the flexible
architecture of LLMoE allows for easy adaptation to various downstream tasks.","['cs.LG', 'q-fin.TR']",http://arxiv.org/abs/2501.09636v2
LLM Reasoner and Automated Planner: A new NPC approach,"In domains requiring intelligent agents to emulate plausible human-like
behaviour, such as formative simulations, traditional techniques like behaviour
trees encounter significant challenges. Large Language Models (LLMs), despite
not always yielding optimal solutions, usually offer plausible and human-like
responses to a given problem. In this paper, we exploit this capability and
propose a novel architecture that integrates an LLM for decision-making with a
classical automated planner that can generate sound plans for that decision.
The combination aims to equip an agent with the ability to make decisions in
various situations, even if they were not anticipated during the design phase.",['cs.AI'],http://arxiv.org/abs/2501.10106v1
LLM360 K2: Building a 65B 360-Open-Source Large Language Model from Scratch,"We detail the training of the LLM360 K2-65B model, scaling up our 360-degree
OPEN SOURCE approach to the largest and most powerful models under project
LLM360. While open-source LLMs continue to advance, the answer to ""How are the
largest LLMs trained?"" remains unclear within the community. The implementation
details for such high-capacity models are often protected due to business
considerations associated with their high cost. This lack of transparency
prevents LLM researchers from leveraging valuable insights from prior
experience, e.g., ""What are the best practices for addressing loss spikes?"" The
LLM360 K2 project addresses this gap by providing full transparency and access
to resources accumulated during the training of LLMs at the largest scale. This
report highlights key elements of the K2 project, including our first model, K2
DIAMOND, a 65 billion-parameter LLM that surpasses LLaMA-65B and rivals
LLaMA2-70B, while requiring fewer FLOPs and tokens. We detail the
implementation steps and present a longitudinal analysis of K2 DIAMOND's
capabilities throughout its training process. We also outline ongoing projects
such as TXT360, setting the stage for future models in the series. By offering
previously unavailable resources, the K2 project also resonates with the
360-degree OPEN SOURCE principles of transparency, reproducibility, and
accessibility, which we believe are vital in the era of resource-intensive AI
research.",['cs.LG'],http://arxiv.org/abs/2501.07124v3
FiLo++: Zero-/Few-Shot Anomaly Detection by Fused Fine-Grained Descriptions and Deformable Localization,"Anomaly detection methods typically require extensive normal samples from the
target class for training, limiting their applicability in scenarios that
require rapid adaptation, such as cold start. Zero-shot and few-shot anomaly
detection do not require labeled samples from the target class in advance,
making them a promising research direction. Existing zero-shot and few-shot
approaches often leverage powerful multimodal models to detect and localize
anomalies by comparing image-text similarity. However, their handcrafted
generic descriptions fail to capture the diverse range of anomalies that may
emerge in different objects, and simple patch-level image-text matching often
struggles to localize anomalous regions of varying shapes and sizes. To address
these issues, this paper proposes the FiLo++ method, which consists of two key
components. The first component, Fused Fine-Grained Descriptions (FusDes),
utilizes large language models to generate anomaly descriptions for each object
category, combines both fixed and learnable prompt templates and applies a
runtime prompt filtering method, producing more accurate and task-specific
textual descriptions. The second component, Deformable Localization (DefLoc),
integrates the vision foundation model Grounding DINO with position-enhanced
text descriptions and a Multi-scale Deformable Cross-modal Interaction (MDCI)
module, enabling accurate localization of anomalies with various shapes and
sizes. In addition, we design a position-enhanced patch matching approach to
improve few-shot anomaly detection performance. Experiments on multiple
datasets demonstrate that FiLo++ achieves significant performance improvements
compared with existing methods. Code will be available at
https://github.com/CASIA-IVA-Lab/FiLo.",['cs.CV'],http://arxiv.org/abs/2501.10067v1
Accelerating Large Language Models through Partially Linear Feed-Forward Network,"Large language models (LLMs) demonstrate remarkable capabilities but face
deployment challenges due to their massive parameter counts. While existing
compression techniques like pruning can reduce model size, it leads to
significant accuracy degradation under high compression ratios. We present a
novel perspective inspired by constant folding in compiler optimization. Our
approach enables parameter reduction by treating activation functions in LLMs
as linear functions.
  However, recent LLMs use complex non-linear activations like GELU that
prevent direct application of this technique. We propose TARDIS, which enables
optimization of LLMs with non-linear activations by partially approximating
them with linear functions in frequently occurring input ranges. For outlier
inputs, TARDIS employs an online predictor to dynamically fall back to original
computations.
  Our experiments demonstrate that TARDIS achieves 80% parameter reduction in
feed-forward networks, while significantly outperforming state-of-the-art
pruning methods Wanda and RIA with up to 65% higher accuracy. In practical
deployments for a 7B model, TARDIS achieves 1.6x end-to-end inference speedup
when integrated with the vLLM serving system, and 1.4x speedup with the widely
adopted HuggingFace implementation, while incurring only a 10.9% accuracy
trade-off.","['cs.LG', 'cs.AI', 'cs.PL', 'D.4; I.2; D.3.4']",http://arxiv.org/abs/2501.10054v1
AirRAG: Activating Intrinsic Reasoning for Retrieval Augmented Generation via Tree-based Search,"Leveraging the autonomous decision-making capabilities of large language
models (LLMs) demonstrates superior performance in reasoning tasks. Despite the
successes of iterative or recursive retrieval-augmented generation (RAG), they
often are trapped in a single solution space when confronted with complex
tasks. In this paper, we propose a novel thinking pattern in RAG which
integrates system analysis with efficient reasoning actions, significantly
activating intrinsic reasoning capabilities and expanding the solution space of
specific tasks via Monte Carlo Tree Search (MCTS), dubbed AirRAG. Specifically,
our approach designs five fundamental reasoning actions that are expanded to a
wide tree-based reasoning spaces using MCTS. The extension also uses
self-consistency verification to explore potential reasoning paths and
implement inference scaling. In addition, computationally optimal strategies
are used to apply more inference computation to key actions to achieve further
performance improvements. Experimental results demonstrate the effectiveness of
AirRAG through considerable performance gains over complex QA datasets.
Furthermore, AirRAG is flexible and lightweight, making it easy to integrate
with other advanced technologies.",['cs.AI'],http://arxiv.org/abs/2501.10053v1
Aligning Instruction Tuning with Pre-training,"Instruction tuning enhances large language models (LLMs) to follow human
instructions across diverse tasks, relying on high-quality datasets to guide
behavior. However, these datasets, whether manually curated or synthetically
generated, are often narrowly focused and misaligned with the broad
distributions captured during pre-training, limiting LLM generalization and
effective use of pre-trained knowledge. We propose *Aligning Instruction Tuning
with Pre-training* (AITP), a method that bridges this gap by identifying
coverage shortfalls in instruction-tuning datasets and rewriting
underrepresented pre-training data into high-quality instruction-response
pairs. This approach enriches dataset diversity while preserving task-specific
objectives. Evaluations on three fully open LLMs across eight benchmarks
demonstrate consistent performance improvements with AITP. Ablations highlight
the benefits of adaptive data selection, controlled rewriting, and balanced
integration, emphasizing the importance of aligning instruction tuning with
pre-training distributions to unlock the full potential of LLMs.",['cs.AI'],http://arxiv.org/abs/2501.09368v2
Mitigating Hallucinations on Object Attributes using Multiview Images and Negative Instructions,"Current popular Large Vision-Language Models (LVLMs) are suffering from
Hallucinations on Object Attributes (HoOA), leading to incorrect determination
of fine-grained attributes in the input images. Leveraging significant
advancements in 3D generation from a single image, this paper proposes a novel
method to mitigate HoOA in LVLMs. This method utilizes multiview images sampled
from generated 3D representations as visual prompts for LVLMs, thereby
providing more visual information from other viewpoints. Furthermore, we
observe the input order of multiple multiview images significantly affects the
performance of LVLMs. Consequently, we have devised Multiview Image Augmented
VLM (MIAVLM), incorporating a Multiview Attributes Perceiver (MAP) submodule
capable of simultaneously eliminating the influence of input image order and
aligning visual information from multiview images with Large Language Models
(LLMs). Besides, we designed and employed negative instructions to mitigate
LVLMs' bias towards ``Yes"" responses. Comprehensive experiments demonstrate the
effectiveness of our method.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.10011v1
Instruction-Guided Fusion of Multi-Layer Visual Features in Large Vision-Language Models,"Large Vision-Language Models (LVLMs) have achieved remarkable success in a
wide range of multimodal tasks by integrating pre-trained vision encoders and
large language models. However, current LVLMs primarily rely on visual features
extracted from the final layers of the vision encoder, overlooking the
complementary information available in shallower layers. While recent
approaches have explored the use of multilayer visual features in LVLMs, they
tend to be task-agnostic and fail to examine the dependencies of hierarchical
visual features on specific tasks. To address these gaps, we systematically
investigate the contributions of visual features from different encoder layers
using 18 benchmarks spanning 6 task categories. Our findings reveal that
multilayer features provide complementary strengths with varying task
dependencies, and uniform fusion leads to suboptimal performance. Building on
these insights, we propose the instruction-guided vision aggregator, a module
that dynamically integrates multi-layer visual features based on textual
instructions, without increasing the number of visual tokens. Extensive
evaluations demonstrate the superior performance of our method. Additionally,
an in-depth analysis of the aggregator's behavior highlights the dominance of
mid-to-high-level features in semantic-rich tasks and the critical role of
low-level features in fine-grained perception.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2501.08443v3
Empowering Large Language Model for Continual Video Question Answering with Collaborative Prompting,"In recent years, the rapid increase in online video content has underscored
the limitations of static Video Question Answering (VideoQA) models trained on
fixed datasets, as they struggle to adapt to new questions or tasks posed by
newly available content. In this paper, we explore the novel challenge of
VideoQA within a continual learning framework, and empirically identify a
critical issue: fine-tuning a large language model (LLM) for a sequence of
tasks often results in catastrophic forgetting. To address this, we propose
Collaborative Prompting (ColPro), which integrates specific question constraint
prompting, knowledge acquisition prompting, and visual temporal awareness
prompting. These prompts aim to capture textual question context, visual
content, and video temporal dynamics in VideoQA, a perspective underexplored in
prior research. Experimental results on the NExT-QA and DramaQA datasets show
that ColPro achieves superior performance compared to existing approaches,
achieving 55.14\% accuracy on NExT-QA and 71.24\% accuracy on DramaQA,
highlighting its practical relevance and effectiveness.","['cs.CV', 'cs.CL']",http://arxiv.org/abs/2410.00771v2
Exploring Iterative Enhancement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models,"Large language models exhibit superior capabilities in processing and
understanding language, yet their applications in educational contexts remain
underexplored. Learnersourcing enhances learning by engaging students in
creating their own educational content. When learnersourcing multiple-choice
questions, creating explanations for the solution of a question is a crucial
step; it helps other students understand the solution and promotes a deeper
understanding of related concepts. However, it is often difficult for students
to craft effective solution explanations, due to limited subject understanding.
To help scaffold the task of automated explanation generation, we present and
evaluate a framework called ""ILearner-LLM"", that iteratively enhances the
generated explanations for the given questions with large language models.
Comprising an explanation generation model and an explanation evaluation model,
the framework generates high-quality student-aligned explanations by
iteratively feeding the quality rating score from the evaluation model back
into the instruction prompt of the explanation generation model. Experimental
results demonstrate the effectiveness of our ILearner-LLM on LLaMA2-13B and
GPT-4 to generate higher quality explanations that are closer to those written
by students on five PeerWise datasets. Our findings represent a promising path
to enrich the learnersourcing experience for students and to enhance the
capabilities of large language models for educational applications.","['cs.AI', 'cs.CL']",http://arxiv.org/abs/2309.10444v5
"A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling, Search Algorithms, and Relevant Frameworks","LLM test-time compute (or LLM inference) via search has emerged as a
promising research area with rapid developments. However, current frameworks
often adopt distinct perspectives on three key aspects (task definition, LLM
profiling, and search procedures), making direct comparisons challenging.
Moreover, the search algorithms employed often diverge from standard
implementations, and their specific characteristics are not thoroughly
specified. In this survey, we provide a comprehensive technical review that
unifies task definitions and provides modular definitions of LLM profiling and
search procedures. The definitions enable precise comparisons of various LLM
inference frameworks while highlighting their departures from conventional
search algorithms. We also discuss the applicability, performance, and
efficiency of these methods. For further details and ongoing updates, please
refer to our GitHub repository:
https://github.com/xinzhel/LLM-Agent-Survey/blob/main/search.md",['cs.AI'],http://arxiv.org/abs/2501.10069v1
AIRCHITECT v2: Learning the Hardware Accelerator Design Space through Unified Representations,"Design space exploration (DSE) plays a crucial role in enabling custom
hardware architectures, particularly for emerging applications like AI, where
optimized and specialized designs are essential. With the growing complexity of
deep neural networks (DNNs) and the introduction of advanced foundational
models (FMs), the design space for DNN accelerators is expanding at an
exponential rate. Additionally, this space is highly non-uniform and
non-convex, making it increasingly difficult to navigate and optimize.
Traditional DSE techniques rely on search-based methods, which involve
iterative sampling of the design space to find the optimal solution. However,
this process is both time-consuming and often fails to converge to the global
optima for such design spaces. Recently, AIrchitect v1, the first attempt to
address the limitations of search-based techniques, transformed DSE into a
constant-time classification problem using recommendation networks. In this
work, we propose AIrchitect v2, a more accurate and generalizable
learning-based DSE technique applicable to large-scale design spaces that
overcomes the shortcomings of earlier approaches. Specifically, we devise an
encoder-decoder transformer model that (a) encodes the complex design space
into a uniform intermediate representation using contrastive learning and (b)
leverages a novel unified representation blending the advantages of
classification and regression to effectively explore the large DSE space
without sacrificing accuracy. Experimental results evaluated on 10^5 real DNN
workloads demonstrate that, on average, AIrchitect v2 outperforms existing
techniques by 15% in identifying optimal design points. Furthermore, to
demonstrate the generalizability of our method, we evaluate performance on
unseen model workloads (LLMs) and attain a 1.7x improvement in inference
latency on the identified hardware architecture.","['cs.LG', 'cs.AI', 'cs.AR']",http://arxiv.org/abs/2501.09954v1
GRASP: A Grid-Based Benchmark for Evaluating Commonsense Spatial Reasoning,"Spatial reasoning, an important faculty of human cognition with many
practical applications, is one of the core commonsense skills that is not
purely language-based and, for satisfying (as opposed to optimal) solutions,
requires some minimum degree of planning. Existing benchmarks of Commonsense
Spatial Reasoning (CSR) tend to evaluate how Large Language Models (LLMs)
interpret text-based spatial $\textit{descriptions}$ rather than directly
evaluate a plan produced by the LLM in response to a $\textit{specific}$
spatial reasoning problem. In this paper, we construct a large-scale benchmark
called GRASP, which consists of 16,000 grid-based environments where the agent
is tasked with an energy collection problem. These environments include 100
grid instances instantiated using each of the 160 different grid settings,
involving five different energy distributions, two modes of agent starting
position, and two distinct obstacle configurations, as well as three kinds of
agent constraints. Using GRASP, we compare classic baseline approaches, such as
random walk and greedy search methods, with advanced LLMs like GPT-3.5-Turbo,
GPT-4o, and GPT-o1-mini. The experimental results indicate that even these
advanced LLMs struggle to consistently achieve satisfactory solutions.","['cs.AI', 'cs.CL']",http://arxiv.org/abs/2407.01892v2
A Vision-Language Framework for Multispectral Scene Representation Using Language-Grounded Features,"Scene understanding in remote sensing often faces challenges in generating
accurate representations for complex environments such as various land use
areas or coastal regions, which may also include snow, clouds, or haze. To
address this, we present a vision-language framework named Spectral LLaVA,
which integrates multispectral data with vision-language alignment techniques
to enhance scene representation and description. Using the BigEarthNet v2
dataset from Sentinel-2, we establish a baseline with RGB-based scene
descriptions and further demonstrate substantial improvements through the
incorporation of multispectral information. Our framework optimizes a
lightweight linear projection layer for alignment while keeping the vision
backbone of SpectralGPT frozen. Our experiments encompass scene classification
using linear probing and language modeling for jointly performing scene
classification and description generation. Our results highlight Spectral
LLaVA's ability to produce detailed and accurate descriptions, particularly for
scenarios where RGB data alone proves inadequate, while also enhancing
classification performance by refining SpectralGPT features into semantically
meaningful representations.",['cs.CV'],http://arxiv.org/abs/2501.10144v1
Tarsier2: Advancing Large Vision-Language Models from Detailed Video Description to Comprehensive Video Understanding,"We introduce Tarsier2, a state-of-the-art large vision-language model (LVLM)
designed for generating detailed and accurate video descriptions, while also
exhibiting superior general video understanding capabilities. Tarsier2 achieves
significant advancements through three key upgrades: (1) Scaling pre-training
data from 11M to 40M video-text pairs, enriching both volume and diversity; (2)
Performing fine-grained temporal alignment during supervised fine-tuning; (3)
Using model-based sampling to automatically construct preference data and
applying DPO training for optimization. Extensive experiments show that
Tarsier2-7B consistently outperforms leading proprietary models, including
GPT-4o and Gemini 1.5 Pro, in detailed video description tasks. On the DREAM-1K
benchmark, Tarsier2-7B improves F1 by 2.8\% over GPT-4o and 5.8\% over
Gemini-1.5-Pro. In human side-by-side evaluations, Tarsier2-7B shows a +8.6\%
performance advantage over GPT-4o and +24.9\% over Gemini-1.5-Pro. Tarsier2-7B
also sets new state-of-the-art results across 15 public benchmarks, spanning
tasks such as video question-answering, video grounding, hallucination test,
and embodied question-answering, demonstrating its versatility as a robust
generalist vision-language model.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.07888v2
Classifier Ensemble for Efficient Uncertainty Calibration of Deep Neural Networks for Image Classification,"This paper investigates novel classifier ensemble techniques for uncertainty
calibration applied to various deep neural networks for image classification.
We evaluate both accuracy and calibration metrics, focusing on Expected
Calibration Error (ECE) and Maximum Calibration Error (MCE). Our work compares
different methods for building simple yet efficient classifier ensembles,
including majority voting and several metamodel-based approaches. Our
evaluation reveals that while state-of-the-art deep neural networks for image
classification achieve high accuracy on standard datasets, they frequently
suffer from significant calibration errors. Basic ensemble techniques like
majority voting provide modest improvements, while metamodel-based ensembles
consistently reduce ECE and MCE across all architectures. Notably, the largest
of our compared metamodels demonstrate the most substantial calibration
improvements, with minimal impact on accuracy. Moreover, classifier ensembles
with metamodels outperform traditional model ensembles in calibration
performance, while requiring significantly fewer parameters. In comparison to
traditional post-hoc calibration methods, our approach removes the need for a
separate calibration dataset. These findings underscore the potential of our
proposed metamodel-based classifier ensembles as an efficient and effective
approach to improving model calibration, thereby contributing to more reliable
deep learning systems.",['cs.CV'],http://arxiv.org/abs/2501.10089v1
TraceFL: Interpretability-Driven Debugging in Federated Learning via Neuron Provenance,"In Federated Learning, clients train models on local data and send updates to
a central server, which aggregates them into a global model using a fusion
algorithm. This collaborative yet privacy-preserving training comes at a cost.
FL developers face significant challenges in attributing global model
predictions to specific clients. Localizing responsible clients is a crucial
step towards (a) excluding clients primarily responsible for incorrect
predictions and (b) encouraging clients who contributed high-quality models to
continue participating in the future. Existing ML debugging approaches are
inherently inapplicable as they are designed for single-model, centralized
training.
  We introduce TraceFL, a fine-grained neuron provenance capturing mechanism
that identifies clients responsible for a global model's prediction by tracking
the flow of information from individual clients to the global model. Since
inference on different inputs activates a different set of neurons of the
global model, TraceFL dynamically quantifies the significance of the global
model's neurons in a given prediction, identifying the most crucial neurons in
the global model. It then maps them to the corresponding neurons in every
participating client to determine each client's contribution, ultimately
localizing the responsible client. We evaluate TraceFL on six datasets,
including two real-world medical imaging datasets and four neural networks,
including advanced models such as GPT. TraceFL achieves 99% accuracy in
localizing the responsible client in FL tasks spanning both image and text
classification tasks. At a time when state-of-the-artML debugging approaches
are mostly domain-specific (e.g., image classification only), TraceFL is the
first technique to enable highly accurate automated reasoning across a wide
range of FL applications.","['cs.LG', 'cs.AI', 'cs.CV', 'cs.DC', 'cs.SE']",http://arxiv.org/abs/2312.13632v4
SRE-Conv: Symmetric Rotation Equivariant Convolution for Biomedical Image Classification,"Convolutional neural networks (CNNs) are essential tools for computer vision
tasks, but they lack traditionally desired properties of extracted features
that could further improve model performance, e.g., rotational equivariance.
Such properties are ubiquitous in biomedical images, which often lack explicit
orientation. While current work largely relies on data augmentation or explicit
modules to capture orientation information, this comes at the expense of
increased training costs or ineffective approximations of the desired
equivariance. To overcome these challenges, we propose a novel and efficient
implementation of the Symmetric Rotation-Equivariant (SRE) Convolution
(SRE-Conv) kernel, designed to learn rotation-invariant features while
simultaneously compressing the model size. The SRE-Conv kernel can easily be
incorporated into any CNN backbone. We validate the ability of a deep SRE-CNN
to capture equivariance to rotation using the public MedMNISTv2 dataset (16
total tasks). SRE-Conv-CNN demonstrated improved rotated image classification
performance accuracy on all 16 test datasets in both 2D and 3D images, all
while increasing efficiency with fewer parameters and reduced memory footprint.
The code is available at https://github.com/XYPB/SRE-Conv.","['cs.CV', 'cs.LG', 'eess.IV']",http://arxiv.org/abs/2501.09753v1
Enhancing Few-Shot Image Classification through Learnable Multi-Scale Embedding and Attention Mechanisms,"In the context of few-shot classification, the goal is to train a classifier
using a limited number of samples while maintaining satisfactory performance.
However, traditional metric-based methods exhibit certain limitations in
achieving this objective. These methods typically rely on a single distance
value between the query feature and support feature, thereby overlooking the
contribution of shallow features. To overcome this challenge, we propose a
novel approach in this paper. Our approach involves utilizing a multi-output
embedding network that maps samples into distinct feature spaces. The proposed
method extracts feature vectors at different stages, enabling the model to
capture both global and abstract features. By utilizing these diverse feature
spaces, our model enhances its performance. Moreover, employing a
self-attention mechanism improves the refinement of features at each stage,
leading to even more robust representations and improved overall performance.
Furthermore, assigning learnable weights to each stage significantly improved
performance and results. We conducted comprehensive evaluations on the
MiniImageNet and FC100 datasets, specifically in the 5-way 1-shot and 5-way
5-shot scenarios. Additionally, we performed cross-domain tasks across eight
benchmark datasets, achieving high accuracy in the testing domains. These
evaluations demonstrate the efficacy of our proposed method in comparison to
state-of-the-art approaches. https://github.com/FatemehAskari/MSENet","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2409.07989v2
HydraMix: Multi-Image Feature Mixing for Small Data Image Classification,"Training deep neural networks requires datasets with a large number of
annotated examples. The collection and annotation of these datasets is not only
extremely expensive but also faces legal and privacy problems. These factors
are a significant limitation for many real-world applications. To address this,
we introduce HydraMix, a novel architecture that generates new image
compositions by mixing multiple different images from the same class. HydraMix
learns the fusion of the content of various images guided by a
segmentation-based mixing mask in feature space and is optimized via a
combination of unsupervised and adversarial training. Our data augmentation
scheme allows the creation of models trained from scratch on very small
datasets. We conduct extensive experiments on ciFAIR-10, STL-10, and
ciFAIR-100. Additionally, we introduce a novel text-image metric to assess the
generality of the augmented datasets. Our results show that HydraMix
outperforms existing state-of-the-art methods for image classification on small
datasets.",['cs.CV'],http://arxiv.org/abs/2501.09504v1
reBEN: Refined BigEarthNet Dataset for Remote Sensing Image Analysis,"This paper presents refined BigEarthNet (reBEN) that is a large-scale,
multi-modal remote sensing dataset constructed to support deep learning (DL)
studies for remote sensing image analysis. The reBEN dataset consists of
549,488 pairs of Sentinel-1 and Sentinel-2 image patches. To construct reBEN,
we initially consider the Sentinel-1 and Sentinel-2 tiles used to construct the
BigEarthNet dataset and then divide them into patches of size 1200 m x 1200 m.
We apply atmospheric correction to the Sentinel-2 patches using the latest
version of the sen2cor tool, resulting in higher-quality patches compared to
those present in BigEarthNet. Each patch is then associated with a pixel-level
reference map and scene-level multi-labels. This makes reBEN suitable for
pixel- and scene-based learning tasks. The labels are derived from the most
recent CORINE Land Cover (CLC) map of 2018 by utilizing the 19-class
nomenclature as in BigEarthNet. The use of the most recent CLC map results in
overcoming the label noise present in BigEarthNet. Furthermore, we introduce a
new geographical-based split assignment algorithm that significantly reduces
the spatial correlation among the train, validation, and test sets with respect
to those present in BigEarthNet. This increases the reliability of the
evaluation of DL models. To minimize the DL model training time, we introduce
software tools that convert the reBEN dataset into a DL-optimized data format.
In our experiments, we show the potential of reBEN for multi-modal multi-label
image classification problems by considering several state-of-the-art DL
models. The pre-trained model weights, associated code, and complete dataset
are available at https://bigearth.net.","['cs.CV', 'eess.IV']",http://arxiv.org/abs/2407.03653v3
Efficient Few-Shot Medical Image Analysis via Hierarchical Contrastive Vision-Language Learning,"Few-shot learning in medical image classification presents a significant
challenge due to the limited availability of annotated data and the complex
nature of medical imagery. In this work, we propose Adaptive Vision-Language
Fine-tuning with Hierarchical Contrastive Alignment (HiCA), a novel framework
that leverages the capabilities of Large Vision-Language Models (LVLMs) for
medical image analysis. HiCA introduces a two-stage fine-tuning strategy,
combining domain-specific pretraining and hierarchical contrastive learning to
align visual and textual representations at multiple levels. We evaluate our
approach on two benchmark datasets, Chest X-ray and Breast Ultrasound,
achieving state-of-the-art performance in both few-shot and zero-shot settings.
Further analyses demonstrate the robustness, generalizability, and
interpretability of our method, with substantial improvements in performance
compared to existing baselines. Our work highlights the potential of
hierarchical contrastive strategies in adapting LVLMs to the unique challenges
of medical imaging tasks.","['cs.CV', 'cs.CL']",http://arxiv.org/abs/2501.09294v1
Rethinking Pre-Trained Feature Extractor Selection in Multiple Instance Learning for Whole Slide Image Classification,"Multiple instance learning (MIL) has become a preferred method for gigapixel
whole slide image (WSI) classification without requiring patch-level
annotations. Current MIL research primarily relies on embedding-based
approaches, which extract patch features using a pre-trained feature extractor
and aggregate them for slide-level prediction. Despite the critical role of
feature extraction, there is limited guidance on selecting optimal feature
extractors to maximize WSI performance. This study addresses this gap by
systematically evaluating MIL feature extractors across three dimensions:
pre-training dataset, backbone model, and pre-training method. Extensive
experiments were conducted on two public WSI datasets (TCGA-NSCLC and
Camelyon16) using four state-of-the-art (SOTA) MIL models. Our findings reveal
that: 1) selecting a robust self-supervised learning (SSL) method has a greater
impact on performance than relying solely on an in-domain pre-training dataset;
2) prioritizing Transformer-based backbones with deeper architectures over
CNN-based models; and 3) using larger, more diverse pre-training datasets
significantly enhances classification outcomes. We hope that these insights can
provide practical guidance for optimizing WSI classification and explain the
reasons behind the performance advantages of the current SOTA pathology
foundation models. Furthermore, this work may inform the development of more
effective pathology foundation models. Our code is publicly available at
https://github.com/bryanwong17/MIL-Feature-Extractor-Selection",['cs.CV'],http://arxiv.org/abs/2408.01167v3
IDEA: Image Description Enhanced CLIP-Adapter,"CLIP (Contrastive Language-Image Pre-training) has attained great success in
pattern recognition and computer vision. Transferring CLIP to downstream tasks
(e.g. zero- or few-shot classification) is a hot topic in multimodal learning.
However, current studies primarily focus on either prompt learning for text or
adapter tuning for vision, without fully exploiting the complementary
information and correlations among image-text pairs. In this paper, we propose
an Image Description Enhanced CLIP-Adapter (IDEA) method to adapt CLIP to
few-shot image classification tasks. This method captures fine-grained features
by leveraging both visual features and textual descriptions of images. IDEA is
a training-free method for CLIP, and it can be comparable to or even exceeds
state-of-the-art models on multiple tasks. Furthermore, we introduce
Trainable-IDEA (T-IDEA), which extends IDEA by adding two lightweight learnable
components (i.e., a projector and a learnable latent space), further enhancing
the model's performance and achieving SOTA results on 11 datasets. As one
important contribution, we employ the Llama model and design a comprehensive
pipeline to generate textual descriptions for images of 11 datasets, resulting
in a total of 1,637,795 image-text pairs, named ""IMD-11"". Our code and data are
released at https://github.com/FourierAI/IDEA.","['cs.CV', 'cs.AI', 'cs.LG']",http://arxiv.org/abs/2501.08816v1
MIAFEx: An Attention-based Feature Extraction Method for Medical Image Classification,"Feature extraction techniques are crucial in medical image classification;
however, classical feature extractors in addition to traditional machine
learning classifiers often exhibit significant limitations in providing
sufficient discriminative information for complex image sets. While
Convolutional Neural Networks (CNNs) and Vision Transformer (ViT) have shown
promise in feature extraction, they are prone to overfitting due to the
inherent characteristics of medical imaging data, including small sample sizes
or high intra-class variance. In this work, the Medical Image Attention-based
Feature Extractor (MIAFEx) is proposed, a novel method that employs a learnable
refinement mechanism to enhance the classification token within the Transformer
encoder architecture. This mechanism adjusts the token based on learned
weights, improving the extraction of salient features and enhancing the model's
adaptability to the challenges presented by medical imaging data. The MIAFEx
output features quality is compared against classical feature extractors using
traditional and hybrid classifiers. Also, the performance of these features is
compared against modern CNN and ViT models in classification tasks,
demonstrating its superiority in accuracy and robustness across multiple
complex classification medical imaging datasets. This advantage is particularly
pronounced in scenarios with limited training data, where traditional and
modern models often struggle to generalize effectively. The source code of this
proposal can be found at
https://github.com/Oscar-RamosS/Medical-Image-Attention-based-Feature-Extractor-MIAFEx","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2501.08562v1
Spurious Feature Eraser: Stabilizing Test-Time Adaptation for Vision-Language Foundation Model,"Vision-language foundation models have exhibited remarkable success across a
multitude of downstream tasks due to their scalability on extensive image-text
paired data. However, these models also display significant limitations when
applied to downstream tasks, such as fine-grained image classification, as a
result of ``decision shortcuts'' that hinder their generalization capabilities.
In this work, we find that the CLIP model possesses a rich set of features,
encompassing both \textit{desired invariant causal features} and
\textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP
on downstream tasks originates from its inability to effectively utilize
pre-trained features in accordance with specific task requirements. To address
this challenge, we propose a simple yet effective method, Spurious Feature
Eraser (SEraser), to alleviate the decision shortcuts by erasing the spurious
features. Specifically, we introduce a test-time prompt tuning paradigm that
optimizes a learnable prompt, thereby compelling the model to exploit invariant
features while disregarding decision shortcuts during the inference phase. The
proposed method effectively alleviates excessive dependence on potentially
misleading spurious information. We conduct comparative analysis of the
proposed method against various approaches which validates the significant
superiority.","['cs.CV', 'cs.AI', 'cs.LG']",http://arxiv.org/abs/2403.00376v3
deepTerra -- AI Land Classification Made Easy,"deepTerra is a comprehensive platform designed to facilitate the
classification of land surface features using machine learning and satellite
imagery. The platform includes modules for data collection, image augmentation,
training, testing, and prediction, streamlining the entire workflow for image
classification tasks. This paper presents a detailed overview of the
capabilities of deepTerra, shows how it has been applied to various research
areas, and discusses the future directions it might take.","['cs.CV', 'cs.AI', 'cs.LG']",http://arxiv.org/abs/2501.07859v1
A Low-cost and Ultra-lightweight Binary Neural Network for Traffic Signal Recognition,"The deployment of neural networks in vehicle platforms and wearable
Artificial Intelligence-of-Things (AIOT) scenarios has become a research area
that has attracted much attention. With the continuous evolution of deep
learning technology, many image classification models are committed to
improving recognition accuracy, but this is often accompanied by problems such
as large model resource usage, complex structure, and high power consumption,
which makes it challenging to deploy on resource-constrained platforms. Herein,
we propose an ultra-lightweight binary neural network (BNN) model designed for
hardware deployment, and conduct image classification research based on the
German Traffic Sign Recognition Benchmark (GTSRB) dataset. In addition, we also
verify it on the Chinese Traffic Sign (CTS) and Belgian Traffic Sign (BTS)
datasets. The proposed model shows excellent recognition performance with an
accuracy of up to 97.64%, making it one of the best performing BNN models in
the GTSRB dataset. Compared with the full-precision model, the accuracy loss is
controlled within 1%, and the parameter storage overhead of the model is only
10% of that of the full-precision model. More importantly, our network model
only relies on logical operations and low-bit width fixed-point addition and
subtraction operations during the inference phase, which greatly simplifies the
design complexity of the processing element (PE). Our research shows the great
potential of BNN in the hardware deployment of computer vision models,
especially in the field of computer vision tasks related to autonomous driving.","['cs.AI', 'cs.CV', 'eess.IV']",http://arxiv.org/abs/2501.07808v1
Balance Divergence for Knowledge Distillation,"Knowledge distillation has been widely adopted in computer vision task
processing, since it can effectively enhance the performance of lightweight
student networks by leveraging the knowledge transferred from cumbersome
teacher networks. Most existing knowledge distillation methods utilize
Kullback-Leibler divergence to mimic the logit output probabilities between the
teacher network and the student network. Nonetheless, these methods may neglect
the negative parts of the teacher's ''dark knowledge'' because the divergence
calculations may ignore the effect of the minute probabilities from the
teacher's logit output. This deficiency may lead to suboptimal performance in
logit mimicry during the distillation process and result in an imbalance of
information acquired by the student network. In this paper, we investigate the
impact of this imbalance and propose a novel method, named Balance Divergence
Distillation. By introducing a compensatory operation using reverse
Kullback-Leibler divergence, our method can improve the modeling of the
extremely small values in the negative from the teacher and preserve the
learning capacity for the positive. Furthermore, we test the impact of
different temperature coefficients adjustments, which may conducted to further
balance for knowledge transferring. We evaluate the proposed method on several
computer vision tasks, including image classification and semantic
segmentation. The evaluation results show that our method achieves an accuracy
improvement of 1%~3% for lightweight students on both CIFAR-100 and ImageNet
dataset, and a 4.55% improvement in mIoU for PSP-ResNet18 on the Cityscapes
dataset. The experiments show that our method is a simple yet highly effective
solution that can be smoothly applied to different knowledge distillation
methods.",['cs.CV'],http://arxiv.org/abs/2501.07804v1
Sparse Attention Vectors: Generative Multimodal Model Features Are Discriminative Vision-Language Classifiers,"Generative Large Multimodal Models (LMMs) like LLaVA and Qwen-VL excel at a
wide variety of vision-language (VL) tasks such as image captioning or visual
question answering. Despite strong performance, LMMs are not directly suited
for foundational discriminative vision-language tasks (i.e., tasks requiring
discrete label predictions) such as image classification and multiple-choice
VQA. One key challenge in utilizing LMMs for discriminative tasks is the
extraction of useful features from generative models. To overcome this issue,
we propose an approach for finding features in the model's latent space to more
effectively leverage LMMs for discriminative tasks. Toward this end, we present
Sparse Attention Vectors (SAVs) -- a finetuning-free method that leverages
sparse attention head activations (fewer than 1\% of the heads) in LMMs as
strong features for VL tasks. With only few-shot examples, SAVs demonstrate
state-of-the-art performance compared to a variety of few-shot and finetuned
baselines on a collection of discriminative tasks. Our experiments also imply
that SAVs can scale in performance with additional examples and generalize to
similar tasks, establishing SAVs as both effective and robust multimodal
feature representations.","['cs.CV', 'cs.AI', 'cs.CL']",http://arxiv.org/abs/2412.00142v2
Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad,"Adaptive methods are extremely popular in machine learning as they make
learning rate tuning less expensive. This paper introduces a novel optimization
algorithm named KATE, which presents a scale-invariant adaptation of the
well-known AdaGrad algorithm. We prove the scale-invariance of KATE for the
case of Generalized Linear Models. Moreover, for general smooth non-convex
problems, we establish a convergence rate of $O \left(\frac{\log T}{\sqrt{T}}
\right)$ for KATE, matching the best-known ones for AdaGrad and Adam. We also
compare KATE to other state-of-the-art adaptive algorithms Adam and AdaGrad in
numerical experiments with different problems, including complex machine
learning tasks like image classification and text classification on real data.
The results indicate that KATE consistently outperforms AdaGrad and
matches/surpasses the performance of Adam in all considered scenarios.","['cs.LG', 'cs.AI', 'math.OC']",http://arxiv.org/abs/2403.02648v4
Class Distance Weighted Cross Entropy Loss for Classification of Disease Severity,"Assessing disease severity with ordinal classes, where each class reflects
increasing severity levels, benefits from loss functions designed for this
ordinal structure. Traditional categorical loss functions, like Cross-Entropy
(CE), often perform suboptimally in these scenarios. To address this, we
propose a novel loss function, Class Distance Weighted Cross-Entropy (CDW-CE),
which penalizes misclassifications more severely when the predicted and actual
classes are farther apart. We evaluated CDW-CE using various deep
architectures, comparing its performance against several categorical and
ordinal loss functions. To assess the quality of latent representations, we
used t-distributed stochastic neighbor embedding (t-SNE) and uniform manifold
approximation and projection (UMAP) visualizations, quantified the clustering
quality using the Silhouette Score, and compared Class Activation Maps (CAM)
generated by models trained with CDW-CE and CE loss. Feedback from domain
experts was incorporated to evaluate how well model attention aligns with
expert opinion. Our results show that CDW-CE consistently improves performance
in ordinal image classification tasks. It achieves higher Silhouette Scores,
indicating better class discrimination capability, and its CAM visualizations
show a stronger focus on clinically significant regions, as validated by domain
experts. Receiver operator characteristics (ROC) curves and the area under the
curve (AUC) scores highlight that CDW-CE outperforms other loss functions,
including prominent ordinal loss functions from the literature.",['cs.CV'],http://arxiv.org/abs/2412.01246v3
Uncertainty Guarantees on Automated Precision Weeding using Conformal Prediction,"Precision agriculture in general, and precision weeding in particular, have
greatly benefited from the major advancements in deep learning and computer
vision. A large variety of commercial robotic solutions are already available
and deployed. However, the adoption by farmers of such solutions is still low
for many reasons, an important one being the lack of trust in these systems.
This is in great part due to the opaqueness and complexity of deep neural
networks and the manufacturers' inability to provide valid guarantees on their
performance. Conformal prediction, a well-established methodology in the
machine learning community, is an efficient and reliable strategy for providing
trustworthy guarantees on the predictions of any black-box model under very
minimal constraints. Bridging the gap between the safe machine learning and
precision agriculture communities, this article showcases conformal prediction
in action on the task of precision weeding through deep learning-based image
classification. After a detailed presentation of the conformal prediction
methodology and the development of a precision spraying pipeline based on a
''conformalized'' neural network and well-defined spraying decision rules, the
article evaluates this pipeline on two real-world scenarios: one under
in-distribution conditions, the other reflecting a near out-of-distribution
setting. The results show that we are able to provide formal, i.e. certifiable,
guarantees on spraying at least 90% of the weeds.","['cs.CV', 'cs.LG', 'stat.AP', 'stat.ML']",http://arxiv.org/abs/2501.07185v1
Fast and reliable uncertainty quantification with neural network ensembles for industrial image classification,"Image classification with neural networks (NNs) is widely used in industrial
processes, situations where the model likely encounters unknown objects during
deployment, i.e., out-of-distribution (OOD) data. Worryingly, NNs tend to make
confident yet incorrect predictions when confronted with OOD data. To increase
the models' reliability, they should quantify the uncertainty in their own
predictions, communicating when the output should (not) be trusted. Deep
ensembles, composed of multiple independent NNs, have been shown to perform
strongly but are computationally expensive. Recent research has proposed more
efficient NN ensembles, namely the snapshot, batch, and multi-input
multi-output ensemble. This study investigates the predictive and uncertainty
performance of efficient NN ensembles in the context of image classification
for industrial processes. It is the first to provide a comprehensive comparison
and it proposes a novel Diversity Quality metric to quantify the ensembles'
performance on the in-distribution and OOD sets in one single metric. The
results highlight the batch ensemble as a cost-effective and competitive
alternative to the deep ensemble. It matches the deep ensemble in both
uncertainty and accuracy while exhibiting considerable savings in training
time, test time, and memory storage.","['cs.LG', 'stat.ML']",http://arxiv.org/abs/2403.10182v5
LarvSeg: Exploring Image Classification Data For Large Vocabulary Semantic Segmentation via Category-wise Attentive Classifier,"Scaling up the vocabulary of semantic segmentation models is extremely
challenging because annotating large-scale mask labels is labour-intensive and
time-consuming. Recently, language-guided segmentation models have been
proposed to address this challenge. However, their performance drops
significantly when applied to out-of-distribution categories. In this paper, we
propose a new large vocabulary semantic segmentation framework, called LarvSeg.
Different from previous works, LarvSeg leverages image classification data to
scale the vocabulary of semantic segmentation models as large-vocabulary
classification datasets usually contain balanced categories and are much easier
to obtain. However, for classification tasks, the category is image-level,
while for segmentation we need to predict the label at pixel level. To address
this issue, we first propose a general baseline framework to incorporate
image-level supervision into the training process of a pixel-level segmentation
model, making the trained network perform semantic segmentation on newly
introduced categories in the classification data. We then observe that a model
trained on segmentation data can group pixel features of categories beyond the
training vocabulary. Inspired by this finding, we design a category-wise
attentive classifier to apply supervision to the precise regions of
corresponding categories to improve the model performance. Extensive
experiments demonstrate that LarvSeg significantly improves the large
vocabulary semantic segmentation performance, especially in the categories
without mask labels. For the first time, we provide a 21K-category semantic
segmentation model with the help of ImageNet21K. The code is available at
https://github.com/HaojunYu1998/large_voc_seg.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.06862v1
Multimodal Structure-Aware Quantum Data Processing,"While large language models (LLMs) have advanced the field of natural
language processing (NLP), their ""black box"" nature obscures their
decision-making processes. To address this, researchers developed structured
approaches using higher order tensors. These are able to model linguistic
relations, but stall when training on classical computers due to their
excessive size. Tensors are natural inhabitants of quantum systems and training
on quantum computers provides a solution by translating text to variational
quantum circuits. In this paper, we develop MultiQ-NLP: a framework for
structure-aware data processing with multimodal text+image data. Here,
""structure"" refers to syntactic and grammatical relationships in language, as
well as the hierarchical organization of visual elements in images. We enrich
the translation with new types and type homomorphisms and develop novel
architectures to represent structure. When tested on a main stream image
classification task (SVO Probes), our best model showed a par performance with
the state of the art classical models; moreover the best model was fully
structured.","['cs.LG', '68T45, 68T50, 68Q12, 68U15, 68U10, 81P45, 81P68', 'I.2.7; I.2.10; H.5.1']",http://arxiv.org/abs/2411.04242v4
Categorical Knowledge Fused Recognition: Fusing Hierarchical Knowledge with Image Classification through Aligning and Deep Metric Learning,"Image classification is a fundamental computer vision task and an important
baseline for deep metric learning. In decades efforts have been made on
enhancing image classification accuracy by using deep learning models while
less attention has been paid on the reasoning aspect of the recognition, i.e.,
predictions could be made because of background or other surrounding objects
rather than the target object. Hierarchical knowledge about image categories
depicts inter-class similarities or dissimilarities. Effective fusion of such
knowledge with deep learning image classification models is promising in
improving target object identification and enhancing the reasoning aspect of
the recognition. In this paper, we propose a novel deep metric learning based
method to effectively fuse prior knowledge about image categories with
mainstream backbone image classification models and enhance the reasoning
aspect of the recognition in an end-to-end manner. Existing deep metric
learning incorporated image classification methods mainly focus on whether
sampled images are from the same class. A new triplet loss function term that
aligns distances in the model latent space with those in knowledge space is
presented and incorporated in the proposed method to facilitate the
dual-modality fusion. Extensive experiments on the CIFAR-10, CIFAR-100,
Mini-ImageNet, and ImageNet-1K datasets evaluated the proposed method, and
results indicate that the proposed method is effective in enhancing the
reasoning aspect of image recognition in terms of weakly-supervised object
localization performance.",['cs.CV'],http://arxiv.org/abs/2407.20600v2
Kolmogorov-Arnold networks for metal surface defect classification,"This paper presents the application of Kolmogorov-Arnold Networks (KAN) in
classifying metal surface defects. Specifically, steel surfaces are analyzed to
detect defects such as cracks, inclusions, patches, pitted surfaces, and
scratches. Drawing on the Kolmogorov-Arnold theorem, KAN provides a novel
approach compared to conventional multilayer perceptrons (MLPs), facilitating
more efficient function approximation by utilizing spline functions. The
results show that KAN networks can achieve better accuracy than convolutional
neural networks (CNNs) with fewer parameters, resulting in faster convergence
and improved performance in image classification.","['cs.LG', 'cs.AI', 'cs.NE']",http://arxiv.org/abs/2501.06389v1
"Dolphin: Closed-loop Open-ended Auto-research through Thinking, Practice, and Feedback","The scientific research paradigm is undergoing a profound transformation
owing to the development of Artificial Intelligence (AI). Recent works
demonstrate that various AI-assisted research methods can largely improve
research efficiency by improving data analysis, accelerating computation, and
fostering novel idea generation. To further move towards the ultimate goal
(i.e., automatic scientific research), in this paper, we propose Dolphin, the
first closed-loop open-ended auto-research framework to further build the
entire process of human scientific research. Dolphin can generate research
ideas, perform experiments, and get feedback from experimental results to
generate higher-quality ideas. More specifically, Dolphin first generates novel
ideas based on relevant papers which are ranked by the topic and task
attributes. Then, the codes are automatically generated and debugged with the
exception-traceback-guided local code structure. Finally, Dolphin automatically
analyzes the results of each idea and feeds the results back to the next round
of idea generation. Experiments are conducted on the benchmark datasets of
different topics and results show that Dolphin can generate novel ideas
continuously and complete the experiment in a loop. We highlight that Dolphin
can automatically propose methods that are comparable to the state-of-the-art
in some tasks such as 2D image classification and 3D point classification.","['cs.AI', 'cs.CL', 'cs.CV']",http://arxiv.org/abs/2501.03916v2
4-bit Shampoo for Memory-Efficient Network Training,"Second-order optimizers, maintaining a matrix termed a preconditioner, are
superior to first-order optimizers in both theory and practice. The states
forming the preconditioner and its inverse root restrict the maximum size of
models trained by second-order optimizers. To address this, compressing 32-bit
optimizer states to lower bitwidths has shown promise in reducing memory usage.
However, current approaches only pertain to first-order optimizers. In this
paper, we propose the first 4-bit second-order optimizers, exemplified by 4-bit
Shampoo, maintaining performance similar to that of 32-bit ones. We show that
quantizing the eigenvector matrix of the preconditioner in 4-bit Shampoo is
remarkably better than quantizing the preconditioner itself both theoretically
and experimentally. By rectifying the orthogonality of the quantized
eigenvector matrix, we enhance the approximation of the preconditioner's
eigenvector matrix, which also benefits the computation of its inverse 4-th
root. Besides, we find that linear square quantization slightly outperforms
dynamic tree quantization when quantizing second-order optimizer states.
Evaluation on various networks for image classification and natural language
modeling demonstrates that our 4-bit Shampoo achieves comparable performance to
its 32-bit counterpart while being more memory-efficient.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2405.18144v3
CMTNet: Convolutional Meets Transformer Network for Hyperspectral Images Classification,"Hyperspectral remote sensing (HIS) enables the detailed capture of spectral
information from the Earth's surface, facilitating precise classification and
identification of surface crops due to its superior spectral diagnostic
capabilities. However, current convolutional neural networks (CNNs) focus on
local features in hyperspectral data, leading to suboptimal performance when
classifying intricate crop types and addressing imbalanced sample
distributions. In contrast, the Transformer framework excels at extracting
global features from hyperspectral imagery. To leverage the strengths of both
approaches, this research introduces the Convolutional Meet Transformer Network
(CMTNet). This innovative model includes a spectral-spatial feature extraction
module for shallow feature capture, a dual-branch structure combining CNN and
Transformer branches for local and global feature extraction, and a
multi-output constraint module that enhances classification accuracy through
multi-output loss calculations and cross constraints across local,
international, and joint features. Extensive experiments conducted on three
datasets (WHU-Hi-LongKou, WHU-Hi-HanChuan, and WHU-Hi-HongHu) demonstrate that
CTDBNet significantly outperforms other state-of-the-art networks in
classification performance, validating its effectiveness in hyperspectral crop
classification.","['cs.CV', 'cs.GR', 'I.4.6']",http://arxiv.org/abs/2406.14080v4
Cross-Modal Mapping: Eliminating the Modality Gap for Few-Shot Image Classification,"In few-shot image classification tasks, methods based on pretrained
vision-language models (such as CLIP) have achieved significant progress. Many
existing approaches directly utilize visual or textual features as class
prototypes, however, these features fail to adequately represent their
respective classes. We identify that this limitation arises from the modality
gap inherent in pretrained vision-language models, which weakens the connection
between the visual and textual modalities. To eliminate this modality gap and
enable textual features to fully represent class prototypes, we propose a
simple and efficient Cross-Modal Mapping (CMM) method. This method employs a
linear transformation to map image features into the textual feature space,
ensuring that both modalities are comparable within the same feature space.
Nevertheless, the modality gap diminishes the effectiveness of this mapping. To
address this, we further introduce a triplet loss to optimize the spatial
relationships between image features and class textual features, allowing class
textual features to naturally serve as class prototypes for image features.
Experimental results on 11 benchmark demonstrate an average improvement of
approximately 3.5% compared to conventional methods and exhibit competitive
performance on 4 distribution shift benchmarks.",['cs.CV'],http://arxiv.org/abs/2412.20110v2
A Contrastive Symmetric Forward-Forward Algorithm (SFFA) for Continual Learning Tasks,"The so-called Forward-Forward Algorithm (FFA) has recently gained momentum as
an alternative to the conventional back-propagation algorithm for neural
network learning, yielding competitive performance across various modeling
tasks. By replacing the backward pass of gradient back-propagation with two
contrastive forward passes, the FFA avoids several shortcomings undergone by
its predecessor (e.g., vanishing/exploding gradient) by enabling layer-wise
training heuristics. In classification tasks, this contrastive method has been
proven to effectively create a latent sparse representation of the input data,
ultimately favoring discriminability. However, FFA exhibits an inherent
asymmetric gradient behavior due to an imbalanced loss function between
positive and negative data, adversely impacting on the model's generalization
capabilities and leading to an accuracy degradation. To address this issue,
this work proposes the Symmetric Forward-Forward Algorithm (SFFA), a novel
modification of the original FFA which partitions each layer into positive and
negative neurons. This allows the local fitness function to be defined as the
ratio between the activation of positive neurons and the overall layer
activity, resulting in a symmetric loss landscape during the training phase. To
evaluate the enhanced convergence of our method, we conduct several experiments
using multiple image classification benchmarks, comparing the accuracy of
models trained with SFFA to those trained with its FFA counterpart. As a
byproduct of this reformulation, we explore the advantages of using a
layer-wise training algorithm for Continual Learning (CL) tasks. The
specialization of neurons and the sparsity of their activations induced by
layer-wise training algorithms enable efficient CL strategies that incorporate
new knowledge (classes) into the neural network, while preventing catastrophic
forgetting of previously...",['cs.LG'],http://arxiv.org/abs/2409.07387v2
A 1Mb mixed-precision quantized encoder for image classification and patch-based compression,"Even if Application-Specific Integrated Circuits (ASIC) have proven to be a
relevant choice for integrating inference at the edge, they are often limited
in terms of applicability. In this paper, we demonstrate that an ASIC neural
network accelerator dedicated to image processing can be applied to multiple
tasks of different levels: image classification and compression, while
requiring a very limited hardware. The key component is a reconfigurable,
mixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and
activation quantizations combined with convolutional layer structural pruning
to lower hardware-related constraints (memory and computing). We introduce an
automatic adaptation of linear symmetric quantizer scaling factors to perform
quantized levels equalization, aiming at stabilizing quinary and ternary
weights training. In addition, a proposed layer-shared Bit-Shift Normalization
significantly simplifies the implementation of the hardware-expensive Batch
Normalization. For a specific configuration in which the encoder design only
requires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides,
we also show that this quantized encoder can be used to compress image
patch-by-patch while the reconstruction can performed remotely, by a dedicated
full-frame decoder. This solution typically enables an end-to-end compression
almost without any block artifacts, outperforming patch-based state-of-the-art
techniques employing a patch-constant bitrate.","['cs.CV', 'cs.LG', 'eess.IV']",http://arxiv.org/abs/2501.05097v1
MambaHSI: Spatial-Spectral Mamba for Hyperspectral Image Classification,"Transformer has been extensively explored for hyperspectral image (HSI)
classification. However, transformer poses challenges in terms of speed and
memory usage because of its quadratic computational complexity. Recently, the
Mamba model has emerged as a promising approach, which has strong long-distance
modeling capabilities while maintaining a linear computational complexity.
However, representing the HSI is challenging for the Mamba due to the
requirement for an integrated spatial and spectral understanding. To remedy
these drawbacks, we propose a novel HSI classification model based on a Mamba
model, named MambaHSI, which can simultaneously model long-range interaction of
the whole image and integrate spatial and spectral information in an adaptive
manner. Specifically, we design a spatial Mamba block (SpaMB) to model the
long-range interaction of the whole image at the pixel-level. Then, we propose
a spectral Mamba block (SpeMB) to split the spectral vector into multiple
groups, mine the relations across different spectral groups, and extract
spectral features. Finally, we propose a spatial-spectral fusion module (SSFM)
to adaptively integrate spatial and spectral features of a HSI. To our best
knowledge, this is the first image-level HSI classification model based on the
Mamba. We conduct extensive experiments on four diverse HSI datasets. The
results demonstrate the effectiveness and superiority of the proposed model for
HSI classification. This reveals the great potential of Mamba to be the
next-generation backbone for HSI models. Codes are available at
https://github.com/li-yapeng/MambaHSI .",['cs.CV'],http://arxiv.org/abs/2501.04944v1
A New Perspective on Privacy Protection in Federated Learning with Granular-Ball Computing,"Federated Learning (FL) facilitates collaborative model training while
prioritizing privacy by avoiding direct data sharing. However, most existing
articles attempt to address challenges within the model's internal parameters
and corresponding outputs, while neglecting to solve them at the input level.
To address this gap, we propose a novel framework called Granular-Ball
Federated Learning (GrBFL) for image classification. GrBFL diverges from
traditional methods that rely on the finest-grained input data. Instead, it
segments images into multiple regions with optimal coarse granularity, which
are then reconstructed into a graph structure. We designed a two-dimensional
binary search segmentation algorithm based on variance constraints for GrBFL,
which effectively removes redundant information while preserving key
representative features. Extensive theoretical analysis and experiments
demonstrate that GrBFL not only safeguards privacy and enhances efficiency but
also maintains robust utility, consistently outperforming other
state-of-the-art FL methods. The code is available at
https://github.com/AIGNLAI/GrBFL.","['cs.LG', 'cs.CV']",http://arxiv.org/abs/2501.04940v1
"Online Continual Learning: A Systematic Literature Review of Approaches, Challenges, and Benchmarks","Online Continual Learning (OCL) is a critical area in machine learning,
focusing on enabling models to adapt to evolving data streams in real-time
while addressing challenges such as catastrophic forgetting and the
stability-plasticity trade-off. This study conducts the first comprehensive
Systematic Literature Review (SLR) on OCL, analyzing 81 approaches, extracting
over 1,000 features (specific tasks addressed by these approaches), and
identifying more than 500 components (sub-models within approaches, including
algorithms and tools). We also review 83 datasets spanning applications like
image classification, object detection, and multimodal vision-language tasks.
Our findings highlight key challenges, including reducing computational
overhead, developing domain-agnostic solutions, and improving scalability in
resource-constrained environments. Furthermore, we identify promising
directions for future research, such as leveraging self-supervised learning for
multimodal and sequential data, designing adaptive memory mechanisms that
integrate sparse retrieval and generative replay, and creating efficient
frameworks for real-world applications with noisy or evolving task boundaries.
By providing a rigorous and structured synthesis of the current state of OCL,
this review offers a valuable resource for advancing this field and addressing
its critical challenges and opportunities. The complete SLR methodology steps
and extracted data are publicly available through the provided link:
https://github.com/kiyan-rezaee/
Systematic-Literature-Review-on-Online-Continual-Learning",['cs.LG'],http://arxiv.org/abs/2501.04897v1
Discrete Wavelet Transform-Based Capsule Network for Hyperspectral Image Classification,"Hyperspectral image (HSI) classification is a crucial technique for remote
sensing to build large-scale earth monitoring systems. HSI contains much more
information than traditional visual images for identifying the categories of
land covers. One recent feasible solution for HSI is to leverage CapsNets for
capturing spectral-spatial information. However, these methods require high
computational requirements due to the full connection architecture between
stacked capsule layers. To solve this problem, a DWT-CapsNet is proposed to
identify partial but important connections in CapsNet for a effective and
efficient HSI classification. Specifically, we integrate a tailored attention
mechanism into a Discrete Wavelet Transform (DWT)-based downsampling layer,
alleviating the information loss problem of conventional downsampling operation
in feature extractors. Moreover, we propose a novel multi-scale routing
algorithm that prunes a large proportion of connections in CapsNet. A capsule
pyramid fusion mechanism is designed to aggregate the spectral-spatial
relationships in multiple levels of granularity, and then a self-attention
mechanism is further conducted in a partially and locally connected
architecture to emphasize the meaningful relationships. As shown in the
experimental results, our method achieves state-of-the-art accuracy while
keeping lower computational demand regarding running time, flops, and the
number of parameters, rendering it an appealing choice for practical
implementation in HSI classification.",['cs.CV'],http://arxiv.org/abs/2501.04643v1
Forget Vectors at Play: Universal Input Perturbations Driving Machine Unlearning in Image Classification,"Machine unlearning (MU), which seeks to erase the influence of specific
unwanted data from already-trained models, is becoming increasingly vital in
model editing, particularly to comply with evolving data regulations like the
``right to be forgotten''. Conventional approaches are predominantly
model-based, typically requiring retraining or fine-tuning the model's weights
to meet unlearning requirements. In this work, we approach the MU problem from
a novel input perturbation-based perspective, where the model weights remain
intact throughout the unlearning process. We demonstrate the existence of a
proactive input-based unlearning strategy, referred to forget vector, which can
be generated as an input-agnostic data perturbation and remains as effective as
model-based approximate unlearning approaches. We also explore forget vector
arithmetic, whereby multiple class-specific forget vectors are combined through
simple operations (e.g., linear combinations) to generate new forget vectors
for unseen unlearning tasks, such as forgetting arbitrary subsets across
classes. Extensive experiments validate the effectiveness and adaptability of
the forget vector, showcasing its competitive performance relative to
state-of-the-art model-based methods. Codes are available at
https://github.com/Changchangsun/Forget-Vector.","['cs.LG', 'cs.CV']",http://arxiv.org/abs/2412.16780v2
One missing piece in Vision and Language: A Survey on Comics Understanding,"Vision-language models have recently evolved into versatile systems capable
of high performance across a range of tasks, such as document understanding,
visual question answering, and grounding, often in zero-shot settings. Comics
Understanding, a complex and multifaceted field, stands to greatly benefit from
these advances. Comics, as a medium, combine rich visual and textual
narratives, challenging AI models with tasks that span image classification,
object detection, instance segmentation, and deeper narrative comprehension
through sequential panels. However, the unique structure of comics --
characterized by creative variations in style, reading order, and non-linear
storytelling -- presents a set of challenges distinct from those in other
visual-language domains. In this survey, we present a comprehensive review of
Comics Understanding from both dataset and task perspectives. Our contributions
are fivefold: (1) We analyze the structure of the comics medium, detailing its
distinctive compositional elements; (2) We survey the widely used datasets and
tasks in comics research, emphasizing their role in advancing the field; (3) We
introduce the Layer of Comics Understanding (LoCU) framework, a novel taxonomy
that redefines vision-language tasks within comics and lays the foundation for
future work; (4) We provide a detailed review and categorization of existing
methods following the LoCU framework; (5) Finally, we highlight current
research challenges and propose directions for future exploration, particularly
in the context of vision-language models applied to comics. This survey is the
first to propose a task-oriented framework for comics intelligence and aims to
guide future research by addressing critical gaps in data availability and task
definition. A project associated with this survey is available at
https://github.com/emanuelevivoli/awesome-comics-understanding.",['cs.CV'],http://arxiv.org/abs/2409.09502v2
Zero-Shot Prompting and Few-Shot Fine-Tuning: Revisiting Document Image Classification Using Large Language Models,"Classifying scanned documents is a challenging problem that involves image,
layout, and text analysis for document understanding. Nevertheless, for certain
benchmark datasets, notably RVL-CDIP, the state of the art is closing in to
near-perfect performance when considering hundreds of thousands of training
samples. With the advent of large language models (LLMs), which are excellent
few-shot learners, the question arises to what extent the document
classification problem can be addressed with only a few training samples, or
even none at all. In this paper, we investigate this question in the context of
zero-shot prompting and few-shot model fine-tuning, with the aim of reducing
the need for human-annotated training samples as much as possible.",['cs.CV'],http://arxiv.org/abs/2412.13859v1
GlobalDoc: A Cross-Modal Vision-Language Framework for Real-World Document Image Retrieval and Classification,"Visual document understanding (VDU) has rapidly advanced with the development
of powerful multi-modal language models. However, these models typically
require extensive document pre-training data to learn intermediate
representations and often suffer a significant performance drop in real-world
online industrial settings. A primary issue is their heavy reliance on OCR
engines to extract local positional information within document pages, which
limits the models' ability to capture global information and hinders their
generalizability, flexibility, and robustness. In this paper, we introduce
GlobalDoc, a cross-modal transformer-based architecture pre-trained in a
self-supervised manner using three novel pretext objective tasks. GlobalDoc
improves the learning of richer semantic concepts by unifying language and
visual representations, resulting in more transferable models. For proper
evaluation, we also propose two novel document-level downstream VDU tasks,
Few-Shot Document Image Classification (DIC) and Content-based Document Image
Retrieval (DIR), designed to simulate industrial scenarios more closely.
Extensive experimentation has been conducted to demonstrate GlobalDoc's
effectiveness in practical settings.",['cs.CV'],http://arxiv.org/abs/2309.05756v3
DocXplain: A Novel Model-Agnostic Explainability Method for Document Image Classification,"Deep learning (DL) has revolutionized the field of document image analysis,
showcasing superhuman performance across a diverse set of tasks. However, the
inherent black-box nature of deep learning models still presents a significant
challenge to their safe and robust deployment in industry. Regrettably, while a
plethora of research has been dedicated in recent years to the development of
DL-powered document analysis systems, research addressing their transparency
aspects has been relatively scarce. In this paper, we aim to bridge this
research gap by introducing DocXplain, a novel model-agnostic explainability
method specifically designed for generating high interpretability feature
attribution maps for the task of document image classification. In particular,
our approach involves independently segmenting the foreground and background
features of the documents into different document elements and then ablating
these elements to assign feature importance. We extensively evaluate our
proposed approach in the context of document image classification, utilizing 4
different evaluation metrics, 2 widely recognized document benchmark datasets,
and 10 state-of-the-art document image classification models. By conducting a
thorough quantitative and qualitative analysis against 9 existing
state-of-the-art attribution methods, we demonstrate the superiority of our
approach in terms of both faithfulness and interpretability. To the best of the
authors' knowledge, this work presents the first model-agnostic
attribution-based explainability method specifically tailored for document
images. We anticipate that our work will significantly contribute to advancing
research on transparency, fairness, and robustness of document image
classification models.",['cs.CV'],http://arxiv.org/abs/2407.03830v1
DistilDoc: Knowledge Distillation for Visually-Rich Document Applications,"This work explores knowledge distillation (KD) for visually-rich document
(VRD) applications such as document layout analysis (DLA) and document image
classification (DIC). While VRD research is dependent on increasingly
sophisticated and cumbersome models, the field has neglected to study
efficiency via model compression. Here, we design a KD experimentation
methodology for more lean, performant models on document understanding (DU)
tasks that are integral within larger task pipelines. We carefully selected KD
strategies (response-based, feature-based) for distilling knowledge to and from
backbones with different architectures (ResNet, ViT, DiT) and capacities (base,
small, tiny). We study what affects the teacher-student knowledge gap and find
that some methods (tuned vanilla KD, MSE, SimKD with an apt projector) can
consistently outperform supervised student training. Furthermore, we design
downstream task setups to evaluate covariate shift and the robustness of
distilled DLA models on zero-shot layout-aware document visual question
answering (DocVQA). DLA-KD experiments result in a large mAP knowledge gap,
which unpredictably translates to downstream robustness, accentuating the need
to further explore how to efficiently obtain more semantic document layout
awareness.","['cs.CV', 'cs.AI', 'cs.LG']",http://arxiv.org/abs/2406.08226v1
Multimodal Adaptive Inference for Document Image Classification with Anytime Early Exiting,"This work addresses the need for a balanced approach between performance and
efficiency in scalable production environments for visually-rich document
understanding (VDU) tasks. Currently, there is a reliance on large document
foundation models that offer advanced capabilities but come with a heavy
computational burden. In this paper, we propose a multimodal early exit (EE)
model design that incorporates various training strategies, exit layer types
and placements. Our goal is to achieve a Pareto-optimal balance between
predictive performance and efficiency for multimodal document image
classification. Through a comprehensive set of experiments, we compare our
approach with traditional exit policies and showcase an improved
performance-efficiency trade-off. Our multimodal EE design preserves the
model's predictive capabilities, enhancing both speed and latency. This is
achieved through a reduction of over 20% in latency, while fully retaining the
baseline accuracy. This research represents the first exploration of multimodal
EE design within the VDU community, highlighting as well the effectiveness of
calibration in improving confidence scores for exiting at different layers.
Overall, our findings contribute to practical VDU applications by enhancing
both performance and efficiency.","['cs.CV', 'cs.CL', 'cs.LG']",http://arxiv.org/abs/2405.12705v1
CICA: Content-Injected Contrastive Alignment for Zero-Shot Document Image Classification,"Zero-shot learning has been extensively investigated in the broader field of
visual recognition, attracting significant interest recently. However, the
current work on zero-shot learning in document image classification remains
scarce. The existing studies either focus exclusively on zero-shot inference,
or their evaluation does not align with the established criteria of zero-shot
evaluation in the visual recognition domain. We provide a comprehensive
document image classification analysis in Zero-Shot Learning (ZSL) and
Generalized Zero-Shot Learning (GZSL) settings to address this gap. Our
methodology and evaluation align with the established practices of this domain.
Additionally, we propose zero-shot splits for the RVL-CDIP dataset.
Furthermore, we introduce CICA (pronounced 'ki-ka'), a framework that enhances
the zero-shot learning capabilities of CLIP. CICA consists of a novel 'content
module' designed to leverage any generic document-related textual information.
The discriminative features extracted by this module are aligned with CLIP's
text and image features using a novel 'coupled-contrastive' loss. Our module
improves CLIP's ZSL top-1 accuracy by 6.7% and GZSL harmonic mean by 24% on the
RVL-CDIP dataset. Our module is lightweight and adds only 3.3% more parameters
to CLIP. Our work sets the direction for future research in zero-shot document
classification.",['cs.CV'],http://arxiv.org/abs/2405.03660v1
LayoutMask: Enhance Text-Layout Interaction in Multi-modal Pre-training for Document Understanding,"Visually-rich Document Understanding (VrDU) has attracted much research
attention over the past years. Pre-trained models on a large number of document
images with transformer-based backbones have led to significant performance
gains in this field. The major challenge is how to fusion the different
modalities (text, layout, and image) of the documents in a unified model with
different pre-training tasks. This paper focuses on improving text-layout
interactions and proposes a novel multi-modal pre-training model, LayoutMask.
LayoutMask uses local 1D position, instead of global 1D position, as layout
input and has two pre-training objectives: (1) Masked Language Modeling:
predicting masked tokens with two novel masking strategies; (2) Masked Position
Modeling: predicting masked 2D positions to improve layout representation
learning. LayoutMask can enhance the interactions between text and layout
modalities in a unified model and produce adaptive and robust multi-modal
representations for downstream tasks. Experimental results show that our
proposed method can achieve state-of-the-art results on a wide variety of VrDU
problems, including form understanding, receipt understanding, and document
image classification.",['cs.CV'],http://arxiv.org/abs/2305.18721v2
EAML: Ensemble Self-Attention-based Mutual Learning Network for Document Image Classification,"In the recent past, complex deep neural networks have received huge interest
in various document understanding tasks such as document image classification
and document retrieval. As many document types have a distinct visual style,
learning only visual features with deep CNNs to classify document images have
encountered the problem of low inter-class discrimination, and high intra-class
structural variations between its categories. In parallel, text-level
understanding jointly learned with the corresponding visual properties within a
given document image has considerably improved the classification performance
in terms of accuracy. In this paper, we design a self-attention-based fusion
module that serves as a block in our ensemble trainable network. It allows to
simultaneously learn the discriminant features of image and text modalities
throughout the training stage. Besides, we encourage mutual learning by
transferring the positive knowledge between image and text modalities during
the training stage. This constraint is realized by adding a
truncated-Kullback-Leibler divergence loss Tr-KLD-Reg as a new regularization
term, to the conventional supervised setting. To the best of our knowledge,
this is the first time to leverage a mutual learning approach along with a
self-attention-based fusion module to perform document image classification.
The experimental results illustrate the effectiveness of our approach in terms
of accuracy for the single-modal and multi-modal modalities. Thus, the proposed
ensemble self-attention-based mutual learning model outperforms the
state-of-the-art classification results based on the benchmark RVL-CDIP and
Tobacco-3482 datasets.",['cs.CV'],http://arxiv.org/abs/2305.06923v1
Evaluating Adversarial Robustness on Document Image Classification,"Adversarial attacks and defenses have gained increasing interest on computer
vision systems in recent years, but as of today, most investigations are
limited to images. However, many artificial intelligence models actually handle
documentary data, which is very different from real world images. Hence, in
this work, we try to apply the adversarial attack philosophy on documentary and
natural data and to protect models against such attacks. We focus our work on
untargeted gradient-based, transfer-based and score-based attacks and evaluate
the impact of adversarial training, JPEG input compression and grey-scale input
transformation on the robustness of ResNet50 and EfficientNetB0 model
architectures. To the best of our knowledge, no such work has been conducted by
the community in order to study the impact of these attacks on the document
image classification task.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2304.12486v2
DiT: Self-supervised Pre-training for Document Image Transformer,"Image Transformer has recently achieved significant progress for natural
image understanding, either using supervised (ViT, DeiT, etc.) or
self-supervised (BEiT, MAE, etc.) pre-training techniques. In this paper, we
propose \textbf{DiT}, a self-supervised pre-trained \textbf{D}ocument
\textbf{I}mage \textbf{T}ransformer model using large-scale unlabeled text
images for Document AI tasks, which is essential since no supervised
counterparts ever exist due to the lack of human-labeled document images. We
leverage DiT as the backbone network in a variety of vision-based Document AI
tasks, including document image classification, document layout analysis, table
detection as well as text detection for OCR. Experiment results have
illustrated that the self-supervised pre-trained DiT model achieves new
state-of-the-art results on these downstream tasks, e.g. document image
classification (91.11 $\rightarrow$ 92.69), document layout analysis (91.0
$\rightarrow$ 94.9), table detection (94.23 $\rightarrow$ 96.55) and text
detection for OCR (93.07 $\rightarrow$ 94.29). The code and pre-trained models
are publicly available at \url{https://aka.ms/msdit}.",['cs.CV'],http://arxiv.org/abs/2203.02378v3
Domain Agnostic Few-Shot Learning For Document Intelligence,"Few-shot learning aims to generalize to novel classes with only a few samples
with class labels. Research in few-shot learning has borrowed techniques from
transfer learning, metric learning, meta-learning, and Bayesian methods. These
methods also aim to train models from limited training samples, and while
encouraging performance has been achieved, they often fail to generalize to
novel domains. Many of the existing meta-learning methods rely on training data
for which the base classes are sampled from the same domain as the novel
classes used for meta-testing. However, in many applications in the industry,
such as document classification, collecting large samples of data for
meta-learning is infeasible or impossible. While research in the field of the
cross-domain few-shot learning exists, it is mostly limited to computer vision.
To our knowledge, no work yet exists that examines the use of few-shot learning
for classification of semi-structured documents (scans of paper documents)
generated as part of a business workflow (forms, letters, bills, etc.). Here
the domain shift is significant, going from natural images to the
semi-structured documents of interest. In this work, we address the problem of
few-shot document image classification under domain shift. We evaluate our work
by extensive comparisons with existing methods. Experimental results
demonstrate that the proposed method shows consistent improvements on the
few-shot classification performance under domain shift.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2111.00007v1
Efficient Document Image Classification Using Region-Based Graph Neural Network,"Document image classification remains a popular research area because it can
be commercialized in many enterprise applications across different industries.
Recent advancements in large pre-trained computer vision and language models
and graph neural networks has lent document image classification many tools.
However using large pre-trained models usually requires substantial computing
resources which could defeat the cost-saving advantages of automatic document
image classification. In the paper we propose an efficient document image
classification framework that uses graph convolution neural networks and
incorporates textual, visual and layout information of the document. We have
rigorously benchmarked our proposed algorithm against several state-of-art
vision and language models on both publicly available dataset and a real-life
insurance document classification dataset. Empirical results on both publicly
available and real-world data show that our methods achieve near SOTA
performance yet require much less computing resources and time for model
training and inference. This results in solutions than offer better cost
advantages, especially in scalable deployment for enterprise applications. The
results showed that our algorithm can achieve classification performance quite
close to SOTA. We also provide comprehensive comparisons of computing
resources, model sizes, train and inference time between our proposed methods
and baselines. In addition we delineate the cost per image using our method and
other baselines.",['cs.CV'],http://arxiv.org/abs/2106.13802v1
Improving accuracy and speeding up Document Image Classification through parallel systems,"This paper presents a study showing the benefits of the EfficientNet models
compared with heavier Convolutional Neural Networks (CNNs) in the Document
Classification task, essential problem in the digitalization process of
institutions. We show in the RVL-CDIP dataset that we can improve previous
results with a much lighter model and present its transfer learning
capabilities on a smaller in-domain dataset such as Tobacco3482. Moreover, we
present an ensemble pipeline which is able to boost solely image input by
combining image model predictions with the ones generated by BERT model on
extracted text by OCR. We also show that the batch size can be effectively
increased without hindering its accuracy so that the training process can be
sped up by parallelizing throughout multiple GPUs, decreasing the computational
time needed. Lastly, we expose the training performance differences between
PyTorch and Tensorflow Deep Learning frameworks.","['cs.CV', 'cs.DC', 'cs.LG']",http://arxiv.org/abs/2006.09141v1
Self-Supervised Representation Learning on Document Images,"This work analyses the impact of self-supervised pre-training on document
images in the context of document image classification. While previous
approaches explore the effect of self-supervision on natural images, we show
that patch-based pre-training performs poorly on document images because of
their different structural properties and poor intra-sample semantic
information. We propose two context-aware alternatives to improve performance
on the Tobacco-3482 image classification task. We also propose a novel method
for self-supervision, which makes use of the inherent multi-modality of
documents (image and text), which performs better than other popular
self-supervised methods, including supervised ImageNet pre-training, on
document image classification scenarios with a limited amount of data.","['cs.CV', 'cs.LG', 'eess.IV', 'stat.ML', '68T05']",http://arxiv.org/abs/2004.10605v2
Document Image Classification with Intra-Domain Transfer Learning and Stacked Generalization of Deep Convolutional Neural Networks,"In this work, a region-based Deep Convolutional Neural Network framework is
proposed for document structure learning. The contribution of this work
involves efficient training of region based classifiers and effective
ensembling for document image classification. A primary level of `inter-domain'
transfer learning is used by exporting weights from a pre-trained VGG16
architecture on the ImageNet dataset to train a document classifier on whole
document images. Exploiting the nature of region based influence modelling, a
secondary level of `intra-domain' transfer learning is used for rapid training
of deep learning models for image segments. Finally, stacked generalization
based ensembling is utilized for combining the predictions of the base deep
neural network models. The proposed method achieves state-of-the-art accuracy
of 92.2% on the popular RVL-CDIP document image dataset, exceeding benchmarks
set by existing algorithms.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/1801.09321v3
Real-Time Document Image Classification using Deep CNN and Extreme Learning Machines,"This paper presents an approach for real-time training and testing for
document image classification. In production environments, it is crucial to
perform accurate and (time-)efficient training. Existing deep learning
approaches for classifying documents do not meet these requirements, as they
require much time for training and fine-tuning the deep architectures.
Motivated from Computer Vision, we propose a two-stage approach. The first
stage trains a deep network that works as feature extractor and in the second
stage, Extreme Learning Machines (ELMs) are used for classification. The
proposed approach outperforms all previously reported structural and deep
learning based methods with a final accuracy of 83.24% on Tobacco-3482 dataset,
leading to a relative error reduction of 25% when compared to a previous
Convolutional Neural Network (CNN) based approach (DeepDocClassifier). More
importantly, the training time of the ELM is only 1.176 seconds and the overall
prediction time for 2,482 images is 3.066 seconds. As such, this novel approach
makes deep learning-based document classification suitable for large-scale
real-time applications.",['cs.CV'],http://arxiv.org/abs/1711.05862v1
Analysis of Convolutional Neural Networks for Document Image Classification,"Convolutional Neural Networks (CNNs) are state-of-the-art models for document
image classification tasks. However, many of these approaches rely on
parameters and architectures designed for classifying natural images, which
differ from document images. We question whether this is appropriate and
conduct a large empirical study to find what aspects of CNNs most affect
performance on document images. Among other results, we exceed the
state-of-the-art on the RVL-CDIP dataset by using shear transform data
augmentation and an architecture designed for a larger input image.
Additionally, we analyze the learned features and find evidence that CNNs
trained on RVL-CDIP learn region-specific layout features.",['cs.CV'],http://arxiv.org/abs/1708.03273v1
Cutting the Error by Half: Investigation of Very Deep CNN and Advanced Training Strategies for Document Image Classification,"We present an exhaustive investigation of recent Deep Learning architectures,
algorithms, and strategies for the task of document image classification to
finally reduce the error by more than half. Existing approaches, such as the
DeepDocClassifier, apply standard Convolutional Network architectures with
transfer learning from the object recognition domain. The contribution of the
paper is threefold: First, it investigates recently introduced very deep neural
network architectures (GoogLeNet, VGG, ResNet) using transfer learning (from
real images). Second, it proposes transfer learning from a huge set of document
images, i.e. 400,000 documents. Third, it analyzes the impact of the amount of
training data (document images) and other parameters to the classification
abilities. We use two datasets, the Tobacco-3482 and the large-scale RVL-CDIP
dataset. We achieve an accuracy of 91.13% for the Tobacco-3482 dataset while
earlier approaches reach only 77.6%. Thus, a relative error reduction of more
than 60% is achieved. For the large dataset RVL-CDIP, an accuracy of 90.97% is
achieved, corresponding to a relative error reduction of 11.5%.",['cs.CV'],http://arxiv.org/abs/1704.03557v1
"Document image classification, with a specific view on applications of patent images","The main focus of this paper is document image classification and retrieval,
where we analyze and compare different parameters for the RunLeght Histogram
(RL) and Fisher Vector (FV) based image representations. We do an exhaustive
experimental study using different document image datasets, including the MARG
benchmarks, two datasets built on customer data and the images from the Patent
Image Classification task of the Clef-IP 2011. The aim of the study is to give
guidelines on how to best choose the parameters such that the same features
perform well on different tasks. As an example of such need, we describe the
Image-based Patent Retrieval task's of Clef-IP 2011, where we used the same
image representation to predict the image type and retrieve relevant patents.",['cs.CV'],http://arxiv.org/abs/1601.03295v1
Evaluation of Deep Convolutional Nets for Document Image Classification and Retrieval,"This paper presents a new state-of-the-art for document image classification
and retrieval, using features learned by deep convolutional neural networks
(CNNs). In object and scene analysis, deep neural nets are capable of learning
a hierarchical chain of abstraction from pixel inputs to concise and
descriptive representations. The current work explores this capacity in the
realm of document analysis, and confirms that this representation strategy is
superior to a variety of popular hand-crafted alternatives. Experiments also
show that (i) features extracted from CNNs are robust to compression, (ii) CNNs
trained on non-document images transfer well to document analysis tasks, and
(iii) enforcing region-specific feature-learning is unnecessary given
sufficient training data. This work also makes available a new labelled subset
of the IIT-CDIP collection, containing 400,000 document images across 16
categories, useful for training new CNNs for document analysis.","['cs.CV', 'cs.IR', 'cs.LG', 'cs.NE']",http://arxiv.org/abs/1502.07058v1
"On Learning Informative Trajectory Embeddings for Imitation, Classification and Regression","In real-world sequential decision making tasks like autonomous driving,
robotics, and healthcare, learning from observed state-action trajectories is
critical for tasks like imitation, classification, and clustering. For example,
self-driving cars must replicate human driving behaviors, while robots and
healthcare systems benefit from modeling decision sequences, whether or not
they come from expert data. Existing trajectory encoding methods often focus on
specific tasks or rely on reward signals, limiting their ability to generalize
across domains and tasks. Inspired by the success of embedding models like CLIP
and BERT in static domains, we propose a novel method for embedding
state-action trajectories into a latent space that captures the skills and
competencies in the dynamic underlying decision-making processes. This method
operates without the need for reward labels, enabling better generalization
across diverse domains and tasks. Our contributions are threefold: (1) We
introduce a trajectory embedding approach that captures multiple abilities from
state-action data. (2) The learned embeddings exhibit strong representational
power across downstream tasks, including imitation, classification, clustering,
and regression. (3) The embeddings demonstrate unique properties, such as
controlling agent behaviors in IQ-Learn and an additive structure in the latent
space. Experimental results confirm that our method outperforms traditional
approaches, offering more flexible and powerful trajectory representations for
various applications. Our code is available at
https://github.com/Erasmo1015/vte.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2501.09327v2
RichSpace: Enriching Text-to-Video Prompt Space via Text Embedding Interpolation,"Text-to-video generation models have made impressive progress, but they still
struggle with generating videos with complex features. This limitation often
arises from the inability of the text encoder to produce accurate embeddings,
which hinders the video generation model. In this work, we propose a novel
approach to overcome this challenge by selecting the optimal text embedding
through interpolation in the embedding space. We demonstrate that this method
enables the video generation model to produce the desired videos. Additionally,
we introduce a simple algorithm using perpendicular foot embeddings and cosine
similarity to identify the optimal interpolation embedding. Our findings
highlight the importance of accurate text embeddings and offer a pathway for
improving text-to-video generation performance.","['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG']",http://arxiv.org/abs/2501.09982v1
ASTRA: A Scene-aware TRAnsformer-based model for trajectory prediction,"We present ASTRA (A} Scene-aware TRAnsformer-based model for trajectory
prediction), a light-weight pedestrian trajectory forecasting model that
integrates the scene context, spatial dynamics, social inter-agent interactions
and temporal progressions for precise forecasting. We utilised a U-Net-based
feature extractor, via its latent vector representation, to capture scene
representations and a graph-aware transformer encoder for capturing social
interactions. These components are integrated to learn an agent-scene aware
embedding, enabling the model to learn spatial dynamics and forecast the future
trajectory of pedestrians. The model is designed to produce both deterministic
and stochastic outcomes, with the stochastic predictions being generated by
incorporating a Conditional Variational Auto-Encoder (CVAE). ASTRA also
proposes a simple yet effective weighted penalty loss function, which helps to
yield predictions that outperform a wide array of state-of-the-art
deterministic and generative models. ASTRA demonstrates an average improvement
of 27%/10% in deterministic/stochastic settings on the ETH-UCY dataset, and 26%
improvement on the PIE dataset, respectively, along with seven times fewer
parameters than the existing state-of-the-art model (see Figure 1).
Additionally, the model's versatility allows it to generalize across different
perspectives, such as Bird's Eye View (BEV) and Ego-Vehicle View (EVV).","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.09878v1
CrossModalityDiffusion: Multi-Modal Novel View Synthesis with Unified Intermediate Representation,"Geospatial imaging leverages data from diverse sensing modalities-such as EO,
SAR, and LiDAR, ranging from ground-level drones to satellite views. These
heterogeneous inputs offer significant opportunities for scene understanding
but present challenges in interpreting geometry accurately, particularly in the
absence of precise ground truth data. To address this, we propose
CrossModalityDiffusion, a modular framework designed to generate images across
different modalities and viewpoints without prior knowledge of scene geometry.
CrossModalityDiffusion employs modality-specific encoders that take multiple
input images and produce geometry-aware feature volumes that encode scene
structure relative to their input camera positions. The space where the feature
volumes are placed acts as a common ground for unifying input modalities. These
feature volumes are overlapped and rendered into feature images from novel
perspectives using volumetric rendering techniques. The rendered feature images
are used as conditioning inputs for a modality-specific diffusion model,
enabling the synthesis of novel images for the desired output modality. In this
paper, we show that jointly training different modules ensures consistent
geometric understanding across all modalities within the framework. We validate
CrossModalityDiffusion's capabilities on the synthetic ShapeNet cars dataset,
demonstrating its effectiveness in generating accurate and consistent novel
views across multiple imaging modalities and perspectives.","['cs.CV', 'cs.AI', 'eess.IV']",http://arxiv.org/abs/2501.09838v1
Generalized Single-Image-Based Morphing Attack Detection Using Deep Representations from Vision Transformer,"Face morphing attacks have posed severe threats to Face Recognition Systems
(FRS), which are operated in border control and passport issuance use cases.
Correspondingly, morphing attack detection algorithms (MAD) are needed to
defend against such attacks. MAD approaches must be robust enough to handle
unknown attacks in an open-set scenario where attacks can originate from
various morphing generation algorithms, post-processing and the diversity of
printers/scanners. The problem of generalization is further pronounced when the
detection has to be made on a single suspected image. In this paper, we propose
a generalized single-image-based MAD (S-MAD) algorithm by learning the encoding
from Vision Transformer (ViT) architecture. Compared to CNN-based
architectures, ViT model has the advantage on integrating local and global
information and hence can be suitable to detect the morphing traces widely
distributed among the face region. Extensive experiments are carried out on
face morphing datasets generated using publicly available FRGC face datasets.
Several state-of-the-art (SOTA) MAD algorithms, including representative ones
that have been publicly evaluated, have been selected and benchmarked with our
ViT-based approach. Obtained results demonstrate the improved detection
performance of the proposed S-MAD method on inter-dataset testing (when
different data is used for training and testing) and comparable performance on
intra-dataset testing (when the same data is used for training and testing)
experimental protocol.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.09817v1
Distilling Multi-modal Large Language Models for Autonomous Driving,"Autonomous driving demands safe motion planning, especially in critical
""long-tail"" scenarios. Recent end-to-end autonomous driving systems leverage
large language models (LLMs) as planners to improve generalizability to rare
events. However, using LLMs at test time introduces high computational costs.
To address this, we propose DiMA, an end-to-end autonomous driving system that
maintains the efficiency of an LLM-free (or vision-based) planner while
leveraging the world knowledge of an LLM. DiMA distills the information from a
multi-modal LLM to a vision-based end-to-end planner through a set of specially
designed surrogate tasks. Under a joint training strategy, a scene encoder
common to both networks produces structured representations that are
semantically grounded as well as aligned to the final planning objective.
Notably, the LLM is optional at inference, enabling robust planning without
compromising on efficiency. Training with DiMA results in a 37% reduction in
the L2 trajectory error and an 80% reduction in the collision rate of the
vision-based planner, as well as a 44% trajectory error reduction in longtail
scenarios. DiMA also achieves state-of-the-art performance on the nuScenes
planning benchmark.","['cs.CV', 'cs.RO']",http://arxiv.org/abs/2501.09757v1
"Lost in Translation, Found in Context: Sign Language Translation with Contextual Cues","Our objective is to translate continuous sign language into spoken language
text. Inspired by the way human interpreters rely on context for accurate
translation, we incorporate additional contextual cues together with the
signing video, into a new translation framework. Specifically, besides visual
sign recognition features that encode the input video, we integrate
complementary textual information from (i) captions describing the background
show, (ii) translation of previous sentences, as well as (iii) pseudo-glosses
transcribing the signing. These are automatically extracted and inputted along
with the visual features to a pre-trained large language model (LLM), which we
fine-tune to generate spoken language translations in text form. Through
extensive ablation studies, we show the positive contribution of each input cue
to the translation performance. We train and evaluate our approach on BOBSL --
the largest British Sign Language dataset currently available. We show that our
contextual approach significantly enhances the quality of the translations
compared to previously reported results on BOBSL, and also to state-of-the-art
methods that we implement as baselines. Furthermore, we demonstrate the
generality of our approach by applying it also to How2Sign, an American Sign
Language dataset, and achieve competitive results.",['cs.CV'],http://arxiv.org/abs/2501.09754v1
Robin: a Suite of Multi-Scale Vision-Language Models and the CHIRP Evaluation Benchmark,"The proliferation of Vision-Language Models (VLMs) in the past several years
calls for rigorous and comprehensive evaluation methods and benchmarks. This
work analyzes existing VLM evaluation techniques, including automated metrics,
AI-based assessments, and human evaluations across diverse tasks. We first
introduce Robin - a novel suite of VLMs that we built by combining Large
Language Models (LLMs) and Vision Encoders (VEs) at multiple scales, and use
Robin to identify shortcomings of current evaluation approaches across scales.
Next, to overcome the identified limitations, we introduce CHIRP - a new long
form response benchmark we developed for more robust and complete VLM
evaluation. We provide open access to the Robin training code, model suite, and
CHIRP benchmark to promote reproducibility and advance VLM research.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.09672v1
Fokker-Planck to Callan-Symanzik: evolution of weight matrices under training,"The dynamical evolution of a neural network during training has been an
incredibly fascinating subject of study. First principal derivation of generic
evolution of variables in statistical physics systems has proved useful when
used to describe training dynamics conceptually, which in practice means
numerically solving equations such as Fokker-Planck equation. Simulating entire
networks inevitably runs into the curse of dimensionality. In this paper, we
utilize Fokker-Planck to simulate the probability density evolution of
individual weight matrices in the bottleneck layers of a simple
2-bottleneck-layered auto-encoder and compare the theoretical evolutions
against the empirical ones by examining the output data distributions. We also
derive physically relevant partial differential equations such as
Callan-Symanzik and Kardar-Parisi-Zhang equations from the dynamical equation
we have.",['cs.LG'],http://arxiv.org/abs/2501.09659v1
Driving in the Occupancy World: Vision-Centric 4D Occupancy Forecasting and Planning via World Models for Autonomous Driving,"World models envision potential future states based on various ego actions.
They embed extensive knowledge about the driving environment, facilitating safe
and scalable autonomous driving. Most existing methods primarily focus on
either data generation or the pretraining paradigms of world models. Unlike the
aforementioned prior works, we propose Drive-OccWorld, which adapts a
vision-centric 4D forecasting world model to end-to-end planning for autonomous
driving. Specifically, we first introduce a semantic and motion-conditional
normalization in the memory module, which accumulates semantic and dynamic
information from historical BEV embeddings. These BEV features are then
conveyed to the world decoder for future occupancy and flow forecasting,
considering both geometry and spatiotemporal modeling. Additionally, we propose
injecting flexible action conditions, such as velocity, steering angle,
trajectory, and commands, into the world model to enable controllable
generation and facilitate a broader range of downstream applications.
Furthermore, we explore integrating the generative capabilities of the 4D world
model with end-to-end planning, enabling continuous forecasting of future
states and the selection of optimal trajectories using an occupancy-based cost
function. Comprehensive experiments conducted on the nuScenes,
nuScenes-Occupancy, and Lyft-Level5 datasets illustrate that our method can
generate plausible and controllable 4D occupancy, paving the way for
advancements in driving world generation and end-to-end planning. Project page:
https://drive-occworld.github.io/",['cs.CV'],http://arxiv.org/abs/2408.14197v3
Steering Large Language Models with Feature Guided Activation Additions,"Effective and reliable control over large language model (LLM) behavior is a
significant challenge. While activation steering methods, which add steering
vectors to a model's hidden states, are a promising approach, existing
techniques often lack precision and interpretability in how they influence
model outputs. We introduce Feature Guided Activation Additions (FGAA), a novel
activation steering method that leverages insights from Contrastive Activation
Addition (CAA) and Sparse Autoencoder-Targeted Steering (SAE-TS). By operating
in the latent space of a Sparse Autoencoder (SAE) and employing optimization
techniques to select desired SAE features, FGAA constructs precise steering
vectors that provide better steering effects while maintaining coherence of
steered model outputs. In this regard, evaluations on Gemma-2-2B and Gemma-2-9B
models across various steering tasks demonstrate that FGAA outperforms existing
steering methods of CAA, SAE decoder steering, and SAE-TS. Our results also
highlight important trade-offs between steering scale and general model
capabilities that are consistent across all tested steering methods.","['cs.LG', 'cs.AI', 'cs.CL']",http://arxiv.org/abs/2501.09929v1
"GenSC-6G: A Prototype Testbed for Integrated Generative AI, Quantum, and Semantic Communication","We introduce a prototyping testbed, GenSC-6G, developed to generate a
comprehensive dataset that supports the integration of generative artificial
intelligence (AI), quantum computing, and semantic communication for emerging
sixth-generation (6G) applications. The GenSC-6G dataset is designed with
noise-augmented synthetic data optimized for semantic decoding, classification,
and localization tasks, significantly enhancing flexibility for diverse
AI-driven communication applications. This adaptable prototype supports
seamless modifications across baseline models, communication modules, and
goal-oriented decoders. Case studies demonstrate its application in lightweight
classification, semantic upsampling, and edge-based language inference under
noise conditions. The GenSC-6G dataset serves as a scalable and robust resource
for developing goal-oriented communication systems tailored to the growing
demands of 6G networks.","['cs.AI', 'eess.SP', 'quant-ph']",http://arxiv.org/abs/2501.09918v1
Super-class guided Transformer for Zero-Shot Attribute Classification,"Attribute classification is crucial for identifying specific characteristics
within image regions. Vision-Language Models (VLMs) have been effective in
zero-shot tasks by leveraging their general knowledge from large-scale
datasets. Recent studies demonstrate that transformer-based models with
class-wise queries can effectively address zero-shot multi-label
classification. However, poor utilization of the relationship between seen and
unseen attributes makes the model lack generalizability. Additionally,
attribute classification generally involves many attributes, making maintaining
the model's scalability difficult. To address these issues, we propose
Super-class guided transFormer (SugaFormer), a novel framework that leverages
super-classes to enhance scalability and generalizability for zero-shot
attribute classification. SugaFormer employs Super-class Query Initialization
(SQI) to reduce the number of queries, utilizing common semantic information
from super-classes, and incorporates Multi-context Decoding (MD) to handle
diverse visual cues. To strengthen generalizability, we introduce two knowledge
transfer strategies that utilize VLMs. During training, Super-class guided
Consistency Regularization (SCR) aligns model's features with VLMs using
super-class guided prompts, and during inference, Zero-shot Retrieval-based
Score Enhancement (ZRSE) refines predictions for unseen attributes. Extensive
experiments demonstrate that SugaFormer achieves state-of-the-art performance
across three widely-used attribute classification benchmarks under zero-shot,
and cross-dataset transfer settings. Our code is available at
https://github.com/mlvlab/SugaFormer.",['cs.CV'],http://arxiv.org/abs/2501.05728v2
Text-driven Adaptation of Foundation Models for Few-shot Surgical Workflow Analysis,"Purpose: Surgical workflow analysis is crucial for improving surgical
efficiency and safety. However, previous studies rely heavily on large-scale
annotated datasets, posing challenges in cost, scalability, and reliance on
expert annotations. To address this, we propose Surg-FTDA (Few-shot Text-driven
Adaptation), designed to handle various surgical workflow analysis tasks with
minimal paired image-label data.
  Methods: Our approach has two key components. First, Few-shot selection-based
modality alignment selects a small subset of images and aligns their embeddings
with text embeddings from the downstream task, bridging the modality gap.
Second, Text-driven adaptation leverages only text data to train a decoder,
eliminating the need for paired image-text data. This decoder is then applied
to aligned image embeddings, enabling image-related tasks without explicit
image-text pairs.
  Results: We evaluate our approach to generative tasks (image captioning) and
discriminative tasks (triplet recognition and phase recognition). Results show
that Surg-FTDA outperforms baselines and generalizes well across downstream
tasks.
  Conclusion: We propose a text-driven adaptation approach that mitigates the
modality gap and handles multiple downstream tasks in surgical workflow
analysis, with minimal reliance on large annotated datasets. The code and
dataset will be released in https://github.com/TingxuanSix/Surg-FTDA.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.09555v1
Teaching Wav2Vec2 the Language of the Brain,"The decoding of continuously spoken speech from neuronal activity has the
potential to become an important clinical solution for paralyzed patients. Deep
Learning Brain Computer Interfaces (BCIs) have recently successfully mapped
neuronal activity to text contents in subjects who attempted to formulate
speech. However, only small BCI datasets are available. In contrast, labeled
data and pre-trained models for the closely related task of speech recognition
from audio are widely available. One such model is Wav2Vec2 which has been
trained in a self-supervised fashion to create meaningful representations of
speech audio data. In this study, we show that patterns learned by Wav2Vec2 are
transferable to brain data. Specifically, we replace its audio feature
extractor with an untrained Brain Feature Extractor (BFE) model. We then
execute full fine-tuning with pre-trained weights for Wav2Vec2, training ''from
scratch'' without pre-trained weights as well as freezing a pre-trained
Wav2Vec2 and training only the BFE each for 45 different BFE architectures.
Across these experiments, the best run is from full fine-tuning with
pre-trained weights, achieving a Character Error Rate (CER) of 18.54\%,
outperforming the best training from scratch run by 20.46\% and that of frozen
Wav2Vec2 training by 15.92\% percentage points. These results indicate that
knowledge transfer from audio speech recognition to brain decoding is possible
and significantly improves brain decoding performance for the same
architectures. Related source code is available at
https://github.com/tfiedlerdev/Wav2Vec2ForBrain.",['cs.LG'],http://arxiv.org/abs/2501.09459v1
Making Your Dreams A Reality: Decoding the Dreams into a Coherent Video Story from fMRI Signals,"This paper studies the brave new idea for Multimedia community, and proposes
a novel framework to convert dreams into coherent video narratives using fMRI
data. Essentially, dreams have intrigued humanity for centuries, offering
glimpses into our subconscious minds. Recent advancements in brain imaging,
particularly functional magnetic resonance imaging (fMRI), have provided new
ways to explore the neural basis of dreaming. By combining subjective dream
experiences with objective neurophysiological data, we aim to understand the
visual aspects of dreams and create complete video narratives. Our process
involves three main steps: reconstructing visual perception, decoding dream
imagery, and integrating dream stories. Using innovative techniques in fMRI
analysis and language modeling, we seek to push the boundaries of dream
research and gain deeper insights into visual experiences during sleep. This
technical report introduces a novel approach to visually decoding dreams using
fMRI signals and weaving dream visuals into narratives using language models.
We gather a dataset of dreams along with descriptions to assess the
effectiveness of our framework.",['cs.CV'],http://arxiv.org/abs/2501.09350v1
UVRM: A Scalable 3D Reconstruction Model from Unposed Videos,"Large Reconstruction Models (LRMs) have recently become a popular method for
creating 3D foundational models. Training 3D reconstruction models with 2D
visual data traditionally requires prior knowledge of camera poses for the
training samples, a process that is both time-consuming and prone to errors.
Consequently, 3D reconstruction training has been confined to either synthetic
3D datasets or small-scale datasets with annotated poses. In this study, we
investigate the feasibility of 3D reconstruction using unposed video data of
various objects. We introduce UVRM, a novel 3D reconstruction model capable of
being trained and evaluated on monocular videos without requiring any
information about the pose. UVRM uses a transformer network to implicitly
aggregate video frames into a pose-invariant latent feature space, which is
then decoded into a tri-plane 3D representation. To obviate the need for
ground-truth pose annotations during training, UVRM employs a combination of
the score distillation sampling (SDS) method and an analysis-by-synthesis
approach, progressively synthesizing pseudo novel-views using a pre-trained
diffusion model. We qualitatively and quantitatively evaluate UVRM's
performance on the G-Objaverse and CO3D datasets without relying on pose
information. Extensive experiments show that UVRM is capable of effectively and
efficiently reconstructing a wide range of 3D objects from unposed videos.",['cs.CV'],http://arxiv.org/abs/2501.09347v1
Prompt-CAM: A Simpler Interpretable Transformer for Fine-Grained Analysis,"We present a simple usage of pre-trained Vision Transformers (ViTs) for
fine-grained analysis, aiming to identify and localize the traits that
distinguish visually similar categories, such as different bird species or dog
breeds. Pre-trained ViTs such as DINO have shown remarkable capabilities to
extract localized, informative features. However, using saliency maps like
Grad-CAM can hardly point out the traits: they often locate the whole object by
a blurred, coarse heatmap, not traits. We propose a novel approach Prompt Class
Attention Map (Prompt-CAM) to the rescue. Prompt-CAM learns class-specific
prompts to a pre-trained ViT and uses the corresponding outputs for
classification. To classify an image correctly, the true-class prompt must
attend to the unique image patches not seen in other classes' images, i.e.,
traits. As such, the true class's multi-head attention maps reveal traits and
their locations. Implementation-wise, Prompt-CAM is almost a free lunch by
simply modifying the prediction head of Visual Prompt Tuning (VPT). This makes
Prompt-CAM fairly easy to train and apply, sharply contrasting other
interpretable methods that design specific models and training processes. It is
even simpler than the recently published INterpretable TRansformer (INTR),
whose encoder-decoder architecture prevents it from leveraging pre-trained
ViTs. Extensive empirical studies on a dozen datasets from various domains
(e.g., birds, fishes, insects, fungi, flowers, food, and cars) validate
Prompt-CAM superior interpretation capability.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.09333v1
"New Fashion Products Performance Forecasting: A Survey on Evolutions, Models and Emerging Trends","The fast fashion industry's insatiable demand for new styles and rapid
production cycles has led to a significant environmental burden.
Overproduction, excessive waste, and harmful chemicals have contributed to the
negative environmental impact of the industry. To mitigate these issues, a
paradigm shift that prioritizes sustainability and efficiency is urgently
needed. Integrating learning-based predictive analytics into the fashion
industry represents a significant opportunity to address environmental
challenges and drive sustainable practices. By forecasting fashion trends and
optimizing production, brands can reduce their ecological footprint while
remaining competitive in a rapidly changing market. However, one of the key
challenges in forecasting fashion sales is the dynamic nature of consumer
preferences. Fashion is acyclical, with trends constantly evolving and
resurfacing. In addition, cultural changes and unexpected events can disrupt
established patterns. This problem is also known as New Fashion Products
Performance Forecasting (NFPPF), and it has recently gained more and more
interest in the global research landscape. Given its multidisciplinary nature,
the field of NFPPF has been approached from many different angles. This
comprehensive survey wishes to provide an up-to-date overview that focuses on
learning-based NFPPF strategies. The survey is based on the Preferred Reporting
Items for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow,
allowing for a systematic and complete literature review. In particular, we
propose the first taxonomy that covers the learning panorama for NFPPF,
examining in detail the different methodologies used to increase the amount of
multimodal information, as well as the state-of-the-art available datasets.
Finally, we discuss the challenges and future directions.","['cs.LG', 'cs.CV']",http://arxiv.org/abs/2501.10324v1
Exploring the Implementation of AI in Early Onset Interviews to Help Mitigate Bias,"This paper investigates the application of artificial intelligence (AI) in
early-stage recruitment interviews in order to reduce inherent bias,
specifically sentiment bias. Traditional interviewers are often subject to
several biases, including interviewer bias, social desirability effects, and
even confirmation bias. In turn, this leads to non-inclusive hiring practices,
and a less diverse workforce. This study further analyzes various AI
interventions that are present in the marketplace today such as multimodal
platforms and interactive candidate assessment tools in order to gauge the
current market usage of AI in early-stage recruitment. However, this paper aims
to use a unique AI system that was developed to transcribe and analyze
interview dynamics, which emphasize skill and knowledge over emotional
sentiments. Results indicate that AI effectively minimizes sentiment-driven
biases by 41.2%, suggesting its revolutionizing power in companies' recruitment
processes for improved equity and efficiency.",['cs.AI'],http://arxiv.org/abs/2501.09890v1
The Dark Side of Rich Rewards: Understanding and Mitigating Noise in VLM Rewards,"While Vision-Language Models (VLMs) are increasingly used to generate reward
signals for training embodied agents to follow instructions, our research
reveals that agents guided by VLM rewards often underperform compared to those
employing only intrinsic (exploration-driven) rewards, contradicting
expectations set by recent work. We hypothesize that false positive rewards --
instances where unintended trajectories are incorrectly rewarded -- are more
detrimental than false negatives. Our analysis confirms this hypothesis,
revealing that the widely used cosine similarity metric is prone to false
positive reward estimates. To address this, we introduce BiMI ({Bi}nary
{M}utual {I}nformation), a novel reward function designed to mitigate noise.
BiMI significantly enhances learning efficiency across diverse and challenging
embodied navigation environments. Our findings offer a nuanced understanding of
how different types of reward noise impact agent learning and highlight the
importance of addressing multimodal reward signal noise when training embodied
agents","['cs.LG', 'cs.RO']",http://arxiv.org/abs/2409.15922v4
U-Fair: Uncertainty-based Multimodal Multitask Learning for Fairer Depression Detection,"Machine learning bias in mental health is becoming an increasingly pertinent
challenge. Despite promising efforts indicating that multitask approaches often
work better than unitask approaches, there is minimal work investigating the
impact of multitask learning on performance and fairness in depression
detection nor leveraged it to achieve fairer prediction outcomes. In this work,
we undertake a systematic investigation of using a multitask approach to
improve performance and fairness for depression detection. We propose a novel
gender-based task-reweighting method using uncertainty grounded in how the
PHQ-8 questionnaire is structured. Our results indicate that, although a
multitask approach improves performance and fairness compared to a unitask
approach, the results are not always consistent and we see evidence of negative
transfer and a reduction in the Pareto frontier, which is concerning given the
high-stake healthcare setting. Our proposed approach of gender-based
reweighting with uncertainty improves performance and fairness and alleviates
both challenges to a certain extent. Our findings on each PHQ-8 subitem task
difficulty are also in agreement with the largest study conducted on the PHQ-8
subitem discrimination capacity, thus providing the very first tangible
evidence linking ML findings with large-scale empirical population studies
conducted on the PHQ-8.",['cs.LG'],http://arxiv.org/abs/2501.09687v1
VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction,"Recent Multimodal Large Language Models (MLLMs) have typically focused on
integrating visual and textual modalities, with less emphasis placed on the
role of speech in enhancing interaction. However, speech plays a crucial role
in multimodal dialogue systems, and implementing high-performance in both
vision and speech tasks remains a significant challenge due to the fundamental
modality differences. In this paper, we propose a carefully designed
multi-stage training methodology that progressively trains LLM to understand
both visual and speech information, ultimately enabling fluent vision and
speech interaction. Our approach not only preserves strong vision-language
capacity, but also enables efficient speech-to-speech dialogue capabilities
without separate ASR and TTS modules, significantly accelerating multimodal
end-to-end response speed. By comparing our method against state-of-the-art
counterparts across benchmarks for image, video, and speech tasks, we
demonstrate that our model is equipped with both strong visual and speech
capabilities, making near real-time vision and speech interaction.","['cs.CV', 'cs.SD', 'eess.AS']",http://arxiv.org/abs/2501.01957v2
Multimodal Marvels of Deep Learning in Medical Diagnosis: A Comprehensive Review of COVID-19 Detection,"This study presents a comprehensive review of the potential of multimodal
deep learning (DL) in medical diagnosis, using COVID-19 as a case example.
Motivated by the success of artificial intelligence applications during the
COVID-19 pandemic, this research aims to uncover the capabilities of DL in
disease screening, prediction, and classification, and to derive insights that
enhance the resilience, sustainability, and inclusiveness of science,
technology, and innovation systems. Adopting a systematic approach, we
investigate the fundamental methodologies, data sources, preprocessing steps,
and challenges encountered in various studies and implementations. We explore
the architecture of deep learning models, emphasising their data-specific
structures and underlying algorithms. Subsequently, we compare different deep
learning strategies utilised in COVID-19 analysis, evaluating them based on
methodology, data, performance, and prerequisites for future research. By
examining diverse data types and diagnostic modalities, this research
contributes to scientific understanding and knowledge of the multimodal
application of DL and its effectiveness in diagnosis. We have implemented and
analysed 11 deep learning models using COVID-19 image, text, and speech (ie,
cough) data. Our analysis revealed that the MobileNet model achieved the
highest accuracy of 99.97% for COVID-19 image data and 93.73% for speech data
(i.e., cough). However, the BiGRU model demonstrated superior performance in
COVID-19 text classification with an accuracy of 99.89%. The broader
implications of this research suggest potential benefits for other domains and
disciplines that could leverage deep learning techniques for image, text, and
speech analysis.","['cs.LG', 'cs.SD', 'eess.AS', 'eess.IV']",http://arxiv.org/abs/2501.09506v1
Omni-Emotion: Extending Video MLLM with Detailed Face and Audio Modeling for Multimodal Emotion Analysis,"Understanding emotions accurately is essential for fields like human-computer
interaction. Due to the complexity of emotions and their multi-modal nature
(e.g., emotions are influenced by facial expressions and audio), researchers
have turned to using multi-modal models to understand human emotions rather
than single-modality. However, current video multi-modal large language models
(MLLMs) encounter difficulties in effectively integrating audio and identifying
subtle facial micro-expressions. Furthermore, the lack of detailed emotion
analysis datasets also limits the development of multimodal emotion analysis.
To address these issues, we introduce a self-reviewed dataset and a
human-reviewed dataset, comprising 24,137 coarse-grained samples and 3,500
manually annotated samples with detailed emotion annotations, respectively.
These datasets allow models to learn from diverse scenarios and better
generalize to real-world applications. Moreover, in addition to the audio
modeling, we propose to explicitly integrate facial encoding models into the
existing advanced Video MLLM, enabling the MLLM to effectively unify audio and
the subtle facial cues for emotion understanding. By aligning these features
within a unified space and employing instruction tuning in our proposed
datasets, our Omni-Emotion achieves state-of-the-art performance in both
emotion recognition and reasoning tasks.",['cs.CV'],http://arxiv.org/abs/2501.09502v1
YETI (YET to Intervene) Proactive Interventions by Multimodal AI Agents in Augmented Reality Tasks,"Multimodal AI Agents are AI models that have the capability of interactively
and cooperatively assisting human users to solve day-to-day tasks. Augmented
Reality (AR) head worn devices can uniquely improve the user experience of
solving procedural day-to-day tasks by providing egocentric multimodal (audio
and video) observational capabilities to AI Agents. Such AR capabilities can
help AI Agents see and listen to actions that users take which can relate to
multimodal capabilities of human users. Existing AI Agents, either Large
Language Models (LLMs) or Multimodal Vision-Language Models (VLMs) are reactive
in nature, which means that models cannot take an action without reading or
listening to the human user's prompts. Proactivity of AI Agents on the other
hand can help the human user detect and correct any mistakes in agent observed
tasks, encourage users when they do tasks correctly or simply engage in
conversation with the user - akin to a human teaching or assisting a user. Our
proposed YET to Intervene (YETI) multimodal agent focuses on the research
question of identifying circumstances that may require the agent to intervene
proactively. This allows the agent to understand when it can intervene in a
conversation with human users that can help the user correct mistakes on tasks,
like cooking, using AR. Our YETI Agent learns scene understanding signals based
on interpretable notions of Structural Similarity (SSIM) on consecutive video
frames. We also define the alignment signal which the AI Agent can learn to
identify if the video frames corresponding to the user's actions on the task
are consistent with expected actions. These signals are used by our AI Agent to
determine when it should proactively intervene. We compare our results on the
instances of proactive intervention in the HoloAssist multimodal benchmark for
an expert agent guiding a user to complete procedural tasks.","['cs.AI', 'cs.CV', 'cs.ET', 'cs.MA', 'I.2; I.2.10; I.2.11; I.2.1; I.2.7; I.4.8; I.4.9']",http://arxiv.org/abs/2501.09355v1
Multimodal-to-Text Prompt Engineering in Large Language Models Using Feature Embeddings for GNSS Interference Characterization,"Large language models (LLMs) are advanced AI systems applied across various
domains, including NLP, information retrieval, and recommendation systems.
Despite their adaptability and efficiency, LLMs have not been extensively
explored for signal processing tasks, particularly in the domain of global
navigation satellite system (GNSS) interference monitoring. GNSS interference
monitoring is essential to ensure the reliability of vehicle localization on
roads, a critical requirement for numerous applications. However, GNSS-based
positioning is vulnerable to interference from jamming devices, which can
compromise its accuracy. The primary objective is to identify, classify, and
mitigate these interferences. Interpreting GNSS snapshots and the associated
interferences presents significant challenges due to the inherent complexity,
including multipath effects, diverse interference types, varying sensor
characteristics, and satellite constellations. In this paper, we extract
features from a large GNSS dataset and employ LLaVA to retrieve relevant
information from an extensive knowledge base. We employ prompt engineering to
interpret the interferences and environmental factors, and utilize t-SNE to
analyze the feature embeddings. Our findings demonstrate that the proposed
method is capable of visual and logical reasoning within the GNSS context.
Furthermore, our pipeline outperforms state-of-the-art machine learning models
in interference classification tasks.","['cs.AI', 'eess.SP', '68T30, 68T05', 'H.1; H.5; I.4.9; I.4.10']",http://arxiv.org/abs/2501.05079v2
T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video Generation,"Text-to-video (T2V) generative models have advanced significantly, yet their
ability to compose different objects, attributes, actions, and motions into a
video remains unexplored. Previous text-to-video benchmarks also neglect this
important ability for evaluation. In this work, we conduct the first systematic
study on compositional text-to-video generation. We propose T2V-CompBench, the
first benchmark tailored for compositional text-to-video generation.
T2V-CompBench encompasses diverse aspects of compositionality, including
consistent attribute binding, dynamic attribute binding, spatial relationships,
motion binding, action binding, object interactions, and generative numeracy.
We further carefully design evaluation metrics of multimodal large language
model (MLLM)-based, detection-based, and tracking-based metrics, which can
better reflect the compositional text-to-video generation quality of seven
proposed categories with 1400 text prompts. The effectiveness of the proposed
metrics is verified by correlation with human evaluations. We also benchmark
various text-to-video generative models and conduct in-depth analysis across
different models and various compositional categories. We find that
compositional text-to-video generation is highly challenging for current
models, and we hope our attempt could shed light on future research in this
direction.",['cs.CV'],http://arxiv.org/abs/2407.14505v2
Compression with Global Guidance: Towards Training-free High-Resolution MLLMs Acceleration,"Multimodal large language models (MLLMs) have attracted considerable
attention due to their exceptional performance in visual content understanding
and reasoning. However, their inference efficiency has been a notable concern,
as the increasing length of multimodal contexts leads to quadratic complexity.
Token compression techniques, which reduce the number of visual tokens, have
demonstrated their effectiveness in reducing computational costs. Yet, these
approaches have struggled to keep pace with the rapid advancements in MLLMs,
especially the AnyRes strategy in the context of high-resolution image
understanding. In this paper, we propose a novel token compression method,
GlobalCom$^2$, tailored for high-resolution MLLMs that receive both the
thumbnail and multiple crops. GlobalCom$^2$ treats the tokens derived from the
thumbnail as the ""commander"" of the entire token compression process, directing
the allocation of retention ratios and the specific compression for each crop.
In this way, redundant tokens are eliminated while important local details are
adaptively preserved to the highest extent feasible. Empirical results across
10 benchmarks reveal that GlobalCom$^2$ achieves an optimal balance between
performance and efficiency, and consistently outperforms state-of-the-art token
compression methods with LLaVA-NeXT-7B/13B models. Our code is released at
https://github.com/xuyang-liu16/GlobalCom2.",['cs.CV'],http://arxiv.org/abs/2501.05179v2
SAIF: A Comprehensive Framework for Evaluating the Risks of Generative AI in the Public Sector,"The rapid adoption of generative AI in the public sector, encompassing
diverse applications ranging from automated public assistance to welfare
services and immigration processes, highlights its transformative potential
while underscoring the pressing need for thorough risk assessments. Despite its
growing presence, evaluations of risks associated with AI-driven systems in the
public sector remain insufficiently explored. Building upon an established
taxonomy of AI risks derived from diverse government policies and corporate
guidelines, we investigate the critical risks posed by generative AI in the
public sector while extending the scope to account for its multimodal
capabilities. In addition, we propose a Systematic dAta generatIon Framework
for evaluating the risks of generative AI (SAIF). SAIF involves four key
stages: breaking down risks, designing scenarios, applying jailbreak methods,
and exploring prompt types. It ensures the systematic and consistent generation
of prompt data, facilitating a comprehensive evaluation while providing a solid
foundation for mitigating the risks. Furthermore, SAIF is designed to
accommodate emerging jailbreak methods and evolving prompt types, thereby
enabling effective responses to unforeseen risk scenarios. We believe that this
study can play a crucial role in fostering the safe and responsible integration
of generative AI into the public sector.","['cs.AI', 'cs.CL', 'cs.CY']",http://arxiv.org/abs/2501.08814v1
The Devil is in Temporal Token: High Quality Video Reasoning Segmentation,"Existing methods for Video Reasoning Segmentation rely heavily on a single
special token to represent the object in the keyframe or the entire video,
inadequately capturing spatial complexity and inter-frame motion. To overcome
these challenges, we propose VRS-HQ, an end-to-end video reasoning segmentation
approach that leverages Multimodal Large Language Models (MLLMs) to inject rich
spatiotemporal features into hierarchical tokens.Our key innovations include a
Temporal Dynamic Aggregation (TDA) and a Token-driven Keyframe Selection (TKS).
Specifically, we design frame-level <SEG> and temporal-level <TAK> tokens that
utilize MLLM's autoregressive learning to effectively capture both local and
global information. Subsequently, we apply a similarity-based weighted fusion
and frame selection strategy, then utilize SAM2 to perform keyframe
segmentation and propagation. To enhance keyframe localization accuracy, the
TKS filters keyframes based on SAM2's occlusion scores during inference. VRS-HQ
achieves state-of-the-art performance on ReVOS, surpassing VISA by
5.9%/12.5%/9.1% in J&F scores across the three subsets. These results highlight
the strong temporal reasoning and segmentation capabilities of our method. Code
and model weights will be released at VRS-HQ.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2501.08549v1
Multimodal Fake News Video Explanation Generation,"Multi-modal explanation involves the assessment of the veracity of a variety
of different content, and relies on multiple information modalities to
comprehensively consider the relevance and consistency between modalities. Most
existing fake news video detection methods focus on improving accuracy while
ignoring the importance of providing explanations. In this paper, we propose a
novel problem - Fake News Video Explanation (FNVE) - Given a multimodal news
containing both video and caption text, we aim to generate natural language
explanations to reveal the truth of predictions. To this end, we develop
FakeNVE, a new dataset of explanations for truthfully multimodal posts, where
each explanation is a natural language (English) sentence describing the
attribution of a news thread. We benchmark FakeNVE by using a multimodal
transformer-based architecture. Subsequently, a BART-based autoregressive
decoder is used as the generator. Empirical results show compelling results for
various baselines (applicable to FNVE) across multiple evaluation metrics. We
also perform human evaluation on explanation generation, achieving high scores
for both adequacy and fluency.","['cs.CV', 'cs.MM']",http://arxiv.org/abs/2501.08514v1
FLAVARS: A Multimodal Foundational Language and Vision Alignment Model for Remote Sensing,"Remote sensing imagery is dense with objects and contextual visual
information. There is a recent trend to combine paired satellite images and
text captions for pretraining performant encoders for downstream tasks.
However, while contrastive image-text methods like CLIP enable vision-language
alignment and zero-shot classification ability, vision-only downstream
performance tends to degrade compared to image-only pretraining, such as MAE.
In this paper, we propose FLAVARS, a pretraining method that combines the best
of both contrastive learning and masked modeling, along with geospatial
alignment via contrastive location encoding. We find that FLAVARS significantly
outperforms a baseline of SkyCLIP for vision-only tasks such as KNN
classification and semantic segmentation, +6\% mIOU on SpaceNet1, while
retaining the ability to perform zero-shot classification, unlike MAE
pretrained methods.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2501.08490v1
Asymmetric Contrastive Multimodal Learning for Advancing Chemical Understanding,"The versatility of multimodal deep learning holds tremendous promise for
advancing scientific research and practical applications. As this field
continues to evolve, the collective power of cross-modal analysis promises to
drive transformative innovations, leading us to new frontiers in chemical
understanding and discovery. Hence, we introduce Asymmetric Contrastive
Multimodal Learning (ACML) as a novel approach tailored for molecules,
showcasing its potential to advance the field of chemistry. ACML harnesses the
power of effective asymmetric contrastive learning to seamlessly transfer
information from various chemical modalities to molecular graph
representations. By combining pre-trained chemical unimodal encoders and a
shallow-designed graph encoder with 5 layers, ACML facilitates the assimilation
of coordinated chemical semantics from different modalities, leading to
comprehensive representation learning with efficient training. We demonstrate
the effectiveness of this framework through large-scale cross-modality
retrieval and isomer discrimination tasks. Additionally, ACML enhances
interpretability by revealing chemical semantics in graph presentations and
bolsters the expressive power of graph neural networks, as evidenced by
improved performance in molecular property prediction tasks from MoleculeNet
and TDC. ACML exhibits its capability to revolutionize chemical research and
applications, providing a deeper understanding of the chemical semantics of
different modalities.",['cs.LG'],http://arxiv.org/abs/2311.06456v4
Automatic Fused Multimodal Deep Learning for Plant Identification,"Plant classification is vital for ecological conservation and agricultural
productivity, enhancing our understanding of plant growth dynamics and aiding
species preservation. The advent of deep learning (DL) techniques has
revolutionized this field by enabling autonomous feature extraction,
significantly reducing the dependence on manual expertise. However,
conventional DL models often rely solely on single data sources, failing to
capture the full biological diversity of plant species comprehensively. Recent
research has turned to multimodal learning to overcome this limitation by
integrating multiple data types, which enriches the representation of plant
characteristics. This shift introduces the challenge of determining the optimal
point for modality fusion. In this paper, we introduce a pioneering multimodal
DL-based approach for plant classification with automatic modality fusion.
Utilizing the multimodal fusion architecture search, our method integrates
images from multiple plant organs--flowers, leaves, fruits, and stems--into a
cohesive model. Our method achieves 82.61% accuracy on 979 classes of the
PlantCLEF2015 dataset, surpassing state-of-the-art methods and outperforming
late fusion by 10.33%. Through the incorporation of multimodal dropout, our
approach demonstrates strong robustness to missing modalities. We validate our
model against established benchmarks using standard performance metrics and
McNemar's test, further underscoring its superiority.","['cs.CV', 'cs.AI', 'cs.LG']",http://arxiv.org/abs/2406.01455v2
HEALNet: Multimodal Fusion for Heterogeneous Biomedical Data,"Technological advances in medical data collection, such as high-throughput
genomic sequencing and digital high-resolution histopathology, have contributed
to the rising requirement for multimodal biomedical modelling, specifically for
image, tabular and graph data. Most multimodal deep learning approaches use
modality-specific architectures that are often trained separately and cannot
capture the crucial cross-modal information that motivates the integration of
different data sources. This paper presents the Hybrid Early-fusion Attention
Learning Network (HEALNet): a flexible multimodal fusion architecture, which a)
preserves modality-specific structural information, b) captures the cross-modal
interactions and structural information in a shared latent space, c) can
effectively handle missing modalities during training and inference, and d)
enables intuitive model inspection by learning on the raw data input instead of
opaque embeddings. We conduct multimodal survival analysis on Whole Slide
Images and Multi-omic data on four cancer datasets from The Cancer Genome Atlas
(TCGA). HEALNet achieves state-of-the-art performance compared to other
end-to-end trained fusion models, substantially improving over unimodal and
multimodal baselines whilst being robust in scenarios with missing modalities.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2311.09115v3
LUMA: A Benchmark Dataset for Learning from Uncertain and Multimodal Data,"Multimodal Deep Learning enhances decision-making by integrating diverse
information sources, such as texts, images, audio, and videos. To develop
trustworthy multimodal approaches, it is essential to understand how
uncertainty impacts these models. We propose LUMA, a unique benchmark dataset,
featuring audio, image, and textual data from 50 classes, for learning from
uncertain and multimodal data. It extends the well-known CIFAR 10/100 dataset
with audio samples extracted from three audio corpora, and text data generated
using the Gemma-7B Large Language Model (LLM). The LUMA dataset enables the
controlled injection of varying types and degrees of uncertainty to achieve and
tailor specific experiments and benchmarking initiatives. LUMA is also
available as a Python package including the functions for generating multiple
variants of the dataset with controlling the diversity of the data, the amount
of noise for each modality, and adding out-of-distribution samples. A baseline
pre-trained model is also provided alongside three uncertainty quantification
methods: Monte-Carlo Dropout, Deep Ensemble, and Reliable Conflictive
Multi-View Learning. This comprehensive dataset and its benchmarking tools are
intended to promote and support the development, evaluation, and benchmarking
of trustworthy and robust multimodal deep learning approaches. We anticipate
that the LUMA dataset will help the ICLR community to design more trustworthy
and robust machine learning approaches for safety critical applications.","['cs.LG', 'cs.AI', 'cs.CL', 'cs.CV']",http://arxiv.org/abs/2406.09864v2
Validation & Exploration of Multimodal Deep-Learning Camera-Lidar Calibration models,"This article presents an innovative study in exploring, evaluating, and
implementing deep learning architectures for the calibration of multi-modal
sensor systems. The focus behind this is to leverage the use of sensor fusion
to achieve dynamic, real-time alignment between 3D LiDAR and 2D Camera sensors.
static calibration methods are tedious and time-consuming, which is why we
propose utilizing Conventional Neural Networks (CNN) coupled with geometrically
informed learning to solve this issue. We leverage the foundational principles
of Extrinsic LiDAR-Camera Calibration tools such as RegNet, CalibNet, and
LCCNet by exploring open-source models that are available online and comparing
our results with their corresponding research papers. Requirements for
extracting these visual and measurable outputs involved tweaking source code,
fine-tuning, training, validation, and testing for each of these frameworks for
equal comparisons. This approach aims to investigate which of these advanced
networks produces the most accurate and consistent predictions. Through a
series of experiments, we reveal some of their shortcomings and areas for
potential improvements along the way. We find that LCCNet yields the best
results out of all the models that we validated.","['cs.CV', 'cs.AI', 'cs.RO']",http://arxiv.org/abs/2409.13402v1
Integrating Chemical Language and Molecular Graph in Multimodal Fused Deep Learning for Drug Property Prediction,"Accurately predicting molecular properties is a challenging but essential
task in drug discovery. Recently, many mono-modal deep learning methods have
been successfully applied to molecular property prediction. However, the
inherent limitation of mono-modal learning arises from relying solely on one
modality of molecular representation, which restricts a comprehensive
understanding of drug molecules and hampers their resilience against data
noise. To overcome the limitations, we construct multimodal deep learning
models to cover different molecular representations. We convert drug molecules
into three molecular representations, SMILES-encoded vectors, ECFP
fingerprints, and molecular graphs. To process the modal information,
Transformer-Encoder, bi-directional gated recurrent units (BiGRU), and graph
convolutional network (GCN) are utilized for feature learning respectively,
which can enhance the model capability to acquire complementary and naturally
occurring bioinformatics information. We evaluated our triple-modal model on
six molecule datasets. Different from bi-modal learning models, we adopt five
fusion methods to capture the specific features and leverage the contribution
of each modal information better. Compared with mono-modal models, our
multimodal fused deep learning (MMFDL) models outperform single models in
accuracy, reliability, and resistance capability against noise. Moreover, we
demonstrate its generalization ability in the prediction of binding constants
for protein-ligand complex molecules in the refined set of PDBbind. The
advantage of the multimodal model lies in its ability to process diverse
sources of data using proper models and suitable fusion methods, which would
enhance the noise resistance of the model while obtaining data diversity.","['cs.LG', 'physics.bio-ph', 'q-bio.BM']",http://arxiv.org/abs/2312.17495v2
Multimodal self-supervised learning for lesion localization,"Multimodal deep learning utilizing imaging and diagnostic reports has made
impressive progress in the field of medical imaging diagnostics, demonstrating
a particularly strong capability for auxiliary diagnosis in cases where
sufficient annotation information is lacking. Nonetheless, localizing diseases
accurately without detailed positional annotations remains a challenge.
Although existing methods have attempted to utilize local information to
achieve fine-grained semantic alignment, their capability in extracting the
fine-grained semantics of the comprehensive context within reports is limited.
To address this problem, a new method is introduced that takes full sentences
from textual reports as the basic units for local semantic alignment. This
approach combines chest X-ray images with their corresponding textual reports,
performing contrastive learning at both global and local levels. The leading
results obtained by this method on multiple datasets confirm its efficacy in
the task of lesion localization.",['cs.CV'],http://arxiv.org/abs/2401.01524v3
Focus on Focus: Focus-oriented Representation Learning and Multi-view Cross-modal Alignment for Glioma Grading,"Recently, multimodal deep learning, which integrates histopathology slides
and molecular biomarkers, has achieved a promising performance in glioma
grading. Despite great progress, due to the intra-modality complexity and
inter-modality heterogeneity, existing studies suffer from inadequate
histopathology representation learning and inefficient molecular-pathology
knowledge alignment. These two issues hinder existing methods to precisely
interpret diagnostic molecular-pathology features, thereby limiting their
grading performance. Moreover, the real-world applicability of existing
multimodal approaches is significantly restricted as molecular biomarkers are
not always available during clinical deployment. To address these problems, we
introduce a novel Focus on Focus (FoF) framework with paired pathology-genomic
training and applicable pathology-only inference, enhancing molecular-pathology
representation effectively. Specifically, we propose a Focus-oriented
Representation Learning (FRL) module to encourage the model to identify regions
positively or negatively related to glioma grading and guide it to focus on the
diagnostic areas with a consistency constraint. To effectively link the
molecular biomarkers to morphological features, we propose a Multi-view
Cross-modal Alignment (MCA) module that projects histopathology representations
into molecular subspaces, aligning morphological features with corresponding
molecular biomarker status by supervised contrastive learning. Experiments on
the TCGA GBM-LGG dataset demonstrate that our FoF framework significantly
improves the glioma grading. Remarkably, our FoF achieves superior performance
using only histopathology slides compared to existing multimodal methods. The
source code is available at https://github.com/peterlipan/FoF.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2408.08527v1
A Systematic Review of Intermediate Fusion in Multimodal Deep Learning for Biomedical Applications,"Deep learning has revolutionized biomedical research by providing
sophisticated methods to handle complex, high-dimensional data. Multimodal deep
learning (MDL) further enhances this capability by integrating diverse data
types such as imaging, textual data, and genetic information, leading to more
robust and accurate predictive models. In MDL, differently from early and late
fusion methods, intermediate fusion stands out for its ability to effectively
combine modality-specific features during the learning process. This systematic
review aims to comprehensively analyze and formalize current intermediate
fusion methods in biomedical applications. We investigate the techniques
employed, the challenges faced, and potential future directions for advancing
intermediate fusion methods. Additionally, we introduce a structured notation
to enhance the understanding and application of these methods beyond the
biomedical domain. Our findings are intended to support researchers, healthcare
professionals, and the broader deep learning community in developing more
sophisticated and insightful multimodal models. Through this review, we aim to
provide a foundational framework for future research and practical applications
in the dynamic field of MDL.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2408.02686v1
Multimodal Guidance Network for Missing-Modality Inference in Content Moderation,"Multimodal deep learning, especially vision-language models, have gained
significant traction in recent years, greatly improving performance on many
downstream tasks, including content moderation and violence detection. However,
standard multimodal approaches often assume consistent modalities between
training and inference, limiting applications in many real-world use cases, as
some modalities may not be available during inference. While existing research
mitigates this problem through reconstructing the missing modalities, they
unavoidably increase unnecessary computational cost, which could be just as
critical, especially for large, deployed infrastructures in industry. To this
end, we propose a novel guidance network that promotes knowledge sharing during
training, taking advantage of the multimodal representations to train better
single-modality models to be used for inference. Real-world experiments in
violence detection shows that our proposed framework trains single-modality
models that significantly outperform traditionally trained counterparts, while
avoiding increases in computational cost for inference.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2309.03452v2
Advanced Multimodal Deep Learning Architecture for Image-Text Matching,"Image-text matching is a key multimodal task that aims to model the semantic
association between images and text as a matching relationship. With the advent
of the multimedia information age, image, and text data show explosive growth,
and how to accurately realize the efficient and accurate semantic
correspondence between them has become the core issue of common concern in
academia and industry. In this study, we delve into the limitations of current
multimodal deep learning models in processing image-text pairing tasks.
Therefore, we innovatively design an advanced multimodal deep learning
architecture, which combines the high-level abstract representation ability of
deep neural networks for visual information with the advantages of natural
language processing models for text semantic understanding. By introducing a
novel cross-modal attention mechanism and hierarchical feature fusion strategy,
the model achieves deep fusion and two-way interaction between image and text
feature space. In addition, we also optimize the training objectives and loss
functions to ensure that the model can better map the potential association
structure between images and text during the learning process. Experiments show
that compared with existing image-text matching models, the optimized new model
has significantly improved performance on a series of benchmark data sets. In
addition, the new model also shows excellent generalization and robustness on
large and diverse open scenario datasets and can maintain high matching
performance even in the face of previously unseen complex situations.","['cs.LG', 'cs.CL', 'cs.CV']",http://arxiv.org/abs/2406.15306v1
CarbonSense: A Multimodal Dataset and Baseline for Carbon Flux Modelling,"Terrestrial carbon fluxes provide vital information about our biosphere's
health and its capacity to absorb anthropogenic CO$_2$ emissions. The
importance of predicting carbon fluxes has led to the emerging field of
data-driven carbon flux modelling (DDCFM), which uses statistical techniques to
predict carbon fluxes from biophysical data. However, the field lacks a
standardized dataset to promote comparisons between models. To address this
gap, we present CarbonSense, the first machine learning-ready dataset for
DDCFM. CarbonSense integrates measured carbon fluxes, meteorological
predictors, and satellite imagery from 385 locations across the globe, offering
comprehensive coverage and facilitating robust model training. Additionally, we
provide a baseline model using a current state-of-the-art DDCFM approach and a
novel transformer based model. Our experiments illustrate the potential gains
that multimodal deep learning techniques can bring to this domain. By providing
these resources, we aim to lower the barrier to entry for other deep learning
researchers to develop new models and drive new advances in carbon flux
modelling.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2406.04940v1
Multimodal Deep Learning for Low-Resource Settings: A Vector Embedding Alignment Approach for Healthcare Applications,"Large-scale multi-modal deep learning models have revolutionized domains such
as healthcare, highlighting the importance of computational power. However, in
resource-constrained regions like Low and Middle-Income Countries (LMICs),
limited access to GPUs and data poses significant challenges, often leaving
CPUs as the sole resource. To address this, we advocate for leveraging vector
embeddings to enable flexible and efficient computational methodologies,
democratizing multimodal deep learning across diverse contexts.
  Our paper investigates the efficiency and effectiveness of using vector
embeddings from single-modal foundation models and multi-modal Vision-Language
Models (VLMs) for multimodal deep learning in low-resource environments,
particularly in healthcare. Additionally, we propose a simple yet effective
inference-time method to enhance performance by aligning image-text embeddings.
Comparing these approaches with traditional methods, we assess their impact on
computational efficiency and model performance using metrics like accuracy,
F1-score, inference time, training time, and memory usage across three medical
modalities: BRSET (ophthalmology), HAM10000 (dermatology), and SatelliteBench
(public health).
  Our findings show that embeddings reduce computational demands without
compromising model performance. Furthermore, our alignment method improves
performance in medical tasks. This research promotes sustainable AI practices
by optimizing resources in constrained environments, highlighting the potential
of embedding-based approaches for efficient multimodal learning. Vector
embeddings democratize multimodal deep learning in LMICs, particularly in
healthcare, enhancing AI adaptability in varied use cases.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2406.02601v1
Integrating Medical Imaging and Clinical Reports Using Multimodal Deep Learning for Advanced Disease Analysis,"In this paper, an innovative multi-modal deep learning model is proposed to
deeply integrate heterogeneous information from medical images and clinical
reports. First, for medical images, convolutional neural networks were used to
extract high-dimensional features and capture key visual information such as
focal details, texture and spatial distribution. Secondly, for clinical report
text, a two-way long and short-term memory network combined with an attention
mechanism is used for deep semantic understanding, and key statements related
to the disease are accurately captured. The two features interact and integrate
effectively through the designed multi-modal fusion layer to realize the joint
representation learning of image and text. In the empirical study, we selected
a large medical image database covering a variety of diseases, combined with
corresponding clinical reports for model training and validation. The proposed
multimodal deep learning model demonstrated substantial superiority in the
realms of disease classification, lesion localization, and clinical description
generation, as evidenced by the experimental results.","['cs.LG', 'cs.AI', 'cs.CL', 'cs.CV']",http://arxiv.org/abs/2405.17459v1
The Role of Emotions in Informational Support Question-Response Pairs in Online Health Communities: A Multimodal Deep Learning Approach,"This study explores the relationship between informational support seeking
questions, responses, and helpfulness ratings in online health communities. We
created a labeled data set of question-response pairs and developed multimodal
machine learning and deep learning models to reliably predict informational
support questions and responses. We employed explainable AI to reveal the
emotions embedded in informational support exchanges, demonstrating the
importance of emotion in providing informational support. This complex
interplay between emotional and informational support has not been previously
researched. The study refines social support theory and lays the groundwork for
the development of user decision aids. Further implications are discussed.","['cs.AI', 'cs.SI', 'H.4.3; I.2.7']",http://arxiv.org/abs/2405.13099v1
Research on Image Recognition Technology Based on Multimodal Deep Learning,"This project investigates the human multi-modal behavior identification
algorithm utilizing deep neural networks. According to the characteristics of
different modal information, different deep neural networks are used to adapt
to different modal video information. Through the integration of various deep
neural networks, the algorithm successfully identifies behaviors across
multiple modalities. In this project, multiple cameras developed by Microsoft
Kinect were used to collect corresponding bone point data based on acquiring
conventional images. In this way, the motion features in the image can be
extracted. Ultimately, the behavioral characteristics discerned through both
approaches are synthesized to facilitate the precise identification and
categorization of behaviors. The performance of the suggested algorithm was
evaluated using the MSR3D data set. The findings from these experiments
indicate that the accuracy in recognizing behaviors remains consistently high,
suggesting that the algorithm is reliable in various scenarios. Additionally,
the tests demonstrate that the algorithm substantially enhances the accuracy of
detecting pedestrian behaviors in video footage.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2405.03091v1
Feature importance to explain multimodal prediction models. A clinical use case,"Surgery to treat elderly hip fracture patients may cause complications that
can lead to early mortality. An early warning system for complications could
provoke clinicians to monitor high-risk patients more carefully and address
potential complications early, or inform the patient. In this work, we develop
a multimodal deep-learning model for post-operative mortality prediction using
pre-operative and per-operative data from elderly hip fracture patients.
Specifically, we include static patient data, hip and chest images before
surgery in pre-operative data, vital signals, and medications administered
during surgery in per-operative data. We extract features from image modalities
using ResNet and from vital signals using LSTM. Explainable model outcomes are
essential for clinical applicability, therefore we compute Shapley values to
explain the predictions of our multimodal black box model. We find that i)
Shapley values can be used to estimate the relative contribution of each
modality both locally and globally, and ii) a modified version of the chain
rule can be used to propagate Shapley values through a sequence of models
supporting interpretable local explanations. Our findings imply that a
multimodal combination of black box models can be explained by propagating
Shapley values through the model sequence.",['cs.LG'],http://arxiv.org/abs/2404.18631v1
Integrating Wearable Sensor Data and Self-reported Diaries for Personalized Affect Forecasting,"Emotional states, as indicators of affect, are pivotal to overall health,
making their accurate prediction before onset crucial. Current studies are
primarily centered on immediate short-term affect detection using data from
wearable and mobile devices. These studies typically focus on objective sensory
measures, often neglecting other forms of self-reported information like
diaries and notes. In this paper, we propose a multimodal deep learning model
for affect status forecasting. This model combines a transformer encoder with a
pre-trained language model, facilitating the integrated analysis of objective
metrics and self-reported diaries. To validate our model, we conduct a
longitudinal study, enrolling college students and monitoring them over a year,
to collect an extensive dataset including physiological, environmental, sleep,
metabolic, and physical activity parameters, alongside open-ended textual
diaries provided by the participants. Our results demonstrate that the proposed
model achieves predictive accuracy of 82.50% for positive affect and 82.76% for
negative affect, a full week in advance. The effectiveness of our model is
further elevated by its explainability.","['cs.LG', 'cs.AI']",http://arxiv.org/abs/2403.13841v2
Describe-and-Dissect: Interpreting Neurons in Vision Networks with Language Models,"In this paper, we propose Describe-and-Dissect (DnD), a novel method to
describe the roles of hidden neurons in vision networks. DnD utilizes recent
advancements in multimodal deep learning to produce complex natural language
descriptions, without the need for labeled training data or a predefined set of
concepts to choose from. Additionally, DnD is training-free, meaning we don't
train any new models and can easily leverage more capable general purpose
models in the future. We have conducted extensive qualitative and quantitative
analysis to show that DnD outperforms prior work by providing higher quality
neuron descriptions. Specifically, our method on average provides the highest
quality labels and is more than 2 times as likely to be selected as the best
explanation for a neuron than the best baseline.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2403.13771v1
A Multimodal Intermediate Fusion Network with Manifold Learning for Stress Detection,"Multimodal deep learning methods capture synergistic features from multiple
modalities and have the potential to improve accuracy for stress detection
compared to unimodal methods. However, this accuracy gain typically comes from
high computational cost due to the high-dimensional feature spaces, especially
for intermediate fusion. Dimensionality reduction is one way to optimize
multimodal learning by simplifying data and making the features more amenable
to processing and analysis, thereby reducing computational complexity. This
paper introduces an intermediate multimodal fusion network with manifold
learning-based dimensionality reduction. The multimodal network generates
independent representations from biometric signals and facial landmarks through
1D-CNN and 2D-CNN. Finally, these features are fused and fed to another 1D-CNN
layer, followed by a fully connected dense layer. We compared various
dimensionality reduction techniques for different variations of unimodal and
multimodal networks. We observe that the intermediate-level fusion with the
Multi-Dimensional Scaling (MDS) manifold method showed promising results with
an accuracy of 96.00\% in a Leave-One-Subject-Out Cross-Validation (LOSO-CV)
paradigm over other dimensional reduction methods. MDS had the highest
computational cost among manifold learning methods. However, while
outperforming other networks, it managed to reduce the computational cost of
the proposed networks by 25\% when compared to six well-known conventional
feature selection methods used in the preprocessing step.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2403.08077v1
Multimodal deep learning approach to predicting neurological recovery from coma after cardiac arrest,"This work showcases our team's (The BEEGees) contributions to the 2023 George
B. Moody PhysioNet Challenge. The aim was to predict neurological recovery from
coma following cardiac arrest using clinical data and time-series such as
multi-channel EEG and ECG signals. Our modelling approach is multimodal, based
on two-dimensional spectrogram representations derived from numerous EEG
channels, alongside the integration of clinical data and features extracted
directly from EEG recordings. Our submitted model achieved a Challenge score of
$0.53$ on the hidden test set for predictions made $72$ hours after return of
spontaneous circulation. Our study shows the efficacy and limitations of
employing transfer learning in medical classification. With regard to
prospective implementation, our analysis reveals that the performance of the
model is strongly linked to the selection of a decision threshold and exhibits
strong variability across data splits.","['cs.LG', 'eess.SP']",http://arxiv.org/abs/2403.06027v1
Multimodal Learning To Improve Cardiac Late Mechanical Activation Detection From Cine MR Images,"This paper presents a multimodal deep learning framework that utilizes
advanced image techniques to improve the performance of clinical analysis
heavily dependent on routinely acquired standard images. More specifically, we
develop a joint learning network that for the first time leverages the accuracy
and reproducibility of myocardial strains obtained from Displacement Encoding
with Stimulated Echo (DENSE) to guide the analysis of cine cardiac magnetic
resonance (CMR) imaging in late mechanical activation (LMA) detection. An image
registration network is utilized to acquire the knowledge of cardiac motions,
an important feature estimator of strain values, from standard cine CMRs. Our
framework consists of two major components: (i) a DENSE-supervised strain
network leveraging latent motion features learned from a registration network
to predict myocardial strains; and (ii) a LMA network taking advantage of the
predicted strain for effective LMA detection. Experimental results show that
our proposed work substantially improves the performance of strain analysis and
LMA detection from cine CMR images, aligning more closely with the achievements
of DENSE.",['cs.CV'],http://arxiv.org/abs/2402.18507v1
Multimodal Urban Areas of Interest Generation via Remote Sensing Imagery and Geographical Prior,"Urban area-of-interest (AOI) refers to an integrated urban functional zone
with defined polygonal boundaries. The rapid development of urban commerce has
led to increasing demands for highly accurate and timely AOI data. However,
existing research primarily focuses on coarse-grained functional zones for
urban planning or regional economic analysis, and often neglects the expiration
of AOI in the real world. They fail to fulfill the precision demands of Mobile
Internet Online-to-Offline (O2O) businesses. These businesses require accuracy
down to a specific community, school, or hospital. In this paper, we propose a
comprehensive end-to-end multimodal deep learning framework designed for
simultaneously detecting accurate AOI boundaries and validating the reliability
of AOI by leveraging remote sensing imagery coupled with geographical prior,
titled AOITR. Unlike conventional AOI generation methods, such as the Road-cut
method that segments road networks at various levels, our approach diverges
from semantic segmentation algorithms that depend on pixel-level
classification. Instead, our AOITR begins by selecting a point-of-interest
(POI) of specific category, and uses it to retrieve corresponding remote
sensing imagery and geographical prior such as entrance POIs and road nodes.
This information helps to build a multimodal detection model based on
transformer encoder-decoder architecture to regress the AOI polygon.
Additionally, we utilize the dynamic features from human mobility, nearby POIs,
and logistics addresses for AOI reliability evaluation via a cascaded network
module. The experimental results reveal that our algorithm achieves a
significant improvement on Intersection over Union (IoU) metric, surpassing
previous methods by a large margin.","['cs.CV', 'cs.AI', '68T99', 'I.4.9']",http://arxiv.org/abs/2401.06550v3
Predicting the Skies: A Novel Model for Flight-Level Passenger Traffic Forecasting,"Accurate prediction of flight-level passenger traffic is of paramount
importance in airline operations, influencing key decisions from pricing to
route optimization. This study introduces a novel, multimodal deep learning
approach to the challenge of predicting flight-level passenger traffic,
yielding substantial accuracy improvements compared to traditional models.
Leveraging an extensive dataset from American Airlines, our model ingests
historical traffic data, fare closure information, and seasonality attributes
specific to each flight. Our proposed neural network integrates the strengths
of Recurrent Neural Networks (RNN) and Convolutional Neural Networks (CNN),
exploiting the temporal patterns and spatial relationships within the data to
enhance prediction performance. Crucial to the success of our model is a
comprehensive data processing strategy. We construct 3D tensors to represent
data, apply careful masking strategies to mirror real-world dynamics, and
employ data augmentation techniques to enrich the diversity of our training
set. The efficacy of our approach is borne out in the results: our model
demonstrates an approximate 33\% improvement in Mean Squared Error (MSE)
compared to traditional benchmarks. This study, therefore, highlights the
significant potential of deep learning techniques and meticulous data
processing in advancing the field of flight traffic prediction.","['cs.LG', 'cs.AI', 'stat.AP']",http://arxiv.org/abs/2401.03397v2
Language-Assisted Deep Learning for Autistic Behaviors Recognition,"Correctly recognizing the behaviors of children with Autism Spectrum Disorder
(ASD) is of vital importance for the diagnosis of Autism and timely early
intervention. However, the observation and recording during the treatment from
the parents of autistic children may not be accurate and objective. In such
cases, automatic recognition systems based on computer vision and machine
learning (in particular deep learning) technology can alleviate this issue to a
large extent. Existing human action recognition models can now achieve
persuasive performance on challenging activity datasets, e.g. daily activity,
and sports activity. However, problem behaviors in children with ASD are very
different from these general activities, and recognizing these problem
behaviors via computer vision is less studied. In this paper, we first evaluate
a strong baseline for action recognition, i.e. Video Swin Transformer, on two
autism behaviors datasets (SSBD and ESBD) and show that it can achieve high
accuracy and outperform the previous methods by a large margin, demonstrating
the feasibility of vision-based problem behaviors recognition. Moreover, we
propose language-assisted training to further enhance the action recognition
performance. Specifically, we develop a two-branch multimodal deep learning
framework by incorporating the ""freely available"" language description for each
type of problem behavior. Experimental results demonstrate that incorporating
additional language supervision can bring an obvious performance boost for the
autism problem behaviors recognition task as compared to using the video
information only (i.e. 3.49% improvement on ESBD and 1.46% on SSBD).",['cs.CV'],http://arxiv.org/abs/2211.09310v3
TextAug: Test time Text Augmentation for Multimodal Person Re-identification,"Multimodal Person Reidentification is gaining popularity in the research
community due to its effectiveness compared to counter-part unimodal
frameworks. However, the bottleneck for multimodal deep learning is the need
for a large volume of multimodal training examples. Data augmentation
techniques such as cropping, flipping, rotation, etc. are often employed in the
image domain to improve the generalization of deep learning models. Augmenting
in other modalities than images, such as text, is challenging and requires
significant computational resources and external data sources. In this study,
we investigate the effectiveness of two computer vision data augmentation
techniques: cutout and cutmix, for text augmentation in multi-modal person
re-identification. Our approach merges these two augmentation strategies into
one strategy called CutMixOut which involves randomly removing words or
sub-phrases from a sentence (Cutout) and blending parts of two or more
sentences to create diverse examples (CutMix) with a certain probability
assigned to each operation. This augmentation was implemented at inference time
without any prior training. Our results demonstrate that the proposed technique
is simple and effective in improving the performance on multiple multimodal
person re-identification benchmarks.","['cs.CV', 'cs.LG']",http://arxiv.org/abs/2312.01605v1
Multimodal deep learning for mapping forest dominant height by fusing GEDI with earth observation data,"The integration of multisource remote sensing data and deep learning models
offers new possibilities for accurately mapping high spatial resolution forest
height. We found that GEDI relative heights (RH) metrics exhibited strong
correlation with the mean of the top 10 highest trees (dominant height)
measured in situ at the corresponding footprint locations. Consequently, we
proposed a novel deep learning framework termed the multi-modal attention
remote sensing network (MARSNet) to estimate forest dominant height by
extrapolating dominant height derived from GEDI, using Setinel-1 data, ALOS-2
PALSAR-2 data, Sentinel-2 optical data and ancillary data. MARSNet comprises
separate encoders for each remote sensing data modality to extract multi-scale
features, and a shared decoder to fuse the features and estimate height. Using
individual encoders for each remote sensing imagery avoids interference across
modalities and extracts distinct representations. To focus on the efficacious
information from each dataset, we reduced the prevalent spatial and band
redundancies in each remote sensing data by incorporating the extended spatial
and band reconstruction convolution modules in the encoders. MARSNet achieved
commendable performance in estimating dominant height, with an R2 of 0.62 and
RMSE of 2.82 m, outperforming the widely used random forest approach which
attained an R2 of 0.55 and RMSE of 3.05 m. Finally, we applied the trained
MARSNet model to generate wall-to-wall maps at 10 m resolution for Jilin,
China. Through independent validation using field measurements, MARSNet
demonstrated an R2 of 0.58 and RMSE of 3.76 m, compared to 0.41 and 4.37 m for
the random forest baseline. Our research demonstrates the effectiveness of a
multimodal deep learning approach fusing GEDI with SAR and passive optical
imagery for enhancing the accuracy of high resolution dominant height
estimation.","['cs.CV', 'cs.LG', 'eess.IV']",http://arxiv.org/abs/2311.11777v1
Dynamic Task and Weight Prioritization Curriculum Learning for Multimodal Imagery,"This paper explores post-disaster analytics using multimodal deep learning
models trained with curriculum learning method. Studying post-disaster
analytics is important as it plays a crucial role in mitigating the impact of
disasters by providing timely and accurate insights into the extent of damage
and the allocation of resources. We propose a curriculum learning strategy to
enhance the performance of multimodal deep learning models. Curriculum learning
emulates the progressive learning sequence in human education by training deep
learning models on increasingly complex data. Our primary objective is to
develop a curriculum-trained multimodal deep learning model, with a particular
focus on visual question answering (VQA) capable of jointly processing image
and text data, in conjunction with semantic segmentation for disaster analytics
using the
FloodNet\footnote{https://github.com/BinaLab/FloodNet-Challenge-EARTHVISION2021}
dataset. To achieve this, U-Net model is used for semantic segmentation and
image encoding. A custom built text classifier is used for visual question
answering. Existing curriculum learning methods rely on manually defined
difficulty functions. We introduce a novel curriculum learning approach termed
Dynamic Task and Weight Prioritization (DATWEP), which leverages a
gradient-based method to automatically decide task difficulty during curriculum
learning training, thereby eliminating the need for explicit difficulty
computation. The integration of DATWEP into our multimodal model shows
improvement on VQA performance. Source code is available at
https://github.com/fualsan/DATWEP.","['cs.CV', 'cs.AI']",http://arxiv.org/abs/2310.19109v2
A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media,"Over the last decade, there has been a vast increase in eating disorder
diagnoses and eating disorder-attributed deaths, reaching their zenith during
the Covid-19 pandemic. This immense growth derived in part from the stressors
of the pandemic but also from increased exposure to social media, which is rife
with content that promotes eating disorders. This study aimed to create a
multimodal deep learning model that can determine if a given social media post
promotes eating disorders based on a combination of visual and textual data. A
labeled dataset of Tweets was collected from Twitter, recently rebranded as X,
upon which twelve deep learning models were trained and evaluated. Based on
model performance, the most effective deep learning model was the multimodal
fusion of the RoBERTa natural language processing model and the MaxViT image
classification model, attaining accuracy and F1 scores of 95.9% and 0.959,
respectively. The RoBERTa and MaxViT fusion model, deployed to classify an
unlabeled dataset of posts from the social media sites Tumblr and Reddit,
generated results akin to those of previous research studies that did not
employ artificial intelligence-based techniques, indicating that deep learning
models can develop insights congruent to those of researchers. Additionally,
the model was used to conduct a time-series analysis of yet unseen Tweets from
eight Twitter hashtags, uncovering that, since 2014, the relative abundance of
content that promotes eating disorders has decreased drastically within those
communities. Despite this reduction, by 2018, content that promotes eating
disorders had either stopped declining or increased in ampleness anew on those
hashtags.","['cs.LG', 'cs.AI', 'cs.CL', 'cs.SI']",http://arxiv.org/abs/2307.06775v4
A scoping review on multimodal deep learning in biomedical images and texts,"Computer-assisted diagnostic and prognostic systems of the future should be
capable of simultaneously processing multimodal data. Multimodal deep learning
(MDL), which involves the integration of multiple sources of data, such as
images and text, has the potential to revolutionize the analysis and
interpretation of biomedical data. However, it only caught researchers'
attention recently. To this end, there is a critical need to conduct a
systematic review on this topic, identify the limitations of current work, and
explore future directions. In this scoping review, we aim to provide a
comprehensive overview of the current state of the field and identify key
concepts, types of studies, and research gaps with a focus on biomedical images
and texts joint learning, mainly because these two were the most commonly
available data types in MDL research. This study reviewed the current uses of
multimodal deep learning on five tasks: (1) Report generation, (2) Visual
question answering, (3) Cross-modal retrieval, (4) Computer-aided diagnosis,
and (5) Semantic segmentation. Our results highlight the diverse applications
and potential of MDL and suggest directions for future research in the field.
We hope our review will facilitate the collaboration of natural language
processing (NLP) and medical imaging communities and support the next
generation of decision-making and computer-assisted diagnostic system
development.","['cs.CV', 'cs.CL']",http://arxiv.org/abs/2307.07362v3
Towards Unified AI Drug Discovery with Multiple Knowledge Modalities,"In recent years, AI models that mine intrinsic patterns from molecular
structures and protein sequences have shown promise in accelerating drug
discovery. However, these methods partly lag behind real-world pharmaceutical
approaches of human experts that additionally grasp structured knowledge from
knowledge bases and unstructured knowledge from biomedical literature. To
bridge this gap, we propose KEDD, a unified, end-to-end, and multimodal deep
learning framework that optimally incorporates both structured and unstructured
knowledge for vast AI drug discovery tasks. The framework first extracts
underlying characteristics from heterogeneous inputs, and then applies
multimodal fusion for accurate prediction. To mitigate the problem of missing
modalities, we leverage multi-head sparse attention and a modality masking
mechanism to extract relevant information robustly. Benefiting from integrated
knowledge, our framework achieves a deeper understanding of molecule entities,
brings significant improvements over state-of-the-art methods on a wide range
of tasks and benchmarks, and reveals its promising potential in assisting
real-world drug discovery.","['cs.LG', 'cs.AI', 'cs.CE']",http://arxiv.org/abs/2305.01523v2
