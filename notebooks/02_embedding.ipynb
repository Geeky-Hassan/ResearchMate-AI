{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code to generate sentence embeddings using the pre-trained models from the [sentence-transformers](https://www.sbert.net/index.html) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA_BASE = Path.cwd().parent / \"data\"\n",
    "PATH_SENTENCES = Path.cwd().parent / \"models/sentences\"\n",
    "PATH_EMBEDDINGS = Path.cwd().parent / \"models/embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting pandas option to display the full content of DataFrame columns without truncation\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "      <th>terms</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Survey on Deep Learning for Polyp Segmentation: Techniques, Challenges and Future Trends</td>\n",
       "      <td>Early detection and assessment of polyps play a crucial role in the\\nprevention and treatment of colorectal cancer (CRC). Polyp segmentation\\nprovides an effective solution to assist clinicians in accurately locating and\\nsegmenting polyp regions. In the past, people often relied on manually\\nextracted lower-level features such as color, texture, and shape, which often\\nhad issues capturing global context and lacked robustness to complex scenarios.\\nWith the advent of deep learning, more and more outstanding medical image\\nsegmentation algorithms based on deep learning networks have emerged, making\\nsignificant progress in this field. This paper provides a comprehensive review\\nof polyp segmentation algorithms. We first review some traditional algorithms\\nbased on manually extracted features and deep segmentation algorithms, then\\ndetail benchmark datasets related to the topic. Specifically, we carry out a\\ncomprehensive evaluation of recent deep learning models and results based on\\npolyp sizes, considering the pain points of research topics and differences in\\nnetwork structures. Finally, we discuss the challenges of polyp segmentation\\nand future trends in this field. The models, benchmark datasets, and source\\ncode links we collected are all published at\\nhttps://github.com/taozh2017/Awesome-Polyp-Segmentation.</td>\n",
       "      <td>['cs.CV']</td>\n",
       "      <td>http://arxiv.org/abs/2311.18373v3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Multi-Scale Feature Extraction and Fusion Deep Learning Method for Classification of Wheat Diseases</td>\n",
       "      <td>Wheat is an important source of dietary fiber and protein that is negatively\\nimpacted by a number of risks to its growth. The difficulty of identifying and\\nclassifying wheat diseases is discussed with an emphasis on wheat loose smut,\\nleaf rust, and crown and root rot. Addressing conditions like crown and root\\nrot, this study introduces an innovative approach that integrates multi-scale\\nfeature extraction with advanced image segmentation techniques to enhance\\nclassification accuracy. The proposed method uses neural network models\\nXception, Inception V3, and ResNet 50 to train on a large wheat disease\\nclassification dataset 2020 in conjunction with an ensemble of machine vision\\nclassifiers, including voting and stacking. The study shows that the suggested\\nmethodology has a superior accuracy of 99.75% in the classification of wheat\\ndiseases when compared to current state-of-the-art approaches. A deep learning\\nensemble model Xception showed the highest accuracy.</td>\n",
       "      <td>['cs.CV', 'cs.LG']</td>\n",
       "      <td>http://arxiv.org/abs/2501.09938v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Image Segmentation with transformers: An Overview, Challenges and Future</td>\n",
       "      <td>Image segmentation, a key task in computer vision, has traditionally relied\\non convolutional neural networks (CNNs), yet these models struggle with\\ncapturing complex spatial dependencies, objects with varying scales, need for\\nmanually crafted architecture components and contextual information. This paper\\nexplores the shortcomings of CNN-based models and the shift towards transformer\\narchitectures -to overcome those limitations. This work reviews\\nstate-of-the-art transformer-based segmentation models, addressing\\nsegmentation-specific challenges and their solutions. The paper discusses\\ncurrent challenges in transformer-based segmentation and outlines promising\\nfuture trends, such as lightweight architectures and enhanced data efficiency.\\nThis survey serves as a guide for understanding the impact of transformers in\\nadvancing segmentation capabilities and overcoming the limitations of\\ntraditional models.</td>\n",
       "      <td>['cs.CV']</td>\n",
       "      <td>http://arxiv.org/abs/2501.09372v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shape-Based Single Object Classification Using Ensemble Method Classifiers</td>\n",
       "      <td>Nowadays, more and more images are available. Annotation and retrieval of the\\nimages pose classification problems, where each class is defined as the group\\nof database images labelled with a common semantic label. Various systems have\\nbeen proposed for content-based retrieval, as well as for image classification\\nand indexing. In this paper, a hierarchical classification framework has been\\nproposed for bridging the semantic gap effectively and achieving multi-category\\nimage classification. A well known pre-processing and post-processing method\\nwas used and applied to three problems; image segmentation, object\\nidentification and image classification. The method was applied to classify\\nsingle object images from Amazon and Google datasets. The classification was\\ntested for four different classifiers; BayesNetwork (BN), Random Forest (RF),\\nBagging and Vote. The estimated classification accuracies ranged from 20% to\\n99% (using 10-fold cross validation). The Bagging classifier presents the best\\nperformance, followed by the Random Forest classifier.</td>\n",
       "      <td>['cs.CV', 'cs.AI', 'cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/2501.09311v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Few-Shot Adaptation of Training-Free Foundation Model for 3D Medical Image Segmentation</td>\n",
       "      <td>Vision foundation models have achieved remarkable progress across various\\nimage analysis tasks. In the image segmentation task, foundation models like\\nthe Segment Anything Model (SAM) enable generalizable zero-shot segmentation\\nthrough user-provided prompts. However, SAM primarily trained on natural\\nimages, lacks the domain-specific expertise of medical imaging. This limitation\\nposes challenges when applying SAM to medical image segmentation, including the\\nneed for extensive fine-tuning on specialized medical datasets and a dependency\\non manual prompts, which are both labor-intensive and require intervention from\\nmedical experts.\\n  This work introduces the Few-shot Adaptation of Training-frEe SAM (FATE-SAM),\\na novel method designed to adapt the advanced Segment Anything Model 2 (SAM2)\\nfor 3D medical image segmentation. FATE-SAM reassembles pre-trained modules of\\nSAM2 to enable few-shot adaptation, leveraging a small number of support\\nexamples to capture anatomical knowledge and perform prompt-free segmentation,\\nwithout requiring model fine-tuning. To handle the volumetric nature of medical\\nimages, we incorporate a Volumetric Consistency mechanism that enhances spatial\\ncoherence across 3D slices. We evaluate FATE-SAM on multiple medical imaging\\ndatasets and compare it with supervised learning methods, zero-shot SAM\\napproaches, and fine-tuned medical SAM methods. Results show that FATE-SAM\\ndelivers robust and accurate segmentation while eliminating the need for large\\nannotated datasets and expert intervention. FATE-SAM provides a practical,\\nefficient solution for medical image segmentation, making it more accessible\\nfor clinical applications.</td>\n",
       "      <td>['cs.CV']</td>\n",
       "      <td>http://arxiv.org/abs/2501.09138v1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  titles  \\\n",
       "0             A Survey on Deep Learning for Polyp Segmentation: Techniques, Challenges and Future Trends   \n",
       "1  A Multi-Scale Feature Extraction and Fusion Deep Learning Method for Classification of Wheat Diseases   \n",
       "2                               Image Segmentation with transformers: An Overview, Challenges and Future   \n",
       "3                             Shape-Based Single Object Classification Using Ensemble Method Classifiers   \n",
       "4                Few-Shot Adaptation of Training-Free Foundation Model for 3D Medical Image Segmentation   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    abstracts  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                    Early detection and assessment of polyps play a crucial role in the\\nprevention and treatment of colorectal cancer (CRC). Polyp segmentation\\nprovides an effective solution to assist clinicians in accurately locating and\\nsegmenting polyp regions. In the past, people often relied on manually\\nextracted lower-level features such as color, texture, and shape, which often\\nhad issues capturing global context and lacked robustness to complex scenarios.\\nWith the advent of deep learning, more and more outstanding medical image\\nsegmentation algorithms based on deep learning networks have emerged, making\\nsignificant progress in this field. This paper provides a comprehensive review\\nof polyp segmentation algorithms. We first review some traditional algorithms\\nbased on manually extracted features and deep segmentation algorithms, then\\ndetail benchmark datasets related to the topic. Specifically, we carry out a\\ncomprehensive evaluation of recent deep learning models and results based on\\npolyp sizes, considering the pain points of research topics and differences in\\nnetwork structures. Finally, we discuss the challenges of polyp segmentation\\nand future trends in this field. The models, benchmark datasets, and source\\ncode links we collected are all published at\\nhttps://github.com/taozh2017/Awesome-Polyp-Segmentation.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Wheat is an important source of dietary fiber and protein that is negatively\\nimpacted by a number of risks to its growth. The difficulty of identifying and\\nclassifying wheat diseases is discussed with an emphasis on wheat loose smut,\\nleaf rust, and crown and root rot. Addressing conditions like crown and root\\nrot, this study introduces an innovative approach that integrates multi-scale\\nfeature extraction with advanced image segmentation techniques to enhance\\nclassification accuracy. The proposed method uses neural network models\\nXception, Inception V3, and ResNet 50 to train on a large wheat disease\\nclassification dataset 2020 in conjunction with an ensemble of machine vision\\nclassifiers, including voting and stacking. The study shows that the suggested\\nmethodology has a superior accuracy of 99.75% in the classification of wheat\\ndiseases when compared to current state-of-the-art approaches. A deep learning\\nensemble model Xception showed the highest accuracy.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Image segmentation, a key task in computer vision, has traditionally relied\\non convolutional neural networks (CNNs), yet these models struggle with\\ncapturing complex spatial dependencies, objects with varying scales, need for\\nmanually crafted architecture components and contextual information. This paper\\nexplores the shortcomings of CNN-based models and the shift towards transformer\\narchitectures -to overcome those limitations. This work reviews\\nstate-of-the-art transformer-based segmentation models, addressing\\nsegmentation-specific challenges and their solutions. The paper discusses\\ncurrent challenges in transformer-based segmentation and outlines promising\\nfuture trends, such as lightweight architectures and enhanced data efficiency.\\nThis survey serves as a guide for understanding the impact of transformers in\\nadvancing segmentation capabilities and overcoming the limitations of\\ntraditional models.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Nowadays, more and more images are available. Annotation and retrieval of the\\nimages pose classification problems, where each class is defined as the group\\nof database images labelled with a common semantic label. Various systems have\\nbeen proposed for content-based retrieval, as well as for image classification\\nand indexing. In this paper, a hierarchical classification framework has been\\nproposed for bridging the semantic gap effectively and achieving multi-category\\nimage classification. A well known pre-processing and post-processing method\\nwas used and applied to three problems; image segmentation, object\\nidentification and image classification. The method was applied to classify\\nsingle object images from Amazon and Google datasets. The classification was\\ntested for four different classifiers; BayesNetwork (BN), Random Forest (RF),\\nBagging and Vote. The estimated classification accuracies ranged from 20% to\\n99% (using 10-fold cross validation). The Bagging classifier presents the best\\nperformance, followed by the Random Forest classifier.   \n",
       "4  Vision foundation models have achieved remarkable progress across various\\nimage analysis tasks. In the image segmentation task, foundation models like\\nthe Segment Anything Model (SAM) enable generalizable zero-shot segmentation\\nthrough user-provided prompts. However, SAM primarily trained on natural\\nimages, lacks the domain-specific expertise of medical imaging. This limitation\\nposes challenges when applying SAM to medical image segmentation, including the\\nneed for extensive fine-tuning on specialized medical datasets and a dependency\\non manual prompts, which are both labor-intensive and require intervention from\\nmedical experts.\\n  This work introduces the Few-shot Adaptation of Training-frEe SAM (FATE-SAM),\\na novel method designed to adapt the advanced Segment Anything Model 2 (SAM2)\\nfor 3D medical image segmentation. FATE-SAM reassembles pre-trained modules of\\nSAM2 to enable few-shot adaptation, leveraging a small number of support\\nexamples to capture anatomical knowledge and perform prompt-free segmentation,\\nwithout requiring model fine-tuning. To handle the volumetric nature of medical\\nimages, we incorporate a Volumetric Consistency mechanism that enhances spatial\\ncoherence across 3D slices. We evaluate FATE-SAM on multiple medical imaging\\ndatasets and compare it with supervised learning methods, zero-shot SAM\\napproaches, and fine-tuned medical SAM methods. Results show that FATE-SAM\\ndelivers robust and accurate segmentation while eliminating the need for large\\nannotated datasets and expert intervention. FATE-SAM provides a practical,\\nefficient solution for medical image segmentation, making it more accessible\\nfor clinical applications.   \n",
       "\n",
       "                         terms                               urls  \n",
       "0                    ['cs.CV']  http://arxiv.org/abs/2311.18373v3  \n",
       "1           ['cs.CV', 'cs.LG']  http://arxiv.org/abs/2501.09938v1  \n",
       "2                    ['cs.CV']  http://arxiv.org/abs/2501.09372v1  \n",
       "3  ['cs.CV', 'cs.AI', 'cs.CL']  http://arxiv.org/abs/2501.09311v1  \n",
       "4                    ['cs.CV']  http://arxiv.org/abs/2501.09138v1  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(PATH_DATA_BASE / 'filtered_data.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentence-transformers models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a sentence-transformers model?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It maps sentences & paragraphs to a N dimensional dense vector space and can be used for tasks like clustering or semantic search."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all-MiniLM-L6-v2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MiniLM is a smaller variant of the BERT model which has been designed to provide high-quality language understanding capabilities while being significantly smaller and more efficient. The \"`all-MiniLM-L6-v2`\" model refers to a specific configuration of the MiniLM model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some reasons why I have chosen this model for my project:\n",
    "\n",
    "- Efficiency: MiniLM models are smaller and faster than full-size BERT models, which can be a major advantage if you're working on a project with limited computational resources or if you need to process large amounts of data quickly.\n",
    "- Performance: Despite their smaller size, MiniLM models often perform at a comparable level to full-size BERT models on a variety of NLP tasks. This means that you can often use a MiniLM model without sacrificing much in the way of performance. In fact, the `Performance Sentence Embeddings` metric which is the average performance on encoding sentences over 14 diverse tasks from different domains is `68.06` for the `all-MiniLM-L6-v2` model, which is very good to start with.\n",
    "- Ease of Use: If you're using a library like Hugging Face's Transformers, it can be relatively straightforward to load a pre-trained MiniLM model and fine-tune it for your specific task.\n",
    "- Lower Memory Requirements: Given its smaller size, MiniLM requires less memory for training and inference. This could be a crucial factor if you're working with limited hardware resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Our feature we like to encode\n",
    "titles = dataset['titles']\n",
    "urls = dataset['urls']\n",
    "# Features are encoded by calling model.encode()\n",
    "embeddings_titles = model.encode(titles)\n",
    "embeddings_url = model.encode(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A Survey on Deep Learning for Polyp Segmentation: Techniques, Challenges and Future Trends\n",
      "Embedding dimension: 384\n",
      "Title length: 90\n",
      "\n",
      "Title: A Multi-Scale Feature Extraction and Fusion Deep Learning Method for Classification of Wheat Diseases\n",
      "Embedding dimension: 384\n",
      "Title length: 101\n",
      "\n",
      "Title: Image Segmentation with transformers: An Overview, Challenges and Future\n",
      "Embedding dimension: 384\n",
      "Title length: 72\n",
      "\n",
      "Title: Shape-Based Single Object Classification Using Ensemble Method Classifiers\n",
      "Embedding dimension: 384\n",
      "Title length: 74\n",
      "\n",
      "Title: Few-Shot Adaptation of Training-Free Foundation Model for 3D Medical Image Segmentation\n",
      "Embedding dimension: 384\n",
      "Title length: 87\n",
      "\n",
      "Title: Densely Connected Parameter-Efficient Tuning for Referring Image Segmentation\n",
      "Embedding dimension: 384\n",
      "Title length: 77\n",
      "\n",
      "URL: http://arxiv.org/abs/2311.18373v3\n",
      "Embedding dimension: 384\n",
      "URL length: 33\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the embeddings\n",
    "c = 0\n",
    "for title, embedding in zip(titles, embeddings_titles):\n",
    "    print(\"Title:\", title)\n",
    "    print(\"Embedding dimension:\", len(embedding))\n",
    "    print(\"Title length:\", len(title))\n",
    "    print(\"\")\n",
    "\n",
    "    if c >=5:\n",
    "        break\n",
    "    c +=1 \n",
    "    \n",
    "for url, embedding in zip(urls, embeddings_url):\n",
    "    print(\"URL:\", url)\n",
    "    print(\"Embedding dimension:\", len(embedding))\n",
    "    print(\"URL length:\", len(url))\n",
    "    print(\"\")\n",
    "\n",
    "    if c >=5:\n",
    "        break\n",
    "    c +=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'Titles.pkl' saved successfully at: /home/umair/UNI/ML Project/models/Titles.pkl\n",
      "File 'URLS.pkl' saved successfully at: /home/umair/UNI/ML Project/models/URLS.pkl\n",
      "File 'Embedding_Titles.pkl' saved successfully at: /home/umair/UNI/ML Project/models/Embedding_Titles.pkl\n",
      "File 'Embedding_URLS.pkl' saved successfully at: /home/umair/UNI/ML Project/models/Embedding_URLS.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Function to save a pickle file\n",
    "def save_pickle(data, file_name, folder_path):\n",
    "    # Ensure the folder exists\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Define the full file path\n",
    "    file_path = folder_path / file_name\n",
    "    \n",
    "    # Save the pickle file\n",
    "    try:\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        print(f\"File '{file_name}' saved successfully at: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving file '{file_name}': {e}\")\n",
    "\n",
    "# Save the embeddings\n",
    "data_files = {\n",
    "    \"Titles.pkl\": titles,\n",
    "    \"URLS.pkl\": urls,\n",
    "    \"Embedding_Titles.pkl\": embeddings_titles,\n",
    "    \"Embedding_URLS.pkl\": embeddings_url\n",
    "}\n",
    "\n",
    "# Define the folder outside the root directory\n",
    "ROOT = Path.cwd()  # Root folder of the project\n",
    "MODEL = ROOT.parent / \"models\"  # Folder outside the root directory\n",
    "\n",
    "# Save all files\n",
    "for file_name, data in data_files.items():\n",
    "    save_pickle(data, file_name, MODEL)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HI'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_you_like = input(\"Enter your topic of interest here ðŸ‘‡ \\n\")\n",
    "paper_you_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "cosine_scores_titles = util.cos_sim(embeddings_titles, model.encode(paper_you_like))\n",
    "cosine_scores_urls = util.cos_sim(embeddings_url, model.encode(paper_you_like))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "top_similar_papers_titles = torch.topk(cosine_scores_titles,dim=0, k=5,sorted=True)\n",
    "top_similar_papers_urls = torch.topk(cosine_scores_urls,dim=0, k=5,sorted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: On the Convergence of the ELBO to Entropy Sums\n",
      "URL: http://arxiv.org/abs/2209.03077v6\n",
      "\n",
      "Title: Multimodal Adaptive Inference for Document Image Classification with Anytime Early Exiting\n",
      "URL: http://arxiv.org/abs/2405.12705v1\n",
      "\n",
      "Title: ICANet: A Method of Short Video Emotion Recognition Driven by Multimodal Data\n",
      "URL: http://arxiv.org/abs/2208.11346v2\n",
      "\n",
      "Title: Few-shot Structure-Informed Machinery Part Segmentation with Foundation Models and Graph Neural Networks\n",
      "URL: http://arxiv.org/abs/2501.10080v1\n",
      "\n",
      "Title: Diffusion Models in Vision: A Survey\n",
      "URL: http://arxiv.org/abs/2209.04747v6\n",
      "\n",
      "Title: On the Convergence of the ELBO to Entropy Sums\n",
      "URL: http://arxiv.org/abs/2209.03077v6\n",
      "\n",
      "Title: Multimodal Adaptive Inference for Document Image Classification with Anytime Early Exiting\n",
      "URL: http://arxiv.org/abs/2405.12705v1\n",
      "\n",
      "Title: ICANet: A Method of Short Video Emotion Recognition Driven by Multimodal Data\n",
      "URL: http://arxiv.org/abs/2208.11346v2\n",
      "\n",
      "Title: Few-shot Structure-Informed Machinery Part Segmentation with Foundation Models and Graph Neural Networks\n",
      "URL: http://arxiv.org/abs/2501.10080v1\n",
      "\n",
      "Title: Diffusion Models in Vision: A Survey\n",
      "URL: http://arxiv.org/abs/2209.04747v6\n",
      "\n",
      "Title: On the Convergence of the ELBO to Entropy Sums\n",
      "URL: http://arxiv.org/abs/2209.03077v6\n",
      "\n",
      "Title: Multimodal Adaptive Inference for Document Image Classification with Anytime Early Exiting\n",
      "URL: http://arxiv.org/abs/2405.12705v1\n",
      "\n",
      "Title: ICANet: A Method of Short Video Emotion Recognition Driven by Multimodal Data\n",
      "URL: http://arxiv.org/abs/2208.11346v2\n",
      "\n",
      "Title: Few-shot Structure-Informed Machinery Part Segmentation with Foundation Models and Graph Neural Networks\n",
      "URL: http://arxiv.org/abs/2501.10080v1\n",
      "\n",
      "Title: Diffusion Models in Vision: A Survey\n",
      "URL: http://arxiv.org/abs/2209.04747v6\n",
      "\n",
      "Title: On the Convergence of the ELBO to Entropy Sums\n",
      "URL: http://arxiv.org/abs/2209.03077v6\n",
      "\n",
      "Title: Multimodal Adaptive Inference for Document Image Classification with Anytime Early Exiting\n",
      "URL: http://arxiv.org/abs/2405.12705v1\n",
      "\n",
      "Title: ICANet: A Method of Short Video Emotion Recognition Driven by Multimodal Data\n",
      "URL: http://arxiv.org/abs/2208.11346v2\n",
      "\n",
      "Title: Few-shot Structure-Informed Machinery Part Segmentation with Foundation Models and Graph Neural Networks\n",
      "URL: http://arxiv.org/abs/2501.10080v1\n",
      "\n",
      "Title: Diffusion Models in Vision: A Survey\n",
      "URL: http://arxiv.org/abs/2209.04747v6\n",
      "\n",
      "Title: On the Convergence of the ELBO to Entropy Sums\n",
      "URL: http://arxiv.org/abs/2209.03077v6\n",
      "\n",
      "Title: Multimodal Adaptive Inference for Document Image Classification with Anytime Early Exiting\n",
      "URL: http://arxiv.org/abs/2405.12705v1\n",
      "\n",
      "Title: ICANet: A Method of Short Video Emotion Recognition Driven by Multimodal Data\n",
      "URL: http://arxiv.org/abs/2208.11346v2\n",
      "\n",
      "Title: Few-shot Structure-Informed Machinery Part Segmentation with Foundation Models and Graph Neural Networks\n",
      "URL: http://arxiv.org/abs/2501.10080v1\n",
      "\n",
      "Title: Diffusion Models in Vision: A Survey\n",
      "URL: http://arxiv.org/abs/2209.04747v6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in top_similar_papers_titles.indices:\n",
    "    for i in top_similar_papers_urls.indices:\n",
    "        print('Title:', titles[i.item()])\n",
    "        print('URL:', urls[i.item()])\n",
    "        print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
